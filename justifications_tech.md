# üìò Justifications Techniques & M√©thodologiques

## Document de R√©f√©rence pour le Hackathon KPMG

Ce document justifie **chaque d√©cision technique** prise dans l'architecture du syst√®me RAG de veille strat√©gique, en s'appuyant sur :

- ‚úÖ Vos notes de projet (KPMG v2.pdf, hackathon KPMG.pdf)
- ‚úÖ Les documentations officielles (LangChain, Pinecone, Mistral)
- ‚úÖ Les best practices RAG

---

## üèóÔ∏è Architecture Globale

### D√©cision : Architecture RAG vs. Fine-Tuning

**Choix retenu** : RAG (Retrieval-Augmented Generation)

**Justification** :

1. **Selon vos notes (hackathon KPMG.pdf)** :

   > "L'objectif est de b√¢tir un mod√®le capable d'effectuer des recherches sur diff√©rents √©l√©ments pr√©-d√©finis [...] Les r√©sultats du mod√®le doivent indiquer pr√©cis√©ment des sources fiables, r√©centes et de fa√ßons crois√©es."
   >
2. **Avantages du RAG pour KPMG** :

   - ‚úÖ **Tra√ßabilit√©** : Chaque r√©ponse cite ses sources (exigence critique)
   - ‚úÖ **Fra√Æcheur** : Mise √† jour des donn√©es sans r√©entra√Ænement
   - ‚úÖ **Co√ªt** : Pas de fine-tuning co√ªteux
   - ‚úÖ **Explicabilit√©** : Cha√Æne de raisonnement visible
3. **R√©f√©rence LangChain** :

   > "RAG is a technique for augmenting LLM knowledge with additional data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the specific timeframe when they were trained. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs."
   >

   Source : https://python.langchain.com/docs/tutorials/rag/

---

## üóÑÔ∏è Vector Database : Pinecone

### D√©cision : Pinecone vs. Alternatives

**Choix retenu** : Pinecone Serverless

**Alternatives √©valu√©es** :

- ‚ùå Chroma : Difficult√©s de compilation sur Mac 2012 (vos notes)
- ‚ùå FAISS : Pas de persistence cloud, difficile √† scaler
- ‚úÖ Pinecone : Cloud, serverless, namespaces natifs

**Justification** :

1. **Selon vos notes (KPMG v2.pdf)** :

   > "Le passage √† Pinecone et Mistral AI est la meilleure strat√©gie pour vous : tout le travail lourd sera fait sur leurs serveurs, pas sur votre processeur de 2012."
   >
2. **Avantages techniques** :

   - **Serverless Spec** : Scalabilit√© automatique (crucial pour production KPMG)
   - **Namespaces** : Isolation logique des sources (financial_reports, news, etc.)
   - **Pas de gestion d'infrastructure** : Fonctionne m√™me sur Mac 2012
3. **R√©f√©rence Pinecone** :

   > "Namespaces provide a way to separate vectors in a single index. They enable multi-tenancy scenarios where each tenant has isolated data."
   >

   Source : https://docs.pinecone.io/docs/namespaces

---

## üß† Mod√®le LLM : Mistral Medium

### D√©cision : Mistral vs. OpenAI/Claude

**Choix retenu** : Mistral Medium (avec plan gratuit)

**Justification** :

1. **Contrainte budg√©taire (vos notes)** :

   > "Mod√®le : mistral-medium, Plan : gratuit"
   >
2. **Performance Mistral** :

   - Multilingue natif (FR/EN crucial pour KPMG international)
   - Raisonnement comparable √† GPT-3.5
   - Contexte window : 32k tokens ‚âà **20 000 √† 25 000 mots ‚âà **40‚Äì50 pages de texte ( token ‚âà **¬æ de mot anglais**)****

   La *context window* inclut :

   1. **System prompt** (instructions globales)
   2. **Developer prompt** (r√®gles RAG, format, contraintes)
   3. **User prompt** (la question)
   4. **Contexte RAG inject√©** (chunks r√©cup√©r√©s)
   5. **Historique de conversation**
   6. **R√©ponse g√©n√©r√©e par le mod√®le**
3. **Optimisation du prompting** :

   Selon vos notes (KPMG v2.pdf) :

   > "J'ai vu que selon le model certains typ de prompt son plus efficace XML t markdown top pour claude et openain [...] je veux essayer de prompter au llm sous forme de MD"
   >

   **Notre impl√©mentation** : Prompt structur√© en Markdown avec s√©parateurs visuels
4. **R√©f√©rence Mistral** :

   > "Mistral Medium is ideal for language transformation tasks that require moderate complexity, such as customer support chatbots or document summarization."
   >

   Source : https://docs.mistral.ai/getting-started/models/

---

## üìä Embeddings : Mistral-embed

### D√©cision : Mistral-embed (1024 dimensions)

**Choix retenu** : `mistral-embed`

**Alternatives √©valu√©es** :

- OpenAI text-embedding-ada-002 (1536 dim) : Payant
- Sentence Transformers (384-768 dim) : Dimension plus faible

**Justification** :

1. **Coh√©rence avec le LLM** : M√™me fournisseur (Mistral)
2. **Dimension optimale** : 1024 est un bon compromis pr√©cision/co√ªt
3. **Gratuit** : Compatible avec contraintes budg√©taires
4. **Configuration Pinecone** :

   ```python
   pc.create_index(
       name="kpmg-veille",
       dimension=1024,  # Correspond √† Mistral-embed
       metric="cosine"   # Standard pour similarit√© s√©mantique
   )
   ```
5. **R√©f√©rence Mistral** :

   > "The Mistral Embeddings API offers cutting-edge, state-of-the-art embeddings for text, which can be used for many NLP tasks."
   >

   Source : https://docs.mistral.ai/capabilities/embeddings/

---

## ‚úÇÔ∏è Strat√©gie de Chunking

### D√©cision : Chunking Adaptatif par Type de Document

**Choix retenu** : 3 strat√©gies selon le namespace

**Justification** :

1. **Selon vos notes (KPMG v2.pdf)** :

   > "Le Chunking est souvent l'√©tape la plus sous-estim√©e, mais c'est elle qui d√©termine si ton IA va r√©pondre pr√©cis√©ment ou si elle va 'noyer' l'information."
   >
2. **Strat√©gies impl√©ment√©es** :

   | Type Document               | Chunk Size            | Overlap   | Justification                                        |
   | --------------------------- | --------------------- | --------- | ---------------------------------------------------- |
   | **Financial Reports** | 800 chars             | 150 (19%) | Balance contexte/pr√©cision pour chiffres financiers |
   | **News**              | 500 chars             | 100 (20%) | Articles courts, informations denses                 |
   | **HTML Structur√©**   | Variable (par balise) | N/A       | Pr√©serve la structure s√©mantique (H1, H2)          |
3. **R√©f√©rence vos notes** :

   > "Petits chunks (200-500 caract√®res) : Id√©al pour trouver une donn√©e pr√©cise (ex: un chiffre d'affaires, une date). [...] Gros chunks (1000-2000 caract√®res) : Id√©al pour comprendre un raisonnement ou une analyse strat√©gique."
   >
4. **Code impl√©ment√©** :

   ```python
   # Pour les rapports financiers
   RecursiveCharacterTextSplitter(
       chunk_size=800,
       chunk_overlap=150,  # 19% overlap
       separators=["\n\n", "\n", ". ", " ", ""]
   )
   ```
5. **R√©f√©rence LangChain** :

   > "The RecursiveCharacterTextSplitter takes a large text and splits it based on a specified chunk size. It does this by using a set of characters."
   >

   Source : https://python.langchain.com/docs/modules/data_connection/document_transformers/

---

## üîç Strat√©gie de Retrieval

### D√©cision : Retrieval par Namespace avec k=5

**Choix retenu** : Similarit√© cosinus, top 5 documents

**Justification** :

1. **K=5 optimal** :

   - Selon vos notes : "Don't send 20 relevant chunks of data to the AI. Send the top 3 most relevant chunks."
   - Notre compromis : 5 chunks pour √©quilibrer contexte et vitesse
2. **Filtrage par namespace** :

   ```python
   vectorstore = PineconeVectorStore(
       index_name="kpmg-veille",
       namespace="financial_reports"  # Cibl√©
   )
   ```
3. **Avantages** :

   - Requ√™tes cibl√©es (ex: uniquement actualit√©s)
   - R√©duit le bruit (exigence KPMG v2.pdf)
   - Am√©liore la pertinence
4. **R√©f√©rence Pinecone** :

   > "Namespaces enable you to partition vectors within an index. Queries and updates only affect one namespace."
   >

   Source : https://docs.pinecone.io/docs/namespaces

---

## üìù Prompt Engineering KPMG

### D√©cision : Prompt Structur√© avec Citations Obligatoires

**Choix retenu** : Template Markdown avec r√®gles explicites

**Justification** :

1. **Exigences hackathon KPMG.pdf** :

   > "Les r√©sultats du mod√®le doivent indiquer pr√©cis√©ment des sources fiables, r√©centes et de fa√ßons crois√©es."
   >
2. **Format de citation impl√©ment√©** :

   ```
   [Source | Fiabilit√© | Date]

   Exemple :
   "Apple a g√©n√©r√© 394 milliards de revenus en 2023 
   [SEC Filing 10-K | ‚≠ê‚≠ê‚≠ê | 2024-01-15]"
   ```
3. **√âchelle de fiabilit√©** :

   - ‚≠ê‚≠ê‚≠ê : Source primaire (SEC, yfinance)
   - ‚≠ê‚≠ê : Source secondaire (NewsAPI, presse)
   - ‚≠ê : Source tertiaire (blogs)
4. **Structure du prompt** :

   ```
   ‚îÅ‚îÅ‚îÅ R√àGLES DE CITATION ‚îÅ‚îÅ‚îÅ
   [Instructions explicites]

   ‚îÅ‚îÅ‚îÅ CONTEXTE ‚îÅ‚îÅ‚îÅ
   {retrieved_docs}

   ‚îÅ‚îÅ‚îÅ QUESTION ‚îÅ‚îÅ‚îÅ
   {user_query}

   ‚îÅ‚îÅ‚îÅ INSTRUCTIONS R√âPONSE ‚îÅ‚îÅ‚îÅ
   [Format, tone, cas limites]
   ```
5. **R√©f√©rence Mistral** :

   > "We recommend using structured prompts with clear delimiters. This helps the model understand the task better."
   >

   Source : https://docs.mistral.ai/guides/prompting_capabilities/

---

## üîÑ Gestion des Sources

### D√©cision : Loaders Sp√©cialis√©s par Type de Source

**Choix retenu** : 4 loaders diff√©rents

| Source                        | Loader             | Justification                                 |
| ----------------------------- | ------------------ | --------------------------------------------- |
| **SEC EDGAR**           | Requests + API SEC | Donn√©es structur√©es, headers obligatoires   |
| **NewsAPI**             | Requests API       | API REST standard, authentification par cl√©  |
| **Communiqu√©s Presse** | WebBaseLoader      | Web scraping √©thique avec rate limiting      |
| **yfinance**            | yfinance library   | Sp√©cialis√© pour donn√©es financi√®res Yahoo |

**Justification d√©taill√©e** :

1. **SEC EDGAR** :

   - Selon vos notes : "La SEC bloque les requ√™tes sans User-Agent"
   - Impl√©mentation :
     ```python
     headers = {"User-Agent": SEC_USER_AGENT}
     response = requests.get(url, headers=headers)
     ```
2. **WebBaseLoader pour communiqu√©s** :

   - Selon vos notes (KPMG v2.pdf) :

     > "Le WebBaseLoader est un wrapper autour de Requests et BeautifulSoup"
     >
   - Avantages :

     - Extraction automatique du texte
     - Gestion des balises HTML
     - M√©tadonn√©es de source
3. **R√©f√©rence LangChain** :

   > "Document loaders provide a standard interface for reading data from different sources into LangChain's Document format."
   >

   Source : https://python.langchain.com/docs/modules/data_connection/document_loaders/

---

## ‚ö° Optimisations de Performance

### D√©cision : Traitement par Batch + Cache

**Choix retenu** : Batch de 50-100 documents

**Justification** :

1. **Selon vos notes (KPMG v2.pdf)** :

   > "If your RAG chatbot is slow [...] 2Ô∏è‚É£ Cache everything you can. Similar questions get asked all the time. Save the embeddings and responses."
   >
2. **Impl√©mentation batch embeddings** :

   ```python
   def generate_embeddings_batch(documents, batch_size=50):
       for i in range(0, len(documents), batch_size):
           batch = documents[i:i+batch_size]
           texts = [doc.page_content for doc in batch]
           embeddings = embeddings_model.embed_documents(texts)
   ```
3. **Avantages** :

   - R√©duit les appels API (limite rate limiting)
   - Am√©liore la vitesse (parall√©lisation)
   - √âconomise des tokens (plan gratuit Mistral)
4. **Rate limiting Pinecone** :

   ```python
   time.sleep(0.1)  # 10 req/sec max pour tier gratuit
   ```

---

## üìä M√©triques de Validation

### D√©cision : 4 KPIs Standards de l'Industrie

**Choix retenu** : Hit Rate, LLM Judge, Human Feedback, Latence

**Justification** :

1. **Selon vos notes (KPMG v2.pdf)** :

   > "Non-technical stakeholders don't care about embeddings. They want numbers."
   >
2. **M√©triques impl√©ment√©es** :

   | M√©trique                | Objectif | Mesure                                   |
   | ------------------------ | -------- | ---------------------------------------- |
   | **Hit Rate**       | > 75%    | Top 5 docs contiennent la r√©ponse       |
   | **LLM Judge**      | > 0.85   | GPT-4 √©value coh√©rence source/r√©ponse |
   | **Human Feedback** | > 80%    | Boutons üëçüëé dans interface              |
   | **Latence**        | < 5s     | Temps de r√©ponse end-to-end             |
3. **R√©f√©rence vos notes** :

   > "I used GPT-4 to score if each answer contradicted the source docs. The scale I chose was 0-1. It gave an AVG of 0.85. Now I could say '85% accuracy' in meetings."
   >

---

## üîí S√©curit√© & Conformit√©

### D√©cision : Logs d'Audit + M√©tadonn√©es Tra√ßables

**Choix retenu** : Syst√®me de logging centralis√©

**Justification** :

1. **Selon vos notes (KPMG v2.pdf)** :

   > "Les journaux d'audit sont essentiels √† la conformit√© et √† la s√©curit√©. Ils doivent enregistrer [...] les identifiants des utilisateurs, les horodatages, les mod√®les de requ√™te, les documents r√©cup√©r√©s et les r√©ponses g√©n√©r√©es."
   >
2. **Impl√©mentation** :

   ```python
   def log_ingestion(source: str, status: str, details: str):
       timestamp = datetime.now().isoformat()
       log_entry = f"[{timestamp}] {source} - {status} : {details}\n"
       with open("ingestion_logs/ingestion.log", "a") as f:
           f.write(log_entry)
   ```
3. **M√©tadonn√©es enrichies** :

   - Source exacte (URL, API, fichier)
   - Date de r√©cup√©ration
   - Namespace d'origine
   - Type de document
4. **Conformit√© RGPD** :

   - Pas de donn√©es personnelles dans Pinecone
   - Cl√©s API dans .env (hors Git)
   - Logs avec timestamps pour audits

---

## üéØ Gestion des Cas Limites (KPMG)

### D√©cision : Logique de Fallback Explicite

**Choix retenu** : Instructions dans le prompt pour g√©rer ambigu√Øt√©s

**Justification** :

1. **Exigence hackathon KPMG.pdf** :

   > "Si le mod√®le ne parvient pas √† identifier clairement le secteur ou le nom de la cible il doit pouvoir demander des pr√©cisions pour affiner, corriger les r√©sultats ‚Äì par exemple en cas de deux soci√©t√©s homonymes ou de march√©s proche"
   >
2. **Impl√©mentation dans le prompt** :

   ```
   4. CAS LIMITES :
      - Si vous ne trouvez pas l'information : "Les donn√©es disponibles 
        ne permettent pas de r√©pondre. Sources consult√©es : [liste]. 
        Je recommande [action]."
      - Si une entreprise est ambigu√´ : "J'ai identifi√© plusieurs 
        entreprises nomm√©es [X]. Pouvez-vous pr√©ciser : secteur, 
        g√©ographie, ou autre contexte ?"
   ```
3. **Gestion donn√©es payantes** :

   ```python
   if 'paywall' in doc.metadata:
       response += "\n‚ö†Ô∏è  Cette information n√©cessite un acc√®s payant."
   ```

---

## üìà Roadmap & Extensibilit√©

### D√©cision : Architecture Modulaire pour Ajout Futur de Sources

**Choix retenu** : Design pattern "Plugin"

**Justification** :

1. **Selon vos notes (hackathon KPMG.pdf)** :

   > "L'objectif est de b√¢tir un mod√®le capable d'effectuer des recherches sur diff√©rents √©l√©ments pr√©-d√©finis"
   >
2. **Architecture extensible** :

   ```python
   # Ajouter une nouvelle source = cr√©er une fonction
   def load_crunchbase_data(company: str) -> List[Document]:
       # Impl√©mentation
       pass

   # L'ajouter au pipeline
   all_documents["startups"].extend(load_crunchbase_data("Stripe"))
   ```
3. **Namespaces pr√©vus** :

   - ‚úÖ `financial_reports` (SEC)
   - ‚úÖ `news` (NewsAPI, presse)
   - ‚úÖ `macro_data` (yfinance)
   - üîú `startups` (Crunchbase)
   - üîú `social_signals` (Reddit, Twitter)
4. **R√©f√©rence vos notes (KPMG v2.pdf)** :

   > "3 mois : Pilote avec 2 √©quipes KPMG. 6 mois : Productionnalisation compl√®te + 10 secteurs verticaux"
   >

---

## üé® Choix UI : Gradio vs. Streamlit

### D√©cision : Gradio (Recommand√©)

**Choix retenu** : Gradio ChatInterface

**Justification** :

1. **Selon votre code existant** :

   ```python
   demo = gr.ChatInterface(
       fn=chat_response,
       title="Veilleur strat√©gique KPMG",
       share=True  # Lien public 72h pour jury
   )
   ```
2. **Avantages Gradio** :

   - ‚úÖ D√©mo instantan√©e avec `share=True`
   - ‚úÖ Interface chat native
   - ‚úÖ Int√©grable dans syst√®mes KPMG (iframe)
   - ‚úÖ Moins de code que Streamlit
3. **R√©f√©rence Gradio** :

   > "ChatInterface is a high-level abstraction that allows you to create chatbot UIs with minimal code."
   >

   Source : https://www.gradio.app/docs/chatinterface

---

## üìö R√©f√©rences Crois√©es

### Documentation Consult√©e

‚úÖ **LangChain** :

- Document Loaders : https://python.langchain.com/docs/modules/data_connection/document_loaders/
- Text Splitters : https://python.langchain.com/docs/modules/data_connection/document_transformers/
- Vector Stores : https://python.langchain.com/docs/modules/data_connection/vectorstores/
- RAG Tutorial : https://python.langchain.com/docs/tutorials/rag/

‚úÖ **Pinecone** :

- Upsert Data : https://docs.pinecone.io/docs/upsert-data
- Namespaces : https://docs.pinecone.io/docs/namespaces
- Python Client : https://docs.pinecone.io/docs/python-client

‚úÖ **Mistral AI** :

- Models : https://docs.mistral.ai/getting-started/models/
- Embeddings : https://docs.mistral.ai/capabilities/embeddings/
- Prompting : https://docs.mistral.ai/guides/prompting_capabilities/

‚úÖ **Vos Notes Projet** :

- hackathon KPMG (1).pdf : Exigences clients, cas d'usage
- KPMG v2 (1).pdf : Best practices RAG, chunking, m√©triques

---

## ‚úÖ Checklist de Conformit√© KPMG

- [X] Citations obligatoires avec source, fiabilit√©, date
- [X] Gestion des cas limites (ambigu√Øt√©, donn√©es manquantes)
- [X] Isolation des sources par namespaces
- [X] Logs d'audit pour tra√ßabilit√©
- [X] R√©ponses en prose (pas de bullet points par d√©faut)
- [X] M√©triques de validation (Hit Rate, Pr√©cision, Satisfaction)
- [X] Architecture extensible (ajout de sources facilit√©)
- [X] Conformit√© RGPD (pas de donn√©es sensibles index√©es)

---

**Conclusion** : Chaque choix technique de ce syst√®me est justifi√© par :

1. Les exigences explicites du Hackathon KPMG
2. Les best practices document√©es
3. Les contraintes mat√©rielles et budg√©taires identifi√©es
4. L'√©tat de l'art en RAG et veille strat√©gique

Cette architecture garantit un syst√®me **robuste, extensible et conforme** aux standards KPMG.
