{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Début du chargement ---\n",
      "PDF chargé : 119 pages\n",
      "URL chargée : https://fr.wikipedia.org/wiki/Espace_de_Banach\n",
      "Nombre total de fragments créés : 252\n",
      "\n",
      "Exemple de fragment :\n",
      "THERMODYNAMIQUE\n",
      "J. SEBILLEAU\n",
      "1...\n"
     ]
    }
   ],
   "source": [
    "#ingestion.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#Text Splitters are classes for splitting text.\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def charger_donnees():\n",
    "    print(\"--- Début du chargement ---\")\n",
    "    \n",
    "    # A. Chargement du PDF (Cours de Maths)\n",
    "    # Assurez-vous que le fichier est dans le même dossier\n",
    "    pdf_path = \"Poly-Thermo-v1.pdf\" \n",
    "    if os.path.exists(pdf_path):\n",
    "        loader_pdf = PyPDFLoader(pdf_path)\n",
    "        docs_pdf = loader_pdf.load()\n",
    "        print(f\"PDF chargé : {len(docs_pdf)} pages\")\n",
    "    else:\n",
    "        print(\"Fichier PDF non trouvé, saut de cette étape.\")\n",
    "        docs_pdf = []\n",
    "\n",
    "    # B. Chargement de Wikipédia (Espace de Banach)\n",
    "    url = \"https://fr.wikipedia.org/wiki/Espace_de_Banach\"\n",
    "    loader_web = WebBaseLoader(url)\n",
    "    docs_web = loader_web.load()\n",
    "    print(f\"URL chargée : {url}\")\n",
    "\n",
    "    # Fusion des documents\n",
    "    tous_les_docs = docs_pdf + docs_web\n",
    "    \n",
    "\n",
    "    # C. Découpage (Chunking)\n",
    "    # On utilise des chunks de 1000 caractères avec un recouvrement \n",
    "    # pour ne pas couper les phrases au milieu.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=150,\n",
    "        add_start_index=True # Important pour citer la source précise\n",
    "    )\n",
    "    \n",
    "    fragments = text_splitter.split_documents(tous_les_docs)\n",
    "    print(f\"Nombre total de fragments créés : {len(fragments)}\")\n",
    "    \n",
    "    return fragments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fragments = charger_donnees()\n",
    "    # Test : affiche le contenu du premier fragment\n",
    "    if fragments:\n",
    "        print(\"\\nExemple de fragment :\")\n",
    "        print(fragments[0].page_content[:200] + \"...\")\n",
    "\n",
    "''' \n",
    "Le résumé de cette partie de script:\n",
    "\n",
    "Le bouton \"ON/OFF\" automatique :\n",
    "\n",
    "ON : Quand tu lances python ingestion.py, l'assertion est vraie. Le script s'exécute, \n",
    "charge le PDF et Wikipédia, et affiche tes tests.\n",
    "\n",
    "OFF : Quand plus tard tu utiliseras ce fichier comme une \"bibliothèque\" \n",
    "(en faisant import ingestion dans un autre script), l'assertion sera fausse. \n",
    "Le script ne se lancera pas tout seul.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envoi des vecteurs vers Pinecone... (cela peut prendre 1-2 min)\n",
      "Indexation terminée !\n"
     ]
    }
   ],
   "source": [
    "#vectorisation.py\n",
    "\n",
    "\n",
    "#----- J'en suis ici pour lacomprehension du code ( APPROFONDIR AU MAX !!)\n",
    "#----- A CL CL A CHAQUE FOIS LA OU JE M'ARRETE\n",
    "#Puis donner les bonnes e,trées pour la veille ( sites, api etc... N8N eventuellement )\n",
    "#Finir par un beau front avec des KPIS et des graphs\n",
    "# Ne pas oublier de faire des test avec des kpi's pour la prez\n",
    "# Globalement a la fin reprendre toutes les notes ( les 2 pdf typst et en faire un gros pour ne rien oublier de ce qui à été fait)\n",
    "\n",
    "\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Initialisation des Embeddings (Le traducteur texte -> chiffres)\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "index_name = \"kpmg-veille\"\n",
    "\n",
    "# Envoi vers Pinecone\n",
    "print(\"Envoi des vecteurs vers Pinecone... (cela peut prendre 1-2 min)\")\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    fragments, \n",
    "    embeddings, \n",
    "    index_name=index_name\n",
    ")\n",
    "print(\"Indexation terminée !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Recherche en cours ---\n",
      "D'après les extraits fournis du document **Poly-Thermo-v1.pdf**, la **thermodynamique statistique** peut être définie comme suit :\n",
      "\n",
      "### **Définition et essence**\n",
      "La thermodynamique statistique est une approche théorique qui **axiomatise le comportement statistique des molécules** constituant un système macroscopique (comme un gaz). Contrairement à la **thermodynamique classique** (empirique, basée sur l'expérience), elle part des **propriétés microscopiques** (mouvements, interactions des particules) pour en déduire les **grandeurs macroscopiques** (température, pression, entropie, etc.).\n",
      "\n",
      "### **Principe clé**\n",
      "- **Traitement statistique** : Grâce au **très grand nombre de molécules** (ex. : gaz avec *N* atomes), on peut décrire le système via un **petit nombre de grandeurs pertinentes** (ex. : vitesse moyenne, énergie cinétique), dont les **fluctuations statistiques sont négligeables**.\n",
      "- **Lien micro ↔ macro** : Elle explique les lois thermodynamiques (comme le **second principe**) en les reliant aux **comportements probabilistes des particules**, ce qui donne un **sens physique** aux concepts autrement \"mystérieux\" en thermodynamique classique.\n",
      "\n",
      "### **Exemple concret (gaz monoatomique)**\n",
      "Dans le cas d'un gaz homogène au repos (extrait cité) :\n",
      "- Les atomes ont des **vitesses individuelles non nulles** (→ui), mais leur **moyenne statistique est nulle** (<→ui> = 0), ce qui correspond à un **état macroscopique de repos**.\n",
      "- On étudie des quantités comme **∑ →ri · →pi** (somme des produits position-quantité de mouvement) pour relier les échelles microscopique et macroscopique.\n",
      "\n",
      "### **Différence avec la thermodynamique classique**\n",
      "| **Thermodynamique classique** | **Thermodynamique statistique** |\n",
      "|--------------------------------|----------------------------------|\n",
      "| Basée sur l'expérience (lois empiriques). | Basée sur des modèles microscopiques (mécanique statistique). |\n",
      "| Concepts comme l'entropie peuvent sembler abstraits. | L'entropie est interprétée via le **désordre microscopique** (ex. : nombre d'états accessibles). |\n",
      "| Lois comme le second principe sont postulées. | Ces lois émergent des **propriétés statistiques des particules**. |\n",
      "\n",
      "---\n",
      "**Source** : *Poly-Thermo-v1.pdf* (pages non numérotées dans les extraits, mais contenu issu des sections sur l'approche statistique de la température et la comparaison avec la thermodynamique classique).\n",
      "\n",
      "---\n",
      "**Remarque** :\n",
      "Les extraits ne couvrent pas *tous* les aspects (ex. : fonction de partition, ensemble canonique), mais ils soulignent l'**essence** de la discipline : **relier le microscopique au macroscopique via des outils statistiques**. Pour une définition complète, une source comme [Wikipédia](https://fr.wikipedia.org/wiki/Thermodynamique_statistique) ou un manuel de référence (ex. : *Statistical Thermodynamics* de McQuarrie) serait utile.\n"
     ]
    }
   ],
   "source": [
    "#chat_kpmg.py\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Connexion à l'index existant sur Pinecone\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "index_name = \"kpmg-veille\"\n",
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # On récupère les 3 meilleurs extraits\n",
    "\n",
    "# 2. Configuration du \"Cerveau\" (Mistral)\n",
    "llm = ChatMistralAI(model=\"mistral-medium\", temperature=0)\n",
    "\n",
    "# 3. Création du Prompt (Template) adapté aux exigences KPMG\n",
    "template = \"\"\"Vous êtes l'Assistant Intelligent de Veille Stratégique pour KPMG. \n",
    "Utilisez les extraits suivants pour répondre à la question. \n",
    "Si vous ne connaissez pas la réponse, dites-le. \n",
    "Citez TOUJOURS la source (ex: Page X du PDF ou URL Wikipédia).\n",
    "\n",
    "CONTEXTE:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "RÉPONSE ANALYTIQUE:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 4. Construction de la Chaîne LCEL\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"Source: {d.metadata.get('source')} - Contenu: {d.page_content}\" for d in docs])\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 5. Test en conditions réelles\n",
    "if __name__ == \"__main__\":\n",
    "    question = \"qu'est ce que la Thermodynamique statistique ? \"\n",
    "    print(\"--- Recherche en cours ---\")\n",
    "    print(chain.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hobby/Desktop/Python/ML_projects/langchain_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://3c9485f389e4871fff.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3c9485f389e4871fff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lire la documentation gradio : https://www.gradio.app/docs/gradio/chatinterface\n",
    "#Plein de trucs a apprendre pour ameliorer le GUI\n",
    "# Avantage ca s'incorpore dans des systèmes KPMG types web app etc avec gradio \n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- INITIALISATION RAG ---\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "vectorstore = PineconeVectorStore(index_name=\"kpmg-veille\", embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = ChatMistralAI(model=\"mistral-medium\", temperature=0)\n",
    "\n",
    "template = \"\"\"Vous êtes l'Expert Veille KPMG. Répondez à la question selon le contexte.\n",
    "Citez les sources (ex: Page PDF ou URL).\n",
    "CONTEXTE: {context}\n",
    "QUESTION: {question}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"Source: {d.metadata.get('source')} - {d.page_content}\" for d in docs])\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- FONCTION DE CHAT ---\n",
    "def chat_response(message, history):\n",
    "    try:\n",
    "        response = chain.invoke(message)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Erreur technique : {str(e)}\"\n",
    "\n",
    "# --- INTERFACE GRADIO ---\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat_response,\n",
    "    title=\"Veilleur stratégique KPMG  \",\n",
    "    description=\"Assistant RAG pour l'analyse de marché et la veille stratégique.\",\n",
    "    examples=[\"Qu'est-ce qu'un espace de Banach ?\", \"Quelles sont les tendances du marché ?\"]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True) # share=True crée un lien public de 72h pour ton jury !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
