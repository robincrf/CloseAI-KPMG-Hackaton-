{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is working correctly!\n",
      "Created 2 messages\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hello!\")\n",
    "]\n",
    "\n",
    "print(\"LangChain is working correctly!\")\n",
    "print(f\"Created {len(messages)} messages\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain a été divisé en plusieurs morceaux pour être plus léger :\n",
    "\n",
    "langchain-core : Contient les schémas de base (messages, documents).\n",
    "\n",
    "langchain-community : Contient les intégrations tierces.\n",
    "\n",
    "langchain : Contient les chaînes et la logique de haut niveau."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une maintenance adéquate est essentielle pour garantir un environnement stable et efficace après une installation réussie de LangChain. Si l'installation constitue le point de départ, le maintien de la stabilité implique une gestion rigoureuse des dépendances et un contrôle rigoureux des versions. L'alignement des packages principaux et optionnels permet d'éviter les conflits potentiels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.2.0\n",
      "Chain result: TRANSFORM THIS TEXT: HELLO WORLD\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_core.output_parsers import BaseOutputParser \n",
    "\n",
    "# 1. Définition du Parser\n",
    "class SimpleParser(BaseOutputParser):\n",
    "    def parse(self, text: str) -> str:\n",
    "        # On s'assure que text est bien une chaîne\n",
    "        return str(text).strip().upper()\n",
    "\n",
    "# 2. Définition du Prompt\n",
    "template = \"Transform this text: {input_text}\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "parser = SimpleParser()\n",
    "\n",
    "# 3. La Chaîne (Modifiée pour le test)\n",
    "# Pour que ça marche SANS modèle, on doit ajouter une étape qui extrait le texte du prompt\n",
    "chain = prompt | (lambda x: x.to_string()) | parser \n",
    "\n",
    "# 4. Exécution\n",
    "result = chain.invoke({\"input_text\": \"hello world\"})\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "print(f\"Chain result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No API key found - check your .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(f\"API key loaded: {api_key[:8]}...\")\n",
    "else:\n",
    "    print(\"No API key found - check your .env file\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- se rensigner : Pour la gestion des documents, ajoutez des outils tels que pypdf pour les PDF et beautifulsoup4 pour le scraping Web.\n",
    "\n",
    "- Ensuite, choisissez une base de données vectorielle. Pour les tests locaux, Chroma Il s'agit d'une option simple nécessitant une configuration minimale. Pour une production à plus grande échelle, privilégiez les bases de données offrant des performances supérieures, même si elles peuvent nécessiter des configurations d'API supplémentaires.\n",
    "\n",
    "- Vous aurez également besoin de clés API pour activer les fonctionnalités clés. Obtenez une clé API OpenAI pour accéder aux intégrations telles que text-embedding-ada-002 et des modèles tels que gpt-4 or gpt-3.5-turbo. Stockez ces clés en toute sécurité à l'aide de variables d'environnement ou d'outils tels que AWS Secrets Manager.\n",
    " \n",
    "- Commencez par sélectionner les outils appropriés pour charger vos documents. Par exemple, PyPDFLoader gère les fichiers PDF tout en conservant leur formatage, et WebBaseLoader peut extraire le contenu des sites Web avec des options d'analyse flexibles.\n",
    "\n",
    "- Une fois chargé, divisez le texte en morceaux gérables pour améliorer la précision de la récupération. RecursiveCharacterTextSplitter est un outil polyvalent, offrant un équilibre entre la taille des blocs et leur chevauchement. Par exemple, des blocs plus petits de 500 à 800 caractères conviennent parfaitement aux FAQ, tandis que des blocs plus grands de 1,500 2,000 à XNUMX XNUMX caractères sont plus adaptés aux documents techniques. Une fois le texte préparé, vous pouvez passer à la génération d’intégrations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.document_loaders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m PyPDFLoader\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext_splitter\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      4\u001b[0m loader \u001b[39m=\u001b[39m PyPDFLoader(\u001b[39m\"\u001b[39m\u001b[39mdocument.pdf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain.document_loaders'"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"document.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\", \"\", \" \", \"\"]\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les incorporations convertissent des fragments de texte en représentations numériques qui capturent leur signification. text-embedding-ada-002 Le modèle est un choix fiable, générant des vecteurs de 1,536 XNUMX dimensions adaptés à divers contenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "j'aimerais bien le faire avec pinecone :\n",
    "'''\n",
    "Voici comment générer et stocker des intégrations à l'aide de Chroma :\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le processus de recherche identifie les fragments de documents les plus pertinents pour une requête. L'utilisation d'un outil de recherche par similarité avec k=4 récupère les quatre premiers morceaux, en équilibrant les détails et les limites d'entrée pour le modèle de langage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ingénierie des invites est un autre aspect essentiel. Une invite bien conçue garantit que le modèle de langage utilise efficacement le contexte récupéré. Par exemple :\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the provided context. If the context doesn't contain relevant information, say so clearly.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les besoins avancés, des techniques telles que la récupération multi-requêtes ou les méthodes hybrides (combinant similarité sémantique et correspondance de mots-clés) peuvent améliorer les résultats, en particulier pour le contenu technique.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construire le système RAG complet\n",
    "L'étape finale consiste à intégrer tous les composants dans un système RAG unifié. create_retrieval_chain La fonction simplifie cela en coordonnant la récupération et la génération.\n",
    "\n",
    "Voici un exemple :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un de mes problemes est les clé API payante, comment faire ? Je crois que avec mistral il y a des requetes gratuites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\": \"Your question here\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERET N8N : L'amélioration des performances de LangChain RAG (Retrieval-Augmented Generation) implique d'affiner les paramètres de recherche et les méthodes de recherche afin de garantir des réponses précises et contextuelles. L'utilisation de techniques de recherche intelligentes et l'optimisation de la configuration des blocs permettent d'améliorer considérablement l'efficacité du système."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combiner des méthodes de recherche, telles que les approches sémantiques et par mots-clés, peut améliorer la précision dans des domaines spécialisés. Voici un exemple de configuration d'un outil de recherche hybride :\n",
    "\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vectorstore.as_retriever(), bm25_retriever],\n",
    "    weights=[0.6, 0.4]\n",
    ")\n",
    "\n",
    "De plus, les techniques d’expansion de requêtes, comme la récupération multi-requêtes, peuvent générer une formulation alternative pour capturer un contexte plus large et réduire l’impact des requêtes mal formulées.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEASKO LA TEMPERTATURE ??? : Les réglages de température jouent également un rôle essentiel dans la qualité du rendu. Pour les tâches factuelles, des valeurs de température plus basses permettent de minimiser les hallucinations, tandis que des valeurs légèrement plus élevées sont plus adaptées aux tâches exigeant créativité ou flexibilité.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déploiement de production\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour améliorer l'efficacité, implémentez des couches de mise en cache pour les documents fréquemment consultés. Des outils comme Redis or Memcached Vous pouvez stocker les résultats d'intégration pour les requêtes courantes, réduisant ainsi la charge de vos services d'intégration. Définissez des valeurs de durée de vie (TTL) selon que vos données sont statiques ou fréquemment mises à jour.\n",
    "\n",
    "- Pour les applications à fort trafic, répartissez la charge sur plusieurs points de terminaison d'API d'intégration afin d'éviter toute limitation du débit. Vous pouvez également envisager d'utiliser des modèles d'intégration locaux pour maintenir des performances constantes en cas de forte demande\n",
    "  \n",
    "-\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sécurité et conformité des données\n",
    "\n",
    "- Le contrôle d'accès dans les systèmes RAG peut être complexe, car les utilisateurs accèdent indirectement aux informations via les réponses de l'IA. Mettez en œuvre des autorisations au niveau des documents en étiquetant les fragments de documents avec des métadonnées et en filtrant les résultats de récupération en fonction des rôles des utilisateurs avant le traitement. ( pas forcement besoin ici car veille strategique )\n",
    " \n",
    "\n",
    "- Les politiques de conservation des données doivent prendre en compte à la fois les documents sources et les intégrations générées. Des réglementations comme le RGPD peuvent exiger des mécanismes de suppression de données utilisateur spécifiques des bases de données vectorielles. Prévoyez donc la suppression complète des données dès le départ.\n",
    "\n",
    "- Les journaux d'audit sont essentiels à la conformité et à la sécurité. Ils doivent enregistrer des informations clés telles que les identifiants des utilisateurs, les horodatages, les modèles de requête, les documents récupérés et les réponses générées. Assurez-vous de minimiser l'exposition des données sensibles tout en conservant suffisamment de détails pour les rapports de conformité et la détection des fuites de données potentielles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donné que votre Mac est un modèle de 2012, nous allons coder de manière à ce qu'il ne sature pas sa mémoire vive (8 Go) en traitant les documents par petits morceaux."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
