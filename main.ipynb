{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"DEF d'un namespace = un dossier dans une base\\n\\nindex: veille-strategique\\nNAMESPACES (isolation par type de données) :\\n├── namespace: financial_reports\\n│   └── SEC Edgar, SIRENE, rapports d'entreprises\\n├── namespace: news\\n│   └── NewsAPI, Google News RSS, communiqués de presse\\n├── namespace: macro_data\\n│   └── FRED, yfinance, DBnomics, Adzuna\\n├── namespace: social_signals\\n│   └── Bluesky, Reddit (futur), Twitter (futur)\\n├── namespace: web_quarantine\\n│   └── Recherches web dynamiques (RAG agentic)\\n├── namespace: startups\\n│   └── bluesky, crunchbase ...\\n└── namespace: facts\\n    └── Facts structurés (KPIs, deals, market size)\\n\\nBUT : Retrieval ciblé, Éviter le bruit, Prompting plus intelligent (« Réponds uniquement à partir des données issues du namespace financial_reports et cite les sources »)Ça réduit les hallucinations, Scalabilité propre :\\n\\n-Tu peux ajouter de nouvelles sources sans casser l’existant\\n\\n-Tu peux purger un namespace sans toucher aux autres\\n\\n-Tu peux tester des sources expérimentales (social_signals) sans polluer le corpus principal\\n\\n, \\n1 index = 1 projet RAG\\nN namespaces = N types de sources\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''DEF d'un namespace = un dossier dans une base\n",
    "\n",
    "index: veille-strategique\n",
    "NAMESPACES (isolation par type de données) :\n",
    "├── namespace: financial_reports\n",
    "│   └── SEC Edgar, SIRENE, rapports d'entreprises\n",
    "├── namespace: news\n",
    "│   └── NewsAPI, Google News RSS, communiqués de presse\n",
    "├── namespace: macro_data\n",
    "│   └── FRED, yfinance, DBnomics, Adzuna\n",
    "├── namespace: social_signals\n",
    "│   └── Bluesky, Reddit (futur), Twitter (futur)\n",
    "├── namespace: web_quarantine\n",
    "│   └── Recherches web dynamiques (RAG agentic)\n",
    "├── namespace: startups\n",
    "│   └── bluesky, crunchbase ...\n",
    "└── namespace: facts\n",
    "    └── Facts structurés (KPIs, deals, market size)\n",
    "\n",
    "BUT : Retrieval ciblé, Éviter le bruit, Prompting plus intelligent (« Réponds uniquement à partir des données issues du namespace financial_reports et cite les sources »)Ça réduit les hallucinations, Scalabilité propre :\n",
    "\n",
    "-Tu peux ajouter de nouvelles sources sans casser l’existant\n",
    "\n",
    "-Tu peux purger un namespace sans toucher aux autres\n",
    "\n",
    "-Tu peux tester des sources expérimentales (social_signals) sans polluer le corpus principal\n",
    "\n",
    ", \n",
    "1 index = 1 projet RAG\n",
    "N namespaces = N types de sources\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index nettoyé\n"
     ]
    }
   ],
   "source": [
    "# Clean l'index ( ATTENTION A NE PAS EXECUTER A CHAQUZ FOIS !!)\n",
    "\n",
    "\n",
    "'''\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key)\n",
    "INDEX_NAME = \"kpmg-veille\"\n",
    "\n",
    "# Supprimer TOUS les vecteurs de TOUS les namespaces\n",
    "index = pc.Index(INDEX_NAME)\n",
    "stats = index.describe_index_stats()\n",
    "\n",
    "for namespace in stats.namespaces.keys():\n",
    "    print(f\" Suppression du namespace '{namespace}'...\")\n",
    "    index.delete(delete_all=True, namespace=namespace)\n",
    "\n",
    "print(\"Index nettoyé\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Variables d'environnement chargées\n",
      "Client Pinecone initialisé\n",
      " Index 'kpmg-veille' détecté. Suppression en cours...\n",
      " Index supprimé avec succès\n",
      " Création de l'index 'kpmg-veille'...\n",
      " Index créé et opérationnel\n",
      "\n",
      "============================================================\n",
      " CONFIGURATION DE L'INDEX\n",
      "============================================================\n",
      "Nom : kpmg-veille\n",
      "Dimension : 1024 (Mistral-embed)\n",
      "Métrique : cosine\n",
      "Vecteurs totaux : 0\n",
      "\n",
      " Namespaces définis :\n",
      "   - financial_reports\n",
      "   - news\n",
      "   - macro_data\n",
      "   - startups\n",
      "   - social_signals\n",
      "   - web_quarantine\n",
      "   - facts\n",
      "============================================================\n",
      "\n",
      " Index prêt pour l'ingestion de données\n",
      "\n",
      " CHECKLIST COMPLÉTÉE :\n",
      "   ☑ Index existant supprimé\n",
      "   ☑ Nouvel index créé avec dimension 1024\n",
      "   ☑ Métrique cosine configurée\n",
      "   ☑ Serverless spec activé\n",
      "   ☑ Namespaces documentés\n",
      "\n",
      " Prêt pour l'ingestion (Notebook 2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTEBOOK 1 : Configuration et Nettoyage Pinecone\n",
    "================================================\n",
    "\n",
    "OBJECTIF : Réinitialiser complètement l'environnement vectoriel\n",
    "           et créer une architecture propre avec namespaces.\n",
    "\n",
    "RÉFÉRENCES :\n",
    "- Pinecone Docs : https://docs.pinecone.io/docs/python-client\n",
    "- LangChain Pinecone : https://python.langchain.com/docs/integrations/vectorstores/pinecone\n",
    "\n",
    "MÉTHODOLOGIE :\n",
    "1. Supprimer l'index existant (stratégie Option A validée)\n",
    "2. Recréer un index optimisé pour Mistral embeddings (dimension 1024)\n",
    "3. Valider la structure des namespaces\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 1 : CHARGEMENT DES VARIABLES D'ENVIRONNEMENT\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "#PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east-1\")\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\" PINECONE_API_KEY manquante dans .env\")\n",
    "\n",
    "print(\" Variables d'environnement chargées\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 2 : INITIALISATION CLIENT PINECONE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "JUSTIFICATION : \n",
    "Pinecone v3+ utilise une nouvelle API avec ServerlessSpec.\n",
    "Cela permet une scalabilité automatique sans gérer de pods.\n",
    "\n",
    "Référence : https://docs.pinecone.io/docs/new-api\n",
    "\"\"\"\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "print(\"Client Pinecone initialisé\")\n",
    "\n",
    "\n",
    "# NE PAS EXECUTER, UNIQUEMENT SI ON VEUT REPARTIR DE 0.\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 3 : SUPPRESSION DE L'INDEX EXISTANT\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\"\n",
    " Suppression totale\n",
    "\n",
    "POURQUOI ?\n",
    "- Garantit un environnement propre sans données parasites\n",
    "- Évite les conflits de dimension d'embeddings\n",
    "- Permet de repartir sur des métadonnées structurées\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "INDEX_NAME = \"kpmg-veille\"\n",
    "\n",
    "\n",
    "def clean_pinecone_index():\n",
    "    \"\"\"Supprime l'index existant s'il existe\"\"\"\n",
    "    try:\n",
    "        existing_indexes = [idx.name for idx in pc.list_indexes()]\n",
    "        \n",
    "        if INDEX_NAME in existing_indexes:\n",
    "            print(f\" Index '{INDEX_NAME}' détecté. Suppression en cours...\")\n",
    "            pc.delete_index(INDEX_NAME)\n",
    "            \n",
    "            # Attendre la suppression complète (bonnes pratiques Pinecone)\n",
    "            while INDEX_NAME in [idx.name for idx in pc.list_indexes()]:\n",
    "                print(\"    Attente de la suppression...\")\n",
    "                time.sleep(2)\n",
    "            \n",
    "            print(\" Index supprimé avec succès\")\n",
    "        else:\n",
    "            print(f\" Aucun index '{INDEX_NAME}' trouvé (normal si 1ère exécution)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors du nettoyage : {e}\")\n",
    "        raise\n",
    "\n",
    "clean_pinecone_index()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 4 : CRÉATION DU NOUVEL INDEX\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION OPTIMALE POUR MISTRAL EMBEDDINGS\n",
    "\n",
    "1. DIMENSION : 1024\n",
    "   - Mistral-embed génère des vecteurs de 1024 dimensions\n",
    "   - Référence : https://docs.mistral.ai/capabilities/embeddings/\n",
    "\n",
    "2. MÉTRIQUE : cosine\n",
    "   - Standard pour la similarité sémantique\n",
    "   - Recommandée par LangChain pour les embeddings textuels\n",
    "   - Référence : https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
    "\n",
    "3. SERVERLESS SPEC :\n",
    "   - Cloud 'aws', Région 'us-east-1'\n",
    "   - Scalabilité automatique (critique pour la veille en production)\n",
    "   - Pas de gestion de pods\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_optimized_index():\n",
    "    \"\"\"Crée un index Pinecone optimisé pour la veille stratégique\"\"\"\n",
    "    try:\n",
    "        print(f\" Création de l'index '{INDEX_NAME}'...\")\n",
    "        \n",
    "        pc.create_index(\n",
    "            name=INDEX_NAME,\n",
    "            dimension=1024,  # Dimension Mistral-embed\n",
    "            metric=\"cosine\",  # Similarité sémantique\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"  # Utiliser votre région Pinecone\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Attendre que l'index soit prêt\n",
    "        while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "            print(\"    Initialisation de l'index...\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "        print(\" Index créé et opérationnel\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur lors de la création : {e}\")\n",
    "        raise\n",
    "\n",
    "create_optimized_index()\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 5 : VALIDATION DE LA STRUCTURE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "ARCHITECTURE DES NAMESPACES\n",
    "\n",
    "Chaque namespace correspond à un type de source de données :\n",
    "- financial_reports : SEC EDGAR, rapports annuels\n",
    "- news : NewsAPI, communiqués de presse\n",
    "- startups : Crunchbase (futur)\n",
    "- macro_data : yfinance, données économiques\n",
    "- social_signals : Reddit, Twitter (futur)\n",
    "\n",
    "AVANTAGES :\n",
    "✓ Isolation des sources pour des requêtes ciblées\n",
    "✓ Possibilité de filtrer par namespace lors du retrieval\n",
    "✓ Gestion indépendante du cycle de vie des données\n",
    "✓ Facilite le debugging et les audits\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "NAMESPACES = [\n",
    "    \"financial_reports\",  # SEC EDGAR, SIRENE\n",
    "    \"news\",               # NewsAPI, RSS, Press Releases\n",
    "    \"macro_data\",         # yfinance, FRED, DBnomics, Adzuna\n",
    "    \"startups\",           # Crunchbase (À IMPLÉMENTER)\n",
    "    \"social_signals\",     # Bluesky (+ Reddit/Twitter futur)\n",
    "    \"web_quarantine\",     # Recherches web RAG agentic\n",
    "    \"facts\"              # Facts structurés FACT-RAG\n",
    "]\n",
    "\n",
    "def validate_index_structure():\n",
    "    \"\"\"Affiche la configuration de l'index\"\"\"\n",
    "    try:\n",
    "        index_stats = pc.Index(INDEX_NAME).describe_index_stats()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\" CONFIGURATION DE L'INDEX\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Nom : {INDEX_NAME}\")\n",
    "        print(f\"Dimension : 1024 (Mistral-embed)\")\n",
    "        print(f\"Métrique : cosine\")\n",
    "        print(f\"Vecteurs totaux : {index_stats.get('total_vector_count', 0)}\")\n",
    "        print(f\"\\n Namespaces définis :\")\n",
    "        for ns in NAMESPACES:\n",
    "            print(f\"   - {ns}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        print(\" Index prêt pour l'ingestion de données\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur de validation : {e}\")\n",
    "        raise\n",
    "\n",
    "validate_index_structure()\n",
    "\n",
    "'''\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 6 : CHECKLIST DE VALIDATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n CHECKLIST COMPLÉTÉE :\")\n",
    "print(\"   ☑ Index existant supprimé\")\n",
    "print(\"   ☑ Nouvel index créé avec dimension 1024\")\n",
    "print(\"   ☑ Métrique cosine configurée\")\n",
    "print(\"   ☑ Serverless spec activé\")\n",
    "print(\"   ☑ Namespaces documentés\")\n",
    "print(\"\\n Prêt pour l'ingestion (Notebook 2)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration des sources initialisée\n",
      "\n",
      "============================================================\n",
      " DÉMARRAGE DE L'INGESTION MULTI-SOURCES\n",
      "============================================================\n",
      "\n",
      " Chargement SEC EDGAR...\n",
      "    Métadonnées : Apple Inc.\n",
      "[2026-01-29T17:13:01.704521] SEC_EDGAR - INFO : Requête pour CIK 0000320193\n",
      "[2026-01-29T17:13:02.185533] SEC_EDGAR - SUCCESS : 1 documents trouvés pour Apple Inc.\n",
      "    Métadonnées : Microsoft Corp.\n",
      "[2026-01-29T17:13:03.190284] SEC_EDGAR - INFO : Requête pour CIK 0000789019\n",
      "[2026-01-29T17:13:04.535218] SEC_EDGAR - SUCCESS : 1 documents trouvés pour MICROSOFT CORP\n",
      "    Métadonnées : Alphabet Inc. (Google)\n",
      "[2026-01-29T17:13:05.635522] SEC_EDGAR - INFO : Requête pour CIK 0001652044\n",
      "[2026-01-29T17:13:06.188902] SEC_EDGAR - SUCCESS : 1 documents trouvés pour Alphabet Inc.\n",
      "    Métadonnées : Amazon.com Inc.\n",
      "[2026-01-29T17:13:07.198172] SEC_EDGAR - INFO : Requête pour CIK 0001018724\n",
      "[2026-01-29T17:13:07.813886] SEC_EDGAR - SUCCESS : 1 documents trouvés pour AMAZON COM INC\n",
      "    Métadonnées : Meta Platforms Inc.\n",
      "[2026-01-29T17:13:08.821764] SEC_EDGAR - INFO : Requête pour CIK 0001326801\n",
      "[2026-01-29T17:13:09.282833] SEC_EDGAR - SUCCESS : 1 documents trouvés pour Meta Platforms, Inc.\n",
      "    Métadonnées : NVIDIA Corp.\n",
      "[2026-01-29T17:13:10.287466] SEC_EDGAR - INFO : Requête pour CIK 0001045810\n",
      "[2026-01-29T17:13:10.816402] SEC_EDGAR - SUCCESS : 1 documents trouvés pour NVIDIA CORP\n",
      "    Métadonnées : Oracle Corp.\n",
      "[2026-01-29T17:13:11.954906] SEC_EDGAR - INFO : Requête pour CIK 0001341439\n",
      "[2026-01-29T17:13:12.361616] SEC_EDGAR - SUCCESS : 1 documents trouvés pour ORACLE CORP\n",
      "    Métadonnées : IBM Corp.\n",
      "[2026-01-29T17:13:13.371042] SEC_EDGAR - INFO : Requête pour CIK 0000051143\n",
      "[2026-01-29T17:13:13.756090] SEC_EDGAR - SUCCESS : 1 documents trouvés pour INTERNATIONAL BUSINESS MACHINES CORP\n",
      "    Métadonnées : JPMorgan Chase & Co.\n",
      "[2026-01-29T17:13:14.762157] SEC_EDGAR - INFO : Requête pour CIK 0000019617\n",
      "[2026-01-29T17:13:16.409315] SEC_EDGAR - SUCCESS : 1 documents trouvés pour JPMORGAN CHASE & CO\n",
      "    Métadonnées : PayPal Holdings\n",
      "[2026-01-29T17:13:17.433499] SEC_EDGAR - INFO : Requête pour CIK 0001885998\n",
      "[2026-01-29T17:13:17.862919] SEC_EDGAR - SUCCESS : 1 documents trouvés pour Roth CH Acquisition V Co.\n",
      "    Métadonnées : Morgan Stanley\n",
      "[2026-01-29T17:13:18.872856] SEC_EDGAR - INFO : Requête pour CIK 0000063276\n",
      "[2026-01-29T17:13:19.270059] SEC_EDGAR - SUCCESS : 1 documents trouvés pour MATTEL INC /DE/\n",
      "    Métadonnées : Accenture PLC\n",
      "[2026-01-29T17:13:20.279917] SEC_EDGAR - INFO : Requête pour CIK 0001467373\n",
      "[2026-01-29T17:13:20.811516] SEC_EDGAR - SUCCESS : 1 documents trouvés pour Accenture plc\n",
      "    Métadonnées : Infosys Ltd\n",
      "[2026-01-29T17:13:21.817353] SEC_EDGAR - INFO : Requête pour CIK 0001087713\n",
      "[2026-01-29T17:13:23.064728] SEC_EDGAR - SUCCESS : 0 documents trouvés pour KAY GEORGE C\n",
      "    Métadonnées : Cognizant Technology Solutions\n",
      "[2026-01-29T17:13:24.074490] SEC_EDGAR - INFO : Requête pour CIK 0001100640\n",
      "[2026-01-29T17:13:24.441516] SEC_EDGAR - ERROR : 404 Client Error: Not Found for url: https://data.sec.gov/submissions/CIK0001100640.json\n",
      "    Métadonnées : Johnson & Johnson\n",
      "[2026-01-29T17:13:25.448595] SEC_EDGAR - INFO : Requête pour CIK 0000200406\n",
      "[2026-01-29T17:13:26.200368] SEC_EDGAR - SUCCESS : 1 documents trouvés pour JOHNSON & JOHNSON\n",
      "    Métadonnées : Eli Lilly & Co.\n",
      "[2026-01-29T17:13:27.207257] SEC_EDGAR - INFO : Requête pour CIK 0000059478\n",
      "[2026-01-29T17:13:27.648371] SEC_EDGAR - SUCCESS : 1 documents trouvés pour ELI LILLY & Co\n",
      "    Métadonnées : Mondelez International\n",
      "[2026-01-29T17:13:28.656926] SEC_EDGAR - INFO : Requête pour CIK 0001103982\n",
      "[2026-01-29T17:13:29.132843] SEC_EDGAR - SUCCESS : 1 documents trouvés pour Mondelez International, Inc.\n",
      "    Métadonnées : Tesla Inc.\n",
      "[2026-01-29T17:13:30.139091] SEC_EDGAR - INFO : Requête pour CIK 0001318605\n",
      "[2026-01-29T17:13:30.545210] SEC_EDGAR - SUCCESS : 1 documents trouvés pour Tesla, Inc.\n",
      "    Métadonnées : Exxon Mobil Corp.\n",
      "[2026-01-29T17:13:31.550277] SEC_EDGAR - INFO : Requête pour CIK 0000034013\n",
      "[2026-01-29T17:13:32.026554] SEC_EDGAR - ERROR : 404 Client Error: Not Found for url: https://data.sec.gov/submissions/CIK0000034013.json\n",
      "    Métadonnées : Starbucks Corp.\n",
      "[2026-01-29T17:13:33.037208] SEC_EDGAR - INFO : Requête pour CIK 0000829224\n",
      "[2026-01-29T17:13:33.500154] SEC_EDGAR - SUCCESS : 1 documents trouvés pour STARBUCKS CORP\n",
      "    17 métadonnées de rapports chargées.\n",
      "    Téléchargement des rapports complets...\n",
      "    Patientez ~10 minutes (fichiers lourds)...\n",
      "\n",
      "   Téléchargement : Apple Inc.\n",
      "[2026-01-29T17:13:34.504817] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000320193/000032019325000079/aapl-20250927.htm\n",
      "[2026-01-29T17:13:36.844403] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour Apple Inc.\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : MICROSOFT CORP\n",
      "[2026-01-29T17:13:38.853296] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000789019/000095017025100235/msft-20250630.htm\n",
      "[2026-01-29T17:13:53.352330] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour MICROSOFT CORP\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : Alphabet Inc.\n",
      "[2026-01-29T17:13:55.364252] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001652044/000165204425000014/goog-20241231.htm\n",
      "[2026-01-29T17:13:57.772917] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour Alphabet Inc.\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : AMAZON COM INC\n",
      "[2026-01-29T17:13:59.782065] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001018724/000101872425000004/amzn-20241231.htm\n",
      "[2026-01-29T17:14:02.299482] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour AMAZON COM INC\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : Meta Platforms, Inc.\n",
      "[2026-01-29T17:14:04.306882] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001326801/000162828026003942/meta-20251231.htm\n",
      "[2026-01-29T17:14:07.222470] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour Meta Platforms, Inc.\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : NVIDIA CORP\n",
      "[2026-01-29T17:14:09.232306] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001045810/000104581025000023/nvda-20250126.htm\n",
      "[2026-01-29T17:14:12.172363] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour NVIDIA CORP\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : ORACLE CORP\n",
      "[2026-01-29T17:14:14.181344] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001341439/000095017025087926/orcl-20250531.htm\n",
      "[2026-01-29T17:14:23.472033] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour ORACLE CORP\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : INTERNATIONAL BUSINESS MACHINES CORP\n",
      "[2026-01-29T17:14:25.506346] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000051143/000005114325000015/ibm-20241231.htm\n",
      "[2026-01-29T17:14:27.468352] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour INTERNATIONAL BUSINESS MACHINES CORP\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : JPMORGAN CHASE & CO\n",
      "[2026-01-29T17:14:29.480793] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000019617/000001961725000270/jpm-20241231.htm\n",
      "[2026-01-29T17:14:49.278312] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour JPMORGAN CHASE & CO\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : Roth CH Acquisition V Co.\n",
      "[2026-01-29T17:14:51.289731] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001885998/000182912624002099/rothchacq5_10k.htm\n",
      "[2026-01-29T17:14:53.417243] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour Roth CH Acquisition V Co.\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : MATTEL INC /DE/\n",
      "[2026-01-29T17:14:55.433619] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000063276/000162828025007887/mat-20241231.htm\n",
      "[2026-01-29T17:14:58.918111] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour MATTEL INC /DE/\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : Accenture plc\n",
      "[2026-01-29T17:15:00.927180] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001467373/000146737325000217/acn-20250831.htm\n",
      "[2026-01-29T17:15:03.727014] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour Accenture plc\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : JOHNSON & JOHNSON\n",
      "[2026-01-29T17:15:05.735498] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000200406/000020040625000038/jnj-20241229.htm\n",
      "[2026-01-29T17:15:11.065778] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour JOHNSON & JOHNSON\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : ELI LILLY & Co\n",
      "[2026-01-29T17:15:13.074649] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000059478/000005947825000067/lly-20241231.htm\n",
      "[2026-01-29T17:15:17.517255] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour ELI LILLY & Co\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : Mondelez International, Inc.\n",
      "[2026-01-29T17:15:19.523667] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001103982/000110398225000030/mdlz-20241231.htm\n",
      "[2026-01-29T17:15:24.900939] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour Mondelez International, Inc.\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : Tesla, Inc.\n",
      "[2026-01-29T17:15:26.909421] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0001318605/000162828026003952/tsla-20251231.htm\n",
      "[2026-01-29T17:15:29.426222] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour Tesla, Inc.\n",
      "      → 1 chunks extraits\n",
      "   Téléchargement : STARBUCKS CORP\n",
      "[2026-01-29T17:15:31.432501] SEC_EDGAR_FULL - INFO : Traitement de https://www.sec.gov/Archives/edgar/data/0000829224/000082922425000114/sbux-20250928.htm\n",
      "[2026-01-29T17:15:35.195889] SEC_EDGAR_FULL - SUCCESS : Rapport compressé créé pour STARBUCKS CORP\n",
      "      → 1 chunks extraits\n",
      "    17 chunks de documents complets.\n",
      "\n",
      "Collecte des Actualités (NewsAPI Multi-Queries)...\n",
      "    Query 1/20: mergers and acquisitions 2025 trends\n",
      "[2026-01-29T17:15:37.214151] NEWSAPI - INFO : Recherche : 'mergers and acquisitions 2025 trends'\n",
      "[2026-01-29T17:15:37.728098] NEWSAPI - SUCCESS : 100 articles récupérés\n",
      "    Query 2/20: hostile takeover bids 2025\n",
      "[2026-01-29T17:15:38.736833] NEWSAPI - INFO : Recherche : 'hostile takeover bids 2025'\n",
      "[2026-01-29T17:15:38.995677] NEWSAPI - SUCCESS : 2 articles récupérés\n",
      "    Query 3/20: private equity dry powder 2025\n",
      "[2026-01-29T17:15:40.113837] NEWSAPI - INFO : Recherche : 'private equity dry powder 2025'\n",
      "[2026-01-29T17:15:40.685711] NEWSAPI - SUCCESS : 10 articles récupérés\n",
      "    Query 4/20: divestiture and carve-out strategy\n",
      "[2026-01-29T17:15:41.814327] NEWSAPI - INFO : Recherche : 'divestiture and carve-out strategy'\n",
      "[2026-01-29T17:15:42.161040] NEWSAPI - SUCCESS : 0 articles récupérés\n",
      "    Query 5/20: IPO market outlook 2025\n",
      "[2026-01-29T17:15:43.185062] NEWSAPI - INFO : Recherche : 'IPO market outlook 2025'\n",
      "[2026-01-29T17:15:45.075134] NEWSAPI - SUCCESS : 84 articles récupérés\n",
      "    Query 6/20: CSRD implementation challenges\n",
      "[2026-01-29T17:15:46.084005] NEWSAPI - INFO : Recherche : 'CSRD implementation challenges'\n",
      "[2026-01-29T17:15:48.777068] NEWSAPI - SUCCESS : 2 articles récupérés\n",
      "    Query 7/20: SEC climate disclosure ruling\n",
      "[2026-01-29T17:15:49.789447] NEWSAPI - INFO : Recherche : 'SEC climate disclosure ruling'\n",
      "[2026-01-29T17:15:50.256228] NEWSAPI - SUCCESS : 1 articles récupérés\n",
      "    Query 8/20: anti-money laundering directive EU\n",
      "[2026-01-29T17:15:51.266966] NEWSAPI - INFO : Recherche : 'anti-money laundering directive EU'\n",
      "[2026-01-29T17:15:51.669010] NEWSAPI - SUCCESS : 1 articles récupérés\n",
      "    Query 9/20: GDPR fines and enforcement 2025\n",
      "[2026-01-29T17:15:52.675546] NEWSAPI - INFO : Recherche : 'GDPR fines and enforcement 2025'\n",
      "[2026-01-29T17:15:53.058560] NEWSAPI - SUCCESS : 5 articles récupérés\n",
      "    Query 10/20: OECD Pillar Two global minimum tax\n",
      "[2026-01-29T17:15:54.064790] NEWSAPI - INFO : Recherche : 'OECD Pillar Two global minimum tax'\n",
      "[2026-01-29T17:15:54.393170] NEWSAPI - SUCCESS : 7 articles récupérés\n",
      "    Query 11/20: generative AI in corporate auditing\n",
      "[2026-01-29T17:15:55.400961] NEWSAPI - INFO : Recherche : 'generative AI in corporate auditing'\n",
      "[2026-01-29T17:15:55.782935] NEWSAPI - SUCCESS : 4 articles récupérés\n",
      "    Query 12/20: quantum computing in financial modeling\n",
      "[2026-01-29T17:15:56.790505] NEWSAPI - INFO : Recherche : 'quantum computing in financial modeling'\n",
      "[2026-01-29T17:15:57.150346] NEWSAPI - SUCCESS : 15 articles récupérés\n",
      "    Query 13/20: embedded finance and BaaS trends\n",
      "[2026-01-29T17:15:58.177699] NEWSAPI - INFO : Recherche : 'embedded finance and BaaS trends'\n",
      "[2026-01-29T17:15:58.524042] NEWSAPI - SUCCESS : 2 articles récupérés\n",
      "    Query 14/20: decarbonization of heavy industry\n",
      "[2026-01-29T17:15:59.532894] NEWSAPI - INFO : Recherche : 'decarbonization of heavy industry'\n",
      "[2026-01-29T17:16:00.065550] NEWSAPI - SUCCESS : 57 articles récupérés\n",
      "    Query 15/20: supply chain resilience technology\n",
      "[2026-01-29T17:16:01.074715] NEWSAPI - INFO : Recherche : 'supply chain resilience technology'\n",
      "[2026-01-29T17:16:01.834752] NEWSAPI - SUCCESS : 98 articles récupérés\n",
      "    Query 16/20: Deloitte annual revenue 2024 2025\n",
      "[2026-01-29T17:16:02.841974] NEWSAPI - INFO : Recherche : 'Deloitte annual revenue 2024 2025'\n",
      "[2026-01-29T17:16:03.234996] NEWSAPI - SUCCESS : 14 articles récupérés\n",
      "    Query 17/20: PwC global strategy update\n",
      "[2026-01-29T17:16:04.240572] NEWSAPI - INFO : Recherche : 'PwC global strategy update'\n",
      "[2026-01-29T17:16:04.704113] NEWSAPI - SUCCESS : 10 articles récupérés\n",
      "    Query 18/20: EY consulting restructuring\n",
      "[2026-01-29T17:16:05.709817] NEWSAPI - INFO : Recherche : 'EY consulting restructuring'\n",
      "[2026-01-29T17:16:06.017416] NEWSAPI - SUCCESS : 1 articles récupérés\n",
      "    Query 19/20: McKinsey global themes 2025\n",
      "[2026-01-29T17:16:07.169726] NEWSAPI - INFO : Recherche : 'McKinsey global themes 2025'\n",
      "[2026-01-29T17:16:07.740120] NEWSAPI - SUCCESS : 4 articles récupérés\n",
      "    Query 20/20: Accenture acquisitions digital transformation\n",
      "[2026-01-29T17:16:08.747538] NEWSAPI - INFO : Recherche : 'Accenture acquisitions digital transformation'\n",
      "[2026-01-29T17:16:12.035198] NEWSAPI - SUCCESS : 5 articles récupérés\n",
      "    422 articles totaux via NewsAPI.\n",
      "\n",
      " Flux Google News RSS...\n",
      "    RSS Query 1/25: KPMG France news\n",
      "[2026-01-29T17:16:13.044439] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'KPMG France news'\n",
      "[2026-01-29T17:16:16.545001] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 2/25: Deloitte strategic acquisitions\n",
      "[2026-01-29T17:16:18.196472] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'Deloitte strategic acquisitions'\n",
      "[2026-01-29T17:16:19.504572] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 3/25: EY consulting restructuring\n",
      "[2026-01-29T17:16:20.513020] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'EY consulting restructuring'\n",
      "[2026-01-29T17:16:21.369597] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 4/25: McKinsey insights digital transformation\n",
      "[2026-01-29T17:16:22.476288] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'McKinsey insights digital transformation'\n",
      "[2026-01-29T17:16:24.621572] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 5/25: LBO deals France\n",
      "[2026-01-29T17:16:26.514849] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'LBO deals France'\n",
      "[2026-01-29T17:16:27.857707] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 6/25: Private Equity exit strategies 2025\n",
      "[2026-01-29T17:16:29.136267] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'Private Equity exit strategies 2025'\n",
      "[2026-01-29T17:16:30.202873] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 7/25: SPAC liquidation trends\n",
      "[2026-01-29T17:16:31.585174] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'SPAC liquidation trends'\n",
      "[2026-01-29T17:16:32.329130] GOOGLE_NEWS_RSS - SUCCESS : 10 articles récupérés\n",
      "    RSS Query 8/25: CSRD reporting implementation\n",
      "[2026-01-29T17:16:34.327052] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'CSRD reporting implementation'\n",
      "[2026-01-29T17:16:35.706665] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 9/25: OECD global minimum tax pillar 2\n",
      "[2026-01-29T17:16:36.979685] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'OECD global minimum tax pillar 2'\n",
      "[2026-01-29T17:16:38.204777] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 10/25: anti-money laundering directive EU\n",
      "[2026-01-29T17:16:39.512469] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'anti-money laundering directive EU'\n",
      "[2026-01-29T17:16:40.506481] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 11/25: Generative AI enterprise adoption\n",
      "[2026-01-29T17:16:41.933532] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'Generative AI enterprise adoption'\n",
      "[2026-01-29T17:16:43.227079] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 12/25: AI Act compliance requirements\n",
      "[2026-01-29T17:16:44.823623] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'AI Act compliance requirements'\n",
      "[2026-01-29T17:16:46.348979] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 13/25: cybersecurity zero trust architecture\n",
      "[2026-01-29T17:16:47.639063] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'cybersecurity zero trust architecture'\n",
      "[2026-01-29T17:16:49.931816] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 14/25: edge computing industrial IoT\n",
      "[2026-01-29T17:16:51.859648] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'edge computing industrial IoT'\n",
      "[2026-01-29T17:16:53.030494] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 15/25: profit warning S&P 500\n",
      "[2026-01-29T17:16:54.422834] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'profit warning S&P 500'\n",
      "[2026-01-29T17:16:56.103882] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 16/25: C-suite executive turnover\n",
      "[2026-01-29T17:16:57.653377] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'C-suite executive turnover'\n",
      "[2026-01-29T17:17:00.063157] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 17/25: corporate restructuring France\n",
      "[2026-01-29T17:17:01.583258] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'corporate restructuring France'\n",
      "[2026-01-29T17:17:02.982246] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 18/25: insider trading investigations\n",
      "[2026-01-29T17:17:04.969484] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'insider trading investigations'\n",
      "[2026-01-29T17:17:06.118315] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 19/25: banking sector consolidation Europe\n",
      "[2026-01-29T17:17:07.806843] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'banking sector consolidation Europe'\n",
      "[2026-01-29T17:17:08.897682] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 20/25: luxury goods market growth China\n",
      "[2026-01-29T17:17:10.226669] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'luxury goods market growth China'\n",
      "[2026-01-29T17:17:11.587428] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 21/25: automotive transition EV strategy\n",
      "[2026-01-29T17:17:13.058628] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'automotive transition EV strategy'\n",
      "[2026-01-29T17:17:14.056821] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 22/25: defense industry spending EU\n",
      "[2026-01-29T17:17:16.004879] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'defense industry spending EU'\n",
      "[2026-01-29T17:17:17.259314] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 23/25: ECB interest rate forecast\n",
      "[2026-01-29T17:17:19.021869] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'ECB interest rate forecast'\n",
      "[2026-01-29T17:17:20.790332] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 24/25: inflation CPI Eurozone news\n",
      "[2026-01-29T17:17:22.660192] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'inflation CPI Eurozone news'\n",
      "[2026-01-29T17:17:23.730035] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    RSS Query 25/25: US-China trade relations technology\n",
      "[2026-01-29T17:17:25.672640] GOOGLE_NEWS_RSS - INFO : Flux RSS pour 'US-China trade relations technology'\n",
      "[2026-01-29T17:17:28.282233] GOOGLE_NEWS_RSS - SUCCESS : 25 articles récupérés\n",
      "    610 articles totaux via RSS.\n",
      "\n",
      " Scraping Communiqués de presse...\n",
      "[2026-01-29T17:17:30.122107] PRESS_RELEASE - INFO : Scraping https://www.apple.com/newsroom/2025/01/apple-announces-new-ai-features/\n",
      "[2026-01-29T17:17:31.475047] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://www.apple.com/newsroom/2025/01/apple-announces-new-ai-features/\n",
      "[2026-01-29T17:17:33.479413] PRESS_RELEASE - INFO : Scraping https://www.apple.com/newsroom/2025/01/apple-intelligence-expansion-europe/\n",
      "[2026-01-29T17:17:34.754656] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://www.apple.com/newsroom/2025/01/apple-intelligence-expansion-europe/\n",
      "[2026-01-29T17:17:36.755783] PRESS_RELEASE - INFO : Scraping https://www.apple.com/newsroom/2024/12/apple-reports-q4-2024-results/\n",
      "[2026-01-29T17:17:38.304684] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://www.apple.com/newsroom/2024/12/apple-reports-q4-2024-results/\n",
      "[2026-01-29T17:17:40.307416] PRESS_RELEASE - INFO : Scraping https://www.apple.com/newsroom/2023/10/apple-event-scary-fast-recap/\n",
      "[2026-01-29T17:17:41.354544] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://www.apple.com/newsroom/2023/10/apple-event-scary-fast-recap/\n",
      "[2026-01-29T17:17:43.359207] PRESS_RELEASE - INFO : Scraping https://news.microsoft.com/2025/01/microsoft-expands-azure-ai-global-infrastructure/\n",
      "[2026-01-29T17:17:44.561865] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 4 parties depuis https://news.microsoft.com/2025/01/microsoft-expands-azure-ai-global-infrastructure/\n",
      "[2026-01-29T17:17:46.566065] PRESS_RELEASE - INFO : Scraping https://news.microsoft.com/2025/01/microsoft-partnership-mistral-ai-2025/\n",
      "[2026-01-29T17:17:48.260666] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 4 parties depuis https://news.microsoft.com/2025/01/microsoft-partnership-mistral-ai-2025/\n",
      "[2026-01-29T17:17:50.264481] PRESS_RELEASE - INFO : Scraping https://news.microsoft.com/2024/12/github-copilot-enhancements-enterprise/\n",
      "[2026-01-29T17:17:51.382171] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 4 parties depuis https://news.microsoft.com/2024/12/github-copilot-enhancements-enterprise/\n",
      "[2026-01-29T17:17:53.383756] PRESS_RELEASE - INFO : Scraping https://news.microsoft.com/2023/05/microsoft-build-2023-copilot-ecosystem/\n",
      "[2026-01-29T17:17:54.822234] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 4 parties depuis https://news.microsoft.com/2023/05/microsoft-build-2023-copilot-ecosystem/\n",
      "[2026-01-29T17:17:56.824018] PRESS_RELEASE - INFO : Scraping https://news.microsoft.com/2023/03/microsoft-future-of-work-ai/\n",
      "[2026-01-29T17:17:57.809160] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 4 parties depuis https://news.microsoft.com/2023/03/microsoft-future-of-work-ai/\n",
      "[2026-01-29T17:17:59.812406] PRESS_RELEASE - INFO : Scraping https://blog.google/technology/ai/google-gemini-2-0-flash-updates/\n",
      "[2026-01-29T17:18:01.114943] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 3 parties depuis https://blog.google/technology/ai/google-gemini-2-0-flash-updates/\n",
      "[2026-01-29T17:18:03.118846] PRESS_RELEASE - INFO : Scraping https://blog.google/technology/ai/google-ai-agents-business-productivity/\n",
      "[2026-01-29T17:18:04.281352] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 3 parties depuis https://blog.google/technology/ai/google-ai-agents-business-productivity/\n",
      "[2026-01-29T17:18:06.296583] PRESS_RELEASE - INFO : Scraping https://blog.google/products/chrome/chrome-security-updates-ai/\n",
      "[2026-01-29T17:18:07.555155] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 3 parties depuis https://blog.google/products/chrome/chrome-security-updates-ai/\n",
      "[2026-01-29T17:18:09.559204] PRESS_RELEASE - INFO : Scraping https://blog.google/technology/google-deepmind-alphafold-3-research/\n",
      "[2026-01-29T17:18:11.021436] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 3 parties depuis https://blog.google/technology/google-deepmind-alphafold-3-research/\n",
      "[2026-01-29T17:18:13.024711] PRESS_RELEASE - INFO : Scraping https://abc.xyz/investor/news/earnings/2023/Q4/\n",
      "[2026-01-29T17:18:13.480621] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://abc.xyz/investor/news/earnings/2023/Q4/\n",
      "[2026-01-29T17:18:15.484981] PRESS_RELEASE - INFO : Scraping https://blog.google/technology/ai/google-gemini-pro-integration-developers/\n",
      "[2026-01-29T17:18:16.678181] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 3 parties depuis https://blog.google/technology/ai/google-gemini-pro-integration-developers/\n",
      "[2026-01-29T17:18:18.679263] PRESS_RELEASE - INFO : Scraping https://about.fb.com/news/2025/01/meta-ai-llama-4-training-update/\n",
      "[2026-01-29T17:18:19.804471] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 7 parties depuis https://about.fb.com/news/2025/01/meta-ai-llama-4-training-update/\n",
      "[2026-01-29T17:18:21.807930] PRESS_RELEASE - INFO : Scraping https://about.fb.com/news/2024/12/meta-quest-3s-sales-performance/\n",
      "[2026-01-29T17:18:22.781479] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 7 parties depuis https://about.fb.com/news/2024/12/meta-quest-3s-sales-performance/\n",
      "[2026-01-29T17:18:24.782369] PRESS_RELEASE - INFO : Scraping https://about.fb.com/news/2024/10/meta-ai-expansion-uk-brazil/\n",
      "[2026-01-29T17:18:25.851902] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 7 parties depuis https://about.fb.com/news/2024/10/meta-ai-expansion-uk-brazil/\n",
      "[2026-01-29T17:18:27.856293] PRESS_RELEASE - INFO : Scraping https://about.fb.com/news/2023/09/meta-connect-2023-quest-3-launch/\n",
      "[2026-01-29T17:18:28.857235] PRESS_RELEASE - SUCCESS : Article chargé et chunké en 7 parties depuis https://about.fb.com/news/2023/09/meta-connect-2023-quest-3-launch/\n",
      "[2026-01-29T17:18:30.862734] PRESS_RELEASE - INFO : Scraping https://www.tesla.com/blog/tesla-energy-expansion-megapack/\n",
      "[2026-01-29T17:18:31.151510] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://www.tesla.com/blog/tesla-energy-expansion-megapack/\n",
      "[2026-01-29T17:18:33.154418] PRESS_RELEASE - INFO : Scraping https://www.tesla.com/blog/tesla-fsd-v13-release-notes/\n",
      "[2026-01-29T17:18:33.384624] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://www.tesla.com/blog/tesla-fsd-v13-release-notes/\n",
      "[2026-01-29T17:18:35.385968] PRESS_RELEASE - INFO : Scraping https://www.tesla.com/blog/tesla-shareholder-meeting-2024-recap/\n",
      "[2026-01-29T17:18:35.621662] PRESS_RELEASE - SUCCESS : Article court chargé depuis https://www.tesla.com/blog/tesla-shareholder-meeting-2024-recap/\n",
      "    71 documents de communiqués (articles chunkés).\n",
      "\n",
      " Collecte des indicateurs Macro (FRED)...\n",
      "[2026-01-29T17:18:37.626194] FRED - INFO : Récupération de la série : GDP\n",
      "[2026-01-29T17:18:40.301968] FRED - SUCCESS : Série GDP chargée avec succès\n",
      "[2026-01-29T17:18:40.302557] FRED - INFO : Récupération de la série : GDPC1\n",
      "[2026-01-29T17:18:41.607973] FRED - SUCCESS : Série GDPC1 chargée avec succès\n",
      "[2026-01-29T17:18:41.609050] FRED - INFO : Récupération de la série : INDPRO\n",
      "[2026-01-29T17:18:42.797029] FRED - SUCCESS : Série INDPRO chargée avec succès\n",
      "[2026-01-29T17:18:42.799007] FRED - INFO : Récupération de la série : UNRATE\n",
      "[2026-01-29T17:18:44.029837] FRED - SUCCESS : Série UNRATE chargée avec succès\n",
      "[2026-01-29T17:18:44.030488] FRED - INFO : Récupération de la série : JTSJOL\n",
      "[2026-01-29T17:18:45.353757] FRED - SUCCESS : Série JTSJOL chargée avec succès\n",
      "[2026-01-29T17:18:45.355735] FRED - INFO : Récupération de la série : CPIAUCSL\n",
      "[2026-01-29T17:18:46.487033] FRED - SUCCESS : Série CPIAUCSL chargée avec succès\n",
      "[2026-01-29T17:18:46.487720] FRED - INFO : Récupération de la série : CPILFESL\n",
      "[2026-01-29T17:18:48.368116] FRED - SUCCESS : Série CPILFESL chargée avec succès\n",
      "[2026-01-29T17:18:48.369259] FRED - INFO : Récupération de la série : PCEPI\n",
      "[2026-01-29T17:18:50.282662] FRED - SUCCESS : Série PCEPI chargée avec succès\n",
      "[2026-01-29T17:18:50.283158] FRED - INFO : Récupération de la série : T10YIE\n",
      "[2026-01-29T17:18:51.538705] FRED - SUCCESS : Série T10YIE chargée avec succès\n",
      "[2026-01-29T17:18:51.539243] FRED - INFO : Récupération de la série : CPIFABSL\n",
      "[2026-01-29T17:18:52.866151] FRED - SUCCESS : Série CPIFABSL chargée avec succès\n",
      "[2026-01-29T17:18:52.867222] FRED - INFO : Récupération de la série : FEDFUNDS\n",
      "[2026-01-29T17:18:54.342558] FRED - SUCCESS : Série FEDFUNDS chargée avec succès\n",
      "[2026-01-29T17:18:54.343090] FRED - INFO : Récupération de la série : DGS2\n",
      "[2026-01-29T17:18:56.576251] FRED - SUCCESS : Série DGS2 chargée avec succès\n",
      "[2026-01-29T17:18:56.576837] FRED - INFO : Récupération de la série : DPRIME\n",
      "[2026-01-29T17:18:57.842147] FRED - SUCCESS : Série DPRIME chargée avec succès\n",
      "[2026-01-29T17:18:57.842984] FRED - INFO : Récupération de la série : BAA10Y\n",
      "[2026-01-29T17:18:59.131674] FRED - SUCCESS : Série BAA10Y chargée avec succès\n",
      "[2026-01-29T17:18:59.132230] FRED - INFO : Récupération de la série : REALLN15N\n",
      "[2026-01-29T17:18:59.521018] FRED - ERROR : Erreur pour REALLN15N: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series?series_id=REALLN15N&api_key=393d96daac34c4959ac5c9d193363d1b&file_type=json\n",
      "[2026-01-29T17:18:59.521565] FRED - INFO : Récupération de la série : SP500\n",
      "[2026-01-29T17:19:00.743371] FRED - SUCCESS : Série SP500 chargée avec succès\n",
      "[2026-01-29T17:19:00.743909] FRED - INFO : Récupération de la série : DJIA\n",
      "[2026-01-29T17:19:02.037714] FRED - SUCCESS : Série DJIA chargée avec succès\n",
      "[2026-01-29T17:19:02.038998] FRED - INFO : Récupération de la série : NASDAQCOM\n",
      "[2026-01-29T17:19:03.227997] FRED - SUCCESS : Série NASDAQCOM chargée avec succès\n",
      "[2026-01-29T17:19:03.228584] FRED - INFO : Récupération de la série : DEXJPUS\n",
      "[2026-01-29T17:19:04.496556] FRED - SUCCESS : Série DEXJPUS chargée avec succès\n",
      "[2026-01-29T17:19:04.497304] FRED - INFO : Récupération de la série : BAMLH0A0HYM2\n",
      "[2026-01-29T17:19:05.674532] FRED - SUCCESS : Série BAMLH0A0HYM2 chargée avec succès\n",
      "[2026-01-29T17:19:05.675203] FRED - INFO : Récupération de la série : PCE\n",
      "[2026-01-29T17:19:06.931823] FRED - SUCCESS : Série PCE chargée avec succès\n",
      "[2026-01-29T17:19:06.952111] FRED - INFO : Récupération de la série : PSAVERT\n",
      "[2026-01-29T17:19:08.438647] FRED - SUCCESS : Série PSAVERT chargée avec succès\n",
      "[2026-01-29T17:19:08.440437] FRED - INFO : Récupération de la série : UMCSENT\n",
      "[2026-01-29T17:19:09.626434] FRED - SUCCESS : Série UMCSENT chargée avec succès\n",
      "[2026-01-29T17:19:09.627512] FRED - INFO : Récupération de la série : EXHOSLUSM495S\n",
      "[2026-01-29T17:19:10.795572] FRED - SUCCESS : Série EXHOSLUSM495S chargée avec succès\n",
      "[2026-01-29T17:19:10.796251] FRED - INFO : Récupération de la série : DRCLACBS\n",
      "[2026-01-29T17:19:11.985216] FRED - SUCCESS : Série DRCLACBS chargée avec succès\n",
      "[2026-01-29T17:19:11.986380] FRED - INFO : Récupération de la série : NAPM\n",
      "[2026-01-29T17:19:12.306228] FRED - ERROR : Erreur pour NAPM: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series?series_id=NAPM&api_key=393d96daac34c4959ac5c9d193363d1b&file_type=json\n",
      "[2026-01-29T17:19:12.306839] FRED - INFO : Récupération de la série : NAPMNOI\n",
      "[2026-01-29T17:19:12.634707] FRED - ERROR : Erreur pour NAPMNOI: 400 Client Error: Bad Request for url: https://api.stlouisfed.org/fred/series?series_id=NAPMNOI&api_key=393d96daac34c4959ac5c9d193363d1b&file_type=json\n",
      "[2026-01-29T17:19:12.635281] FRED - INFO : Récupération de la série : IPMAN\n",
      "[2026-01-29T17:19:13.797376] FRED - SUCCESS : Série IPMAN chargée avec succès\n",
      "[2026-01-29T17:19:13.798065] FRED - INFO : Récupération de la série : TCU\n",
      "[2026-01-29T17:19:15.150215] FRED - SUCCESS : Série TCU chargée avec succès\n",
      "[2026-01-29T17:19:15.150961] FRED - INFO : Récupération de la série : GFDEBTN\n",
      "[2026-01-29T17:19:16.508400] FRED - SUCCESS : Série GFDEBTN chargée avec succès\n",
      "[2026-01-29T17:19:16.509051] FRED - INFO : Récupération de la série : GFDEGDQ188S\n",
      "[2026-01-29T17:19:17.651431] FRED - SUCCESS : Série GFDEGDQ188S chargée avec succès\n",
      "[2026-01-29T17:19:17.652002] FRED - INFO : Récupération de la série : WM2NS\n",
      "[2026-01-29T17:19:18.807377] FRED - SUCCESS : Série WM2NS chargée avec succès\n",
      "[2026-01-29T17:19:18.808095] FRED - INFO : Récupération de la série : TOTLL\n",
      "[2026-01-29T17:19:20.066182] FRED - SUCCESS : Série TOTLL chargée avec succès\n",
      "[2026-01-29T17:19:20.066752] FRED - INFO : Récupération de la série : BOPGSTB\n",
      "[2026-01-29T17:19:21.263103] FRED - SUCCESS : Série BOPGSTB chargée avec succès\n",
      "[2026-01-29T17:19:21.264700] FRED - INFO : Récupération de la série : IMPGS\n",
      "[2026-01-29T17:19:22.482276] FRED - SUCCESS : Série IMPGS chargée avec succès\n",
      "[2026-01-29T17:19:22.483278] FRED - INFO : Récupération de la série : WPU101\n",
      "[2026-01-29T17:19:23.682267] FRED - SUCCESS : Série WPU101 chargée avec succès\n",
      "[2026-01-29T17:19:23.682760] FRED - INFO : Récupération de la série : WPU03T15M05\n",
      "[2026-01-29T17:19:24.884814] FRED - SUCCESS : Série WPU03T15M05 chargée avec succès\n",
      "    34 séries économiques FRED chargées.\n",
      "\n",
      " Collecte des données entreprises françaises (INSEE SIRENE)...\n",
      "[2026-01-29T17:19:24.902905] SIRENE - INFO : Requête SIREN 442395448\n",
      "[2026-01-29T17:19:25.219756] SIRENE - SUCCESS : SIREN 442395448 ingéré\n",
      "[2026-01-29T17:19:25.725403] SIRENE - INFO : Requête SIREN 775670417\n",
      "[2026-01-29T17:19:25.924905] SIRENE - SUCCESS : SIREN 775670417 ingéré\n",
      "[2026-01-29T17:19:26.442394] SIRENE - INFO : Requête SIREN 632012100\n",
      "[2026-01-29T17:19:26.716688] SIRENE - SUCCESS : SIREN 632012100 ingéré\n",
      "[2026-01-29T17:19:27.222197] SIRENE - INFO : Requête SIREN 954507931\n",
      "[2026-01-29T17:19:27.463527] SIRENE - ERROR : SIREN 954507931 - HTTP 404\n",
      "[2026-01-29T17:19:27.464253] SIRENE - INFO : Requête SIREN 428268023\n",
      "[2026-01-29T17:19:27.819943] SIRENE - SUCCESS : SIREN 428268023 ingéré\n",
      "[2026-01-29T17:19:28.321884] SIRENE - INFO : Requête SIREN 542095336\n",
      "[2026-01-29T17:19:28.576256] SIRENE - SUCCESS : SIREN 542095336 ingéré\n",
      "[2026-01-29T17:19:29.081103] SIRENE - INFO : Requête SIREN 878706389\n",
      "[2026-01-29T17:19:29.312949] SIRENE - ERROR : SIREN 878706389 - HTTP 404\n",
      "[2026-01-29T17:19:29.313668] SIRENE - INFO : Requête SIREN 802174462\n",
      "[2026-01-29T17:19:29.574623] SIRENE - ERROR : SIREN 802174462 - HTTP 404\n",
      "[2026-01-29T17:19:29.575244] SIRENE - INFO : Requête SIREN 552066953\n",
      "[2026-01-29T17:19:29.808665] SIRENE - ERROR : SIREN 552066953 - HTTP 404\n",
      "[2026-01-29T17:19:29.809527] SIRENE - INFO : Requête SIREN 380243956\n",
      "[2026-01-29T17:19:30.083158] SIRENE - SUCCESS : SIREN 380243956 ingéré\n",
      "    6 entreprises françaises analysées.\n",
      "\n",
      "[2026-01-29T17:19:30.588436] DBNOMICS - INFO : Récupération de 14 séries\n",
      "[2026-01-29T17:19:30.589181] DBNOMICS - INFO : Fetch: PIB France (trimestriel)\n",
      "[2026-01-29T17:19:32.477777] DBNOMICS - INFO : Fetch: Inflation France - IPC (mensuel)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished call to 'dbnomics._fetch_response' after 0.537(s), this was the 1st time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 1.877(s), this was the 2nd time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 3.178(s), this was the 3rd time calling it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-29T17:19:35.658429] DBNOMICS - ERROR : Inflation France - IPC (mensuel): Could not fetch data from URL 'https://api.db.nomics.world/v22/series/OECD/MEI/FRA.CP.IXOBSA.M?observations=1&offset=0'\n",
      "[2026-01-29T17:19:35.659408] DBNOMICS - INFO : Fetch: Taux de chômage France (mensuel)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished call to 'dbnomics._fetch_response' after 0.528(s), this was the 1st time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 1.805(s), this was the 2nd time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 3.490(s), this was the 3rd time calling it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-29T17:19:39.171893] DBNOMICS - ERROR : Taux de chômage France (mensuel): Could not fetch data from URL 'https://api.db.nomics.world/v22/series/OECD/MEI/FRA.LR.STSA.M?observations=1&offset=0'\n",
      "[2026-01-29T17:19:39.174184] DBNOMICS - INFO : Fetch: Indicateur composite avancé France (Lead Indicator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished call to 'dbnomics._fetch_response' after 0.401(s), this was the 1st time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 1.516(s), this was the 2nd time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 3.523(s), this was the 3rd time calling it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-29T17:19:42.698403] DBNOMICS - ERROR : Indicateur composite avancé France (Lead Indicator): Could not fetch data from URL 'https://api.db.nomics.world/v22/series/OECD/MEI/FRA.LI.GOLD.IXOBSA.M?observations=1&offset=0'\n",
      "[2026-01-29T17:19:42.699147] DBNOMICS - INFO : Fetch: Confiance des consommateurs France\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished call to 'dbnomics._fetch_response' after 0.380(s), this was the 1st time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 1.529(s), this was the 2nd time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 2.178(s), this was the 3rd time calling it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-29T17:19:44.879352] DBNOMICS - ERROR : Confiance des consommateurs France: Could not fetch data from URL 'https://api.db.nomics.world/v22/series/OECD/MEI/FRA.CSC.GOLD.IXOBSA.M?observations=1&offset=0'\n",
      "[2026-01-29T17:19:44.880272] DBNOMICS - INFO : Fetch: Confiance des entreprises France (Business Climate)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished call to 'dbnomics._fetch_response' after 0.538(s), this was the 1st time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 1.921(s), this was the 2nd time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 4.427(s), this was the 3rd time calling it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-29T17:19:49.309381] DBNOMICS - ERROR : Confiance des entreprises France (Business Climate): Could not fetch data from URL 'https://api.db.nomics.world/v22/series/OECD/MEI/FRA.BSC.GOLD.IXOBSA.M?observations=1&offset=0'\n",
      "[2026-01-29T17:19:49.315682] DBNOMICS - INFO : Fetch: Taux de prêt bancaire aux entreprises (Zone Euro)\n",
      "[2026-01-29T17:19:51.264152] DBNOMICS - INFO : Fetch: Taux long terme (10 ans) France\n",
      "[2026-01-29T17:19:54.047569] DBNOMICS - INFO : Fetch: Ventes de détail France (Consommation)\n",
      "[2026-01-29T17:19:55.526528] DBNOMICS - INFO : Fetch: Production industrielle France (Général)\n",
      "[2026-01-29T17:19:57.255207] DBNOMICS - INFO : Fetch: Production dans la Construction (Immobilier)\n",
      "[2026-01-29T17:19:59.513848] DBNOMICS - INFO : Fetch: PIB USA (trimestriel)\n",
      "[2026-01-29T17:20:00.732227] DBNOMICS - INFO : Fetch: PIB Allemagne (trimestriel)\n",
      "[2026-01-29T17:20:01.898898] DBNOMICS - INFO : Fetch: Inflation USA (mensuel)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished call to 'dbnomics._fetch_response' after 0.383(s), this was the 1st time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 1.637(s), this was the 2nd time calling it.\n",
      "Finished call to 'dbnomics._fetch_response' after 2.387(s), this was the 3rd time calling it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-01-29T17:20:04.288718] DBNOMICS - ERROR : Inflation USA (mensuel): Could not fetch data from URL 'https://api.db.nomics.world/v22/series/OECD/MEI/USA.CP.IXOBSA.M?observations=1&offset=0'\n",
      "[2026-01-29T17:20:04.289947] DBNOMICS - SUCCESS : 8 séries récupérées\n",
      "   8 séries économiques chargées.\n",
      "\n",
      "[2026-01-29T17:20:04.291183] ADZUNA - INFO : Requête search: what='Senior Manager Audit', where='Paris', country=fr\n",
      "[2026-01-29T17:20:05.353945] ADZUNA - INFO : Trouvé 9 offres (moyenne salaire: 48000€)\n",
      "[2026-01-29T17:20:05.355099] ADZUNA - SUCCESS : 9 documents créés\n",
      "[2026-01-29T17:20:06.361257] ADZUNA - INFO : Requête search: what='Consultant M&A Transaction Services', where='France', country=fr\n",
      "[2026-01-29T17:20:07.082725] ADZUNA - INFO : Trouvé 0 offres (moyenne salaire: 0€)\n",
      "[2026-01-29T17:20:07.083321] ADZUNA - SUCCESS : 0 documents créés\n",
      "[2026-01-29T17:20:08.091425] ADZUNA - INFO : Requête search: what='Manager Cybersecurité', where='Île-de-France', country=fr\n",
      "[2026-01-29T17:20:09.493249] ADZUNA - INFO : Trouvé 203 offres (moyenne salaire: 59636€)\n",
      "[2026-01-29T17:20:09.508949] ADZUNA - SUCCESS : 20 documents créés\n",
      "[2026-01-29T17:20:10.515436] ADZUNA - INFO : Requête search: what='Consultant RSE CSRD', where='France', country=fr\n",
      "[2026-01-29T17:20:11.345359] ADZUNA - INFO : Trouvé 0 offres (moyenne salaire: 0€)\n",
      "[2026-01-29T17:20:11.345903] ADZUNA - SUCCESS : 0 documents créés\n",
      "[2026-01-29T17:20:12.356284] ADZUNA - INFO : Requête search: what='Expert Décarbonation Industrie', where='France', country=fr\n",
      "[2026-01-29T17:20:13.473226] ADZUNA - INFO : Trouvé 0 offres (moyenne salaire: 0€)\n",
      "[2026-01-29T17:20:13.473904] ADZUNA - SUCCESS : 0 documents créés\n",
      "[2026-01-29T17:20:14.482008] ADZUNA - INFO : Requête search: what='Generative AI Engineer', where='France', country=fr\n",
      "[2026-01-29T17:20:15.408258] ADZUNA - INFO : Trouvé 1 offres (moyenne salaire: 50000€)\n",
      "[2026-01-29T17:20:15.408946] ADZUNA - SUCCESS : 1 documents créés\n",
      "[2026-01-29T17:20:16.440146] ADZUNA - INFO : Requête search: what='Chief Data Officer', where='Paris', country=fr\n",
      "[2026-01-29T17:20:17.619206] ADZUNA - INFO : Trouvé 34 offres (moyenne salaire: 50333€)\n",
      "[2026-01-29T17:20:17.622110] ADZUNA - SUCCESS : 10 documents créés\n",
      "[2026-01-29T17:20:18.628411] ADZUNA - INFO : Requête histogram: what='Expert Comptable', where='Paris', country=fr\n",
      "[2026-01-29T17:20:19.987595] ADZUNA - SUCCESS : 1 documents créés\n",
      "[2026-01-29T17:20:20.998232] ADZUNA - INFO : Requête histogram: what='Analyste Financier', where='Lyon', country=fr\n",
      "[2026-01-29T17:20:21.765303] ADZUNA - SUCCESS : 1 documents créés\n",
      "[2026-01-29T17:20:22.774481] ADZUNA - INFO : Requête histogram: what='Directeur Financier (CFO)', where='Bordeaux', country=fr\n",
      "[2026-01-29T17:20:23.977202] ADZUNA - SUCCESS : 1 documents créés\n",
      "[2026-01-29T17:20:24.986145] ADZUNA - INFO : Requête top_companies: what='Digital Transformation', where='France', country=fr\n",
      "[2026-01-29T17:20:27.198241] ADZUNA - SUCCESS : 1 documents créés\n",
      "[2026-01-29T17:20:28.202517] ADZUNA - INFO : Requête top_companies: what='SAP S/4HANA', where='France', country=fr\n",
      "[2026-01-29T17:20:28.917903] ADZUNA - SUCCESS : 1 documents créés\n",
      "[2026-01-29T17:20:29.922395] ADZUNA - INFO : Requête top_companies: what='Cloud Migration', where='France', country=fr\n",
      "[2026-01-29T17:20:30.983599] ADZUNA - SUCCESS : 1 documents créés\n",
      "    46 analyses emploi collectées.\n",
      "\n",
      " Collecte des signaux sociaux (Bluesky)...\n",
      "[2026-01-29T17:20:31.988520] BLUESKY - INFO : Connexion à Bluesky...\n",
      "[2026-01-29T17:20:34.003045] BLUESKY - SUCCESS : Connecté en tant que\n",
      "[2026-01-29T17:20:34.003586] BLUESKY - INFO : Recherche: 'KPMG France'\n",
      "[2026-01-29T17:20:34.386971] BLUESKY - INFO : Trouvé 15 posts pour 'KPMG France'\n",
      "[2026-01-29T17:20:35.389757] BLUESKY - INFO : Recherche: 'KPMG Strategy'\n",
      "[2026-01-29T17:20:36.210922] BLUESKY - INFO : Trouvé 0 posts pour 'KPMG Strategy'\n",
      "[2026-01-29T17:20:37.214642] BLUESKY - INFO : Recherche: 'Deloitte France Insights'\n",
      "[2026-01-29T17:20:37.754169] BLUESKY - INFO : Trouvé 0 posts pour 'Deloitte France Insights'\n",
      "[2026-01-29T17:20:38.755919] BLUESKY - INFO : Recherche: 'PwC France'\n",
      "[2026-01-29T17:20:39.113519] BLUESKY - INFO : Trouvé 15 posts pour 'PwC France'\n",
      "[2026-01-29T17:20:40.114970] BLUESKY - INFO : Recherche: 'Mazars audit'\n",
      "[2026-01-29T17:20:40.365368] BLUESKY - INFO : Trouvé 1 posts pour 'Mazars audit'\n",
      "[2026-01-29T17:20:41.369975] BLUESKY - INFO : Recherche: 'CSRD reporting'\n",
      "[2026-01-29T17:20:41.831271] BLUESKY - INFO : Trouvé 15 posts pour 'CSRD reporting'\n",
      "[2026-01-29T17:20:42.835224] BLUESKY - INFO : Recherche: 'taxonomie européenne'\n",
      "[2026-01-29T17:20:43.302239] BLUESKY - INFO : Trouvé 15 posts pour 'taxonomie européenne'\n",
      "[2026-01-29T17:20:44.309291] BLUESKY - INFO : Recherche: 'pilier 2 OCDE'\n",
      "[2026-01-29T17:20:44.576029] BLUESKY - INFO : Trouvé 0 posts pour 'pilier 2 OCDE'\n",
      "[2026-01-29T17:20:45.576543] BLUESKY - INFO : Recherche: 'IFRS standards updates'\n",
      "[2026-01-29T17:20:45.838513] BLUESKY - INFO : Trouvé 0 posts pour 'IFRS standards updates'\n",
      "[2026-01-29T17:20:46.843851] BLUESKY - INFO : Recherche: 'conformité SAP S/4HANA'\n",
      "[2026-01-29T17:20:47.108073] BLUESKY - INFO : Trouvé 0 posts pour 'conformité SAP S/4HANA'\n",
      "[2026-01-29T17:20:48.112110] BLUESKY - INFO : Recherche: 'startup seed fintech France'\n",
      "[2026-01-29T17:20:48.576230] BLUESKY - INFO : Trouvé 0 posts pour 'startup seed fintech France'\n",
      "[2026-01-29T17:20:49.584731] BLUESKY - INFO : Recherche: 'série A levée de fonds'\n",
      "[2026-01-29T17:20:49.913639] BLUESKY - INFO : Trouvé 3 posts pour 'série A levée de fonds'\n",
      "[2026-01-29T17:20:50.918501] BLUESKY - INFO : Recherche: 'redressement judiciaire'\n",
      "[2026-01-29T17:20:51.222634] BLUESKY - INFO : Trouvé 15 posts pour 'redressement judiciaire'\n",
      "[2026-01-29T17:20:52.231266] BLUESKY - INFO : Recherche: 'IA générative audit'\n",
      "[2026-01-29T17:20:52.536194] BLUESKY - INFO : Trouvé 1 posts pour 'IA générative audit'\n",
      "[2026-01-29T17:20:53.539179] BLUESKY - INFO : Recherche: 'automatisation reporting financier'\n",
      "[2026-01-29T17:20:53.833262] BLUESKY - INFO : Trouvé 0 posts pour 'automatisation reporting financier'\n",
      "[2026-01-29T17:20:54.837381] BLUESKY - INFO : Recherche: 'cloud souverain France'\n",
      "[2026-01-29T17:20:55.193271] BLUESKY - INFO : Trouvé 15 posts pour 'cloud souverain France'\n",
      "[2026-01-29T17:20:56.194939] BLUESKY - SUCCESS : 95 posts collectés\n",
      "   95 posts Bluesky.\n",
      "\n",
      "============================================================\n",
      " RÉSUMÉ DE L'INGESTION\n",
      "============================================================\n",
      "   financial_reports    :   40 documents \n",
      "   news                 : 1103 documents \n",
      "   startups             :    0 documents  VIDE\n",
      "   macro_data           :   88 documents \n",
      "   social_signals       :   95 documents \n",
      "------------------------------------------------------------\n",
      "  TOTAL DOCUMENTS : 1326\n",
      "============================================================\n",
      "\n",
      "Documents sauvegardés dans 'ingested_documents.json'\n",
      "Prêt pour le chunking et les embeddings (Notebook 3)\n",
      " Documents sauvegardés dans 'ingested_documents.json'\n",
      " Prêt pour le chunking et les embeddings (Notebook 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTEBOOK 2 : Ingestion Multi-Sources\n",
    "====================================\n",
    "\n",
    "OBJECTIF : Créer un pipeline robuste d'ingestion de données\n",
    "           depuis SEC EDGAR, NewsAPI, communiqués de presse et yfinance.\n",
    "\n",
    "RÉFÉRENCES :\n",
    "- LangChain Document Loaders : https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "- SEC EDGAR : https://www.sec.gov/edgar/sec-api-documentation\n",
    "- NewsAPI : https://newsapi.org/docs\n",
    "- yfinance : https://pypi.org/project/yfinance/\n",
    "\n",
    "ARCHITECTURE :\n",
    "1. Loaders modulaires par source\n",
    "2. Métadonnées riches (source, date, type)\n",
    "3. Gestion d'erreurs et retry\n",
    "4. Logging pour traçabilité\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "NOTEBOOK 2 : Ingestion Multi-Sources (VERSION CORRIGÉE)\n",
    "=======================================================\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# IMPORTS \n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random \n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Web Scraping & Parsing\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "# LangChain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# APIs Financières & Économiques\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "import dbnomics as db\n",
    "\n",
    "from atproto import Client\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# CONFIGURATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")\n",
    "NEWSAPI_ENDPOINT = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "SEC_USER_AGENT = os.getenv(\"SEC_USER_AGENT\", \"Student research@education.com\")\n",
    "SEC_BASE_URL = \"https://data.sec.gov/submissions/\"\n",
    "\n",
    "FRED_API_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "FRED_BASE_URL = os.getenv(\"FRED_BASE_URL\")\n",
    "\n",
    "PAPPERS_API_KEY = os.getenv(\"PAPPERS_API_KEY\")\n",
    "PAPPERS_BASE_URL = os.getenv(\"PAPPERS_BASE_URL\")\n",
    "\n",
    "API_KEY = \"7618440f-8b06-444b-9844-0f8b06144b2e\"\n",
    "BASE_URL = \"https://api.insee.fr/api-sirene/3.11\"\n",
    "\n",
    "ADZUNA_APP_ID= os.getenv(\"ADZUNA_APP_ID\")\n",
    "ADZUNA_APP_KEY= os.getenv(\"ADZUNA_APP_KEY\")\n",
    "\n",
    "BLUESKY_HANDLE= os.getenv(\"BLUESKY_HANDLE\")\n",
    "BLUESKY_APP_PASSWORD= os.getenv(\"BLUESKY_APP_PASSWORD\")\n",
    "\n",
    "LOGS_DIR = \"ingestion_logs\"\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def log_ingestion(source: str, status: str, details: str):\n",
    "    \"\"\"Log avec timestamp\"\"\"\n",
    "    timestamp = datetime.now().isoformat()\n",
    "    log_entry = f\"[{timestamp}] {source} - {status} : {details}\\n\"\n",
    "    \n",
    "    with open(f\"{LOGS_DIR}/ingestion.log\", \"a\") as f:\n",
    "        f.write(log_entry)\n",
    "    \n",
    "    print(log_entry.strip())\n",
    "\n",
    "print(\" Configuration des sources initialisée\\n\")\n",
    "\n",
    "from config import KPMG_DBNOMICS_SERIES, ALL_BLUESKY_QUERIES\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE 1 : SEC EDGAR - METADATA\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_sec_edgar_filing(cik: str, filing_type: str = \"10-K\", limit: int = 5) -> List[Document]:\n",
    "    \"\"\"Charge les dépôts SEC (limité aux N plus récents)\"\"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": SEC_USER_AGENT}\n",
    "        url = f\"{SEC_BASE_URL}CIK{cik.zfill(10)}.json\"\n",
    "        \n",
    "        log_ingestion(\"SEC_EDGAR\", \"INFO\", f\"Requête pour CIK {cik}\")\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        company_name = data.get(\"name\", \"Unknown\")\n",
    "        filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        \n",
    "        documents = []\n",
    "        count = 0\n",
    "        \n",
    "        for i, form in enumerate(filings.get(\"form\", [])):\n",
    "            if form == filing_type and count < limit:\n",
    "                filing_date = filings[\"filingDate\"][i]\n",
    "                accession = filings[\"accessionNumber\"][i]\n",
    "                primary_doc = filings[\"primaryDocument\"][i]\n",
    "                \n",
    "                acc_no_formatted = accession.replace(\"-\", \"\")\n",
    "                doc_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{acc_no_formatted}/{primary_doc}\"\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=f\"SEC Filing {filing_type} for {company_name} on {filing_date}. \"\n",
    "                                f\"This regulatory filing provides financial information and disclosures. \"\n",
    "                                f\"Accession Number: {accession}\",\n",
    "                    metadata={\n",
    "                        \"source\": \"sec_edgar\",\n",
    "                        \"company\": company_name,\n",
    "                        \"cik\": cik,\n",
    "                        \"filing_type\": filing_type,\n",
    "                        \"filing_date\": filing_date,\n",
    "                        \"accession_number\": accession,\n",
    "                        \"url\": doc_url,\n",
    "                        \"namespace\": \"financial_reports\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                count += 1\n",
    "        \n",
    "        log_ingestion(\"SEC_EDGAR\", \"SUCCESS\", f\"{len(documents)} documents trouvés pour {company_name}\")\n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"SEC_EDGAR\", \"ERROR\", str(e))\n",
    "        return []\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SEC EDGAR - DOCUMENTS COMPLETS (Version Optimisée : 1 Doc / Rapport)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_sec_full_document(filing_url: str, filing_metadata: dict) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Télécharge et compresse un rapport SEC complet en UN SEUL document JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_ingestion(\"SEC_EDGAR_FULL\", \"INFO\", f\"Traitement de {filing_url}\")\n",
    "        \n",
    "        headers = {\"User-Agent\": SEC_USER_AGENT}\n",
    "        response = requests.get(filing_url, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # --- NETTOYAGE DRASTIQUE ---\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # On supprime les styles, scripts et SURTOUT les tables (qui pèsent lourd)\n",
    "        for tag in soup([\"script\", \"style\", \"table\", \"footer\", \"header\"]):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Extraction du texte narratif uniquement\n",
    "        clean_text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # --- CRÉATION DU DOCUMENT UNIQUE ---\n",
    "        # On ne fait plus de boucle, on crée un seul objet Document\n",
    "        doc = Document(\n",
    "            page_content=clean_text,\n",
    "            metadata={\n",
    "                \"source\": \"sec_edgar_full\",\n",
    "                \"company\": filing_metadata.get(\"company\"),\n",
    "                \"cik\": filing_metadata.get(\"cik\"),\n",
    "                \"date\": filing_metadata.get(\"filing_date\"),\n",
    "                \"url\": filing_url,\n",
    "                \"type\": filing_metadata.get(\"filing_type\"),\n",
    "                \"namespace\": \"financial_reports\" # Pour ton index Pinecone\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        log_ingestion(\"SEC_EDGAR_FULL\", \"SUCCESS\", f\"Rapport compressé créé pour {filing_metadata.get('company')}\")\n",
    "        return [doc]\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"SEC_EDGAR_FULL\", \"ERROR\", f\"Erreur sur {filing_url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE 2 : NEWSAPI \n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_newsapi_articles(query: str, language: str = \"en\", days_back: int = 7) -> List[Document]:\n",
    "    \"\"\"Charge des articles depuis NewsAPI\"\"\"\n",
    "    if not NEWSAPI_KEY:\n",
    "        log_ingestion(\"NEWSAPI\", \"ERROR\", \"Clé API manquante\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        from_date = (datetime.now() - timedelta(days=days_back)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"from\": from_date,\n",
    "            \"language\": language,\n",
    "            \"sortBy\": \"relevancy\",\n",
    "            \"pageSize\": 100,  # Max 100\n",
    "            \"apiKey\": NEWSAPI_KEY\n",
    "        }\n",
    "        \n",
    "        log_ingestion(\"NEWSAPI\", \"INFO\", f\"Recherche : '{query}'\")\n",
    "        \n",
    "        response = requests.get(NEWSAPI_ENDPOINT, params=params, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        articles = data.get(\"articles\", [])\n",
    "        \n",
    "        documents = []\n",
    "        for article in articles:\n",
    "            content = f\"{article.get('title', '')}. {article.get('description', '')}\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": \"newsapi\",\n",
    "                    \"title\": article.get(\"title\"),\n",
    "                    \"author\": article.get(\"author\") or \"Unknown\",  # Fix None\n",
    "                    \"published_at\": article.get(\"publishedAt\"),\n",
    "                    \"url\": article.get(\"url\"),\n",
    "                    \"source_name\": article.get(\"source\", {}).get(\"name\"),\n",
    "                    \"namespace\": \"news\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        log_ingestion(\"NEWSAPI\", \"SUCCESS\", f\"{len(documents)} articles récupérés\")\n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"NEWSAPI\", \"ERROR\", str(e))\n",
    "        return []\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE 3 : COMMUNIQUÉS DE PRESSE \n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_press_releases(urls: List[str], chunk_articles: bool = True) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Charge des communiqués de presse avec chunking automatique.\n",
    "    \n",
    "    AMÉLIORATIONS par rapport à la version précédente :\n",
    "    - Chunking automatique des articles longs\n",
    "    - Nettoyage HTML (suppression menus, footers)\n",
    "    - Support des URLs multiples en masse\n",
    "    \n",
    "    Paramètres :\n",
    "    -----------\n",
    "    urls : List[str]\n",
    "        Liste des URLs de communiqués individuels (pas les pages d'accueil)\n",
    "    chunk_articles : bool\n",
    "        Si True, découpe les articles longs en chunks (recommandé pour RAG)\n",
    "    \n",
    "    Exemple d'utilisation :\n",
    "    ----------------------\n",
    "    # Scraper 100 articles individuels au lieu d'1 page d'accueil\n",
    "    press_urls = [\n",
    "        \"https://www.apple.com/newsroom/2024/12/apple-announces-q4-results/\",\n",
    "        \"https://www.apple.com/newsroom/2024/11/apple-launches-new-product/\",\n",
    "        # ... 98 autres URLs\n",
    "    ]\n",
    "    docs = load_press_releases(press_urls, chunk_articles=True)\n",
    "    # Résultat : 100-200 documents au lieu de 1\n",
    "    \"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    for url in urls:\n",
    "        try:\n",
    "            log_ingestion(\"PRESS_RELEASE\", \"INFO\", f\"Scraping {url}\")\n",
    "            \n",
    "            # ─────────────────────────────────────────────────────\n",
    "            # 1. CHARGEMENT DE LA PAGE\n",
    "            # ─────────────────────────────────────────────────────\n",
    "            \n",
    "            loader = WebBaseLoader(url)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            if not docs:\n",
    "                log_ingestion(\"PRESS_RELEASE\", \"WARNING\", f\"Aucun contenu trouvé pour {url}\")\n",
    "                continue\n",
    "            \n",
    "            # ─────────────────────────────────────────────────────\n",
    "            # 2. NETTOYAGE DU TEXTE (supprimer le bruit HTML)\n",
    "            # ─────────────────────────────────────────────────────\n",
    "            \n",
    "            raw_doc = docs[0]\n",
    "            raw_text = raw_doc.page_content\n",
    "            \n",
    "            # Parsing avec BeautifulSoup pour un nettoyage plus précis\n",
    "            soup = BeautifulSoup(raw_text, 'html.parser')\n",
    "            \n",
    "            # Supprimer navigation, footer, scripts\n",
    "            for tag in soup([\"nav\", \"footer\", \"script\", \"style\", \"aside\"]):\n",
    "                tag.decompose()\n",
    "            \n",
    "            # Extraire le texte propre\n",
    "            clean_text = soup.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            # Supprimer lignes vides multiples\n",
    "            lines = [line for line in clean_text.split('\\n') if line.strip()]\n",
    "            clean_text = '\\n'.join(lines)\n",
    "            \n",
    "            # ─────────────────────────────────────────────────────\n",
    "            # 3. CHUNKING (si l'article est long)\n",
    "            # ─────────────────────────────────────────────────────\n",
    "            \n",
    "            if chunk_articles and len(clean_text) > 2000:  # Articles > 2000 caractères\n",
    "                from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "                \n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=1000,\n",
    "                    chunk_overlap=150,\n",
    "                    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "                )\n",
    "                \n",
    "                chunks = splitter.split_text(clean_text)\n",
    "                \n",
    "                # Créer un document par chunk\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    doc = Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"source\": \"press_release\",\n",
    "                            \"url\": url,\n",
    "                            \"scrape_date\": datetime.now().isoformat(),\n",
    "                            \"chunk_index\": i,\n",
    "                            \"total_chunks\": len(chunks),\n",
    "                            \"namespace\": \"news\"\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                \n",
    "                log_ingestion(\"PRESS_RELEASE\", \"SUCCESS\", \n",
    "                    f\"Article chargé et chunké en {len(chunks)} parties depuis {url}\")\n",
    "            \n",
    "            else:\n",
    "                # Article court, pas de chunking\n",
    "                doc = Document(\n",
    "                    page_content=clean_text,\n",
    "                    metadata={\n",
    "                        \"source\": \"press_release\",\n",
    "                        \"url\": url,\n",
    "                        \"scrape_date\": datetime.now().isoformat(),\n",
    "                        \"namespace\": \"news\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                log_ingestion(\"PRESS_RELEASE\", \"SUCCESS\", f\"Article court chargé depuis {url}\")\n",
    "            \n",
    "            time.sleep(2)  # Rate limiting éthique\n",
    "        \n",
    "        except Exception as e:\n",
    "            log_ingestion(\"PRESS_RELEASE\", \"ERROR\", f\"{url} - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE 4 : GOOGLE NEWS RSS\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_google_news_rss(query: str, limit: int = 10) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Charge des actualités depuis Google News RSS (GRATUIT)\n",
    "    \n",
    "    Alternative à Google News API qui n'existe plus en version gratuite.\n",
    "    Utilise les flux RSS publics de Google News.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL du flux RSS Google News\n",
    "        # Format : https://news.google.com/rss/search?q=QUERY&hl=en&gl=US\n",
    "        query_encoded = requests.utils.quote(query)\n",
    "        rss_url = f\"https://news.google.com/rss/search?q={query_encoded}&hl=en&gl=US&ceid=US:en\"\n",
    "        \n",
    "        log_ingestion(\"GOOGLE_NEWS_RSS\", \"INFO\", f\"Flux RSS pour '{query}'\")\n",
    "        \n",
    "        # Parser le flux RSS avec feedparser\n",
    "        feed = feedparser.parse(rss_url)\n",
    "        \n",
    "        documents = []\n",
    "        for entry in feed.entries[:limit]:\n",
    "            content = f\"{entry.title}. {entry.get('summary', '')}\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": \"google_news_rss\",\n",
    "                    \"title\": entry.title,\n",
    "                    \"published_at\": entry.get(\"published\", \"Unknown\"),\n",
    "                    \"url\": entry.link,\n",
    "                    \"source_name\": entry.get(\"source\", {}).get(\"title\", \"Google News\"),\n",
    "                    \"namespace\": \"news\"\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        log_ingestion(\"GOOGLE_NEWS_RSS\", \"SUCCESS\", f\"{len(documents)} articles récupérés\")\n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"GOOGLE_NEWS_RSS\", \"ERROR\", str(e))\n",
    "        return []\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE 5 : YFINANCE --> gérer les problèmes API.\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_yfinance_data(ticker: str, max_retries: int = 3) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Charge les données financières avec gestion du rate limiting\n",
    "    \n",
    "    CORRECTIONS :\n",
    "    - Ajout de délais entre requêtes (6 secondes)\n",
    "    - Retry en cas d'erreur 429\n",
    "    - Fallback vers données basiques si quoteSummary échoue\n",
    "    - User-Agent personnalisé\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_ingestion(\"YFINANCE\", \"INFO\", f\"Récupération données pour {ticker}\")\n",
    "        \n",
    "        # Délai AVANT la requête pour éviter 429\n",
    "        time.sleep(6)\n",
    "        \n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        # Tentative 1 : Données complètes\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                info = stock.info\n",
    "                \n",
    "                # Vérifier si on a bien récupéré des données\n",
    "                if info and 'symbol' in info:\n",
    "                    break\n",
    "                \n",
    "                log_ingestion(\"YFINANCE\", \"WARNING\", f\"Tentative {attempt + 1}/{max_retries} - Données incomplètes\")\n",
    "                time.sleep(10)  # Attente plus longue entre retry\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    log_ingestion(\"YFINANCE\", \"WARNING\", f\"Rate limit - Retry dans 15s (tentative {attempt + 1})\")\n",
    "                    time.sleep(15)\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        # Si toujours pas de données, fallback vers historique\n",
    "        if not info or 'symbol' not in info:\n",
    "            log_ingestion(\"YFINANCE\", \"WARNING\", f\"Fallback vers données historiques pour {ticker}\")\n",
    "            \n",
    "            hist = stock.history(period=\"1d\")\n",
    "            if hist.empty:\n",
    "                raise ValueError(\"Aucune donnée disponible\")\n",
    "            \n",
    "            current_price = hist['Close'].iloc[-1]\n",
    "            \n",
    "            content = f\"\"\"\n",
    "            Financial Data for {ticker}:\n",
    "            - Current Price: ${current_price:.2f}\n",
    "            - Data Source: Historical prices (quoteSummary unavailable due to rate limiting)\n",
    "            - Last Updated: {datetime.now().strftime('%Y-%m-%d')}\n",
    "            \"\"\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": \"yfinance\",\n",
    "                    \"ticker\": ticker,\n",
    "                    \"current_price\": float(current_price),\n",
    "                    \"retrieval_date\": datetime.now().isoformat(),\n",
    "                    \"namespace\": \"macro_data\",\n",
    "                    \"data_type\": \"historical_fallback\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            log_ingestion(\"YFINANCE\", \"SUCCESS\", f\"Données historiques récupérées pour {ticker}\")\n",
    "            return [doc]\n",
    "        \n",
    "        # Données complètes disponibles\n",
    "        metrics = {\n",
    "            \"market_cap\": info.get(\"marketCap\"),\n",
    "            \"revenue\": info.get(\"totalRevenue\"),\n",
    "            \"profit_margin\": info.get(\"profitMargins\"),\n",
    "            \"pe_ratio\": info.get(\"trailingPE\"),\n",
    "            \"current_price\": info.get(\"currentPrice\"),\n",
    "            \"52week_high\": info.get(\"fiftyTwoWeekHigh\"),\n",
    "            \"52week_low\": info.get(\"fiftyTwoWeekLow\")\n",
    "        }\n",
    "        \n",
    "        # Formatage sécurisé\n",
    "        def format_value(v, prefix=\"$\", suffix=\"\", format_spec=\",\"):\n",
    "            if v is None:\n",
    "                return \"N/A\"\n",
    "            if isinstance(v, (int, float)):\n",
    "                return f\"{prefix}{v:{format_spec}}{suffix}\"\n",
    "            return str(v)\n",
    "        \n",
    "        content = f\"\"\"\n",
    "        Financial Overview for {ticker}:\n",
    "        - Market Cap: {format_value(metrics['market_cap'])}\n",
    "        - Revenue: {format_value(metrics['revenue'])}\n",
    "        - Profit Margin: {format_value(metrics['profit_margin'], prefix=\"\", suffix=\"%\", format_spec=\".2f\")}\n",
    "        - P/E Ratio: {format_value(metrics['pe_ratio'], prefix=\"\", format_spec=\".2f\")}\n",
    "        - Current Price: {format_value(metrics['current_price'], format_spec=\".2f\")}\n",
    "        - 52-Week Range: {format_value(metrics['52week_low'], format_spec=\".2f\")} - {format_value(metrics['52week_high'], format_spec=\".2f\")}\n",
    "        \"\"\"\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"source\": \"yfinance\",\n",
    "                \"ticker\": ticker,\n",
    "                \"company_name\": info.get(\"longName\", ticker),\n",
    "                \"sector\": info.get(\"sector\") or \"Unknown\",\n",
    "                \"industry\": info.get(\"industry\") or \"Unknown\",\n",
    "                \"retrieval_date\": datetime.now().isoformat(),\n",
    "                \"namespace\": \"macro_data\",\n",
    "                **{k: v for k, v in metrics.items() if v is not None}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        log_ingestion(\"YFINANCE\", \"SUCCESS\", f\"Données complètes récupérées pour {ticker}\")\n",
    "        return [doc]\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"YFINANCE\", \"ERROR\", f\"{ticker} - {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE 6 : FRED \n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_fred_macro_data(series_ids: List[str] = [\"GDP\", \"CPIAUCSL\", \"FEDFUNDS\", \"UNRATE\"]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Charge les indicateurs macro-économiques depuis St. Louis FRED.\n",
    "    Par défaut : PIB, Inflation (CPI), Taux de la FED, Chômage.\n",
    "    \"\"\"\n",
    "    if not FRED_API_KEY:\n",
    "        log_ingestion(\"FRED\", \"ERROR\", \"Clé API FRED_API_KEY manquante dans .env\")\n",
    "        return []\n",
    "\n",
    "    documents = []\n",
    "    \n",
    "    for series_id in series_ids:\n",
    "        try:\n",
    "            log_ingestion(\"FRED\", \"INFO\", f\"Récupération de la série : {series_id}\")\n",
    "            \n",
    "            # 1. Récupérer les infos de la série (nom, unités)\n",
    "            info_url = f\"{FRED_BASE_URL}series?series_id={series_id}&api_key={FRED_API_KEY}&file_type=json\"\n",
    "            info_resp = requests.get(info_url, timeout=10)\n",
    "            info_resp.raise_for_status()\n",
    "            series_info = info_resp.json()['seriess'][0]\n",
    "            \n",
    "            # 2. Récupérer les dernières observations (les 5 dernières pour le contexte)\n",
    "            obs_url = f\"{FRED_BASE_URL}series/observations?series_id={series_id}&api_key={FRED_API_KEY}&sort_order=desc&limit=5&file_type=json\"\n",
    "            obs_resp = requests.get(obs_url, timeout=10)\n",
    "            obs_resp.raise_for_status()\n",
    "            observations = obs_resp.json()['observations']\n",
    "\n",
    "            # 3. Création du contenu textuel pour le RAG\n",
    "            title = series_info.get('title', series_id)\n",
    "            units = series_info.get('units_short', '')\n",
    "            obs_text = \"\\n\".join([f\"- Date: {o['date']}, Valeur: {o['value']} {units}\" for o in observations])\n",
    "            \n",
    "            content = f\"Série Économique : {title} ({series_id}).\\n\" \\\n",
    "                      f\"Description : {series_info.get('notes', 'N/A')}\\n\" \\\n",
    "                      f\"Dernières observations :\\n{obs_text}\"\n",
    "\n",
    "            # 4. Formatage Document (compatible avec votre pipeline)\n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"source\": \"fred\",\n",
    "                    \"series_id\": series_id,\n",
    "                    \"title\": title,\n",
    "                    \"last_updated\": observations[0]['date'] if observations else \"Unknown\",\n",
    "                    \"namespace\": \"macro_data\"  # Envoi direct dans le bon namespace Pinecone\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "            \n",
    "            time.sleep(0.5) # Respect du rate limit de la FRED\n",
    "            log_ingestion(\"FRED\", \"SUCCESS\", f\"Série {series_id} chargée avec succès\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_ingestion(\"FRED\", \"ERROR\", f\"Erreur pour {series_id}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE : API SIRENE (INSEE - FRANCE)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_sirene_data_by_siren(siren_list, delay_seconds=0.5):\n",
    "\n",
    "    headers = {\n",
    "        \"X-INSEE-Api-Key-Integration\": API_KEY,\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"User-Agent\": \"curl/8.4.0\"\n",
    "    }\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for siren in siren_list:\n",
    "        siren = siren.strip()\n",
    "        url = f\"{BASE_URL}/siren/{siren}\"\n",
    "\n",
    "        log_ingestion(\"SIRENE\", \"INFO\", f\"Requête SIREN {siren}\")\n",
    "\n",
    "        try:\n",
    "            # ─────────────────────────────────────────────\n",
    "            # APPEL API (curl-like)\n",
    "            # ─────────────────────────────────────────────\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                log_ingestion(\"SIRENE\", \"ERROR\", f\"SIREN {siren} - HTTP {response.status_code}\")\n",
    "                continue\n",
    "\n",
    "            data = response.json()\n",
    "            unite = data.get(\"uniteLegale\", {})\n",
    "\n",
    "            if not unite:\n",
    "                log_ingestion(\"SIRENE\", \"ERROR\", f\"SIREN {siren} - JSON vide\")\n",
    "                continue\n",
    "\n",
    "            # ─────────────────────────────────────────────\n",
    "            # TRI DU JSON (ON GARDE L’UTILE)\n",
    "            # ─────────────────────────────────────────────\n",
    "\n",
    "            # période courante = dateFin == null\n",
    "            current_period = None\n",
    "            for p in unite.get(\"periodesUniteLegale\", []):\n",
    "                if p.get(\"dateFin\") is None:\n",
    "                    current_period = p\n",
    "                    break\n",
    "\n",
    "            if not current_period:\n",
    "                log_ingestion(\"SIRENE\", \"ERROR\", f\"SIREN {siren} - période courante absente\")\n",
    "                continue\n",
    "\n",
    "            # anciens noms\n",
    "            old_names = []\n",
    "            old_activities = []\n",
    "\n",
    "            for p in unite.get(\"periodesUniteLegale\", []):\n",
    "                name = p.get(\"denominationUniteLegale\")\n",
    "                activity = p.get(\"activitePrincipaleUniteLegale\")\n",
    "\n",
    "                if name and name not in old_names:\n",
    "                    old_names.append(name)\n",
    "\n",
    "                if activity and activity not in old_activities:\n",
    "                    old_activities.append(activity)\n",
    "\n",
    "            # JSON nettoyé (MENTALEMENT SIMPLE)\n",
    "            cleaned = {\n",
    "                \"siren\": unite.get(\"siren\"),\n",
    "                \"date_creation\": unite.get(\"dateCreationUniteLegale\"),\n",
    "                \"categorie_entreprise\": unite.get(\"categorieEntreprise\"),\n",
    "                \"effectifs\": {\n",
    "                    \"tranche\": unite.get(\"trancheEffectifsUniteLegale\"),\n",
    "                    \"annee\": unite.get(\"anneeEffectifsUniteLegale\")\n",
    "                },\n",
    "                \"current\": {\n",
    "                    \"denomination\": current_period.get(\"denominationUniteLegale\"),\n",
    "                    \"naf\": current_period.get(\"activitePrincipaleUniteLegale\"),\n",
    "                    \"categorie_juridique\": current_period.get(\"categorieJuridiqueUniteLegale\"),\n",
    "                    \"etat\": current_period.get(\"etatAdministratifUniteLegale\")\n",
    "                },\n",
    "                \"history\": {\n",
    "                    \"denominations\": old_names,\n",
    "                    \"activities\": old_activities\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # ─────────────────────────────────────────────\n",
    "            # TRANSFORMATION POUR LE PIPELINE (Document)\n",
    "            # ─────────────────────────────────────────────\n",
    "\n",
    "            page_content = f\"\"\"\n",
    "            ENTREPRISE – INSEE SIRENE\n",
    "            SIREN : {cleaned['siren']}\n",
    "            Dénomination actuelle : {cleaned['current']['denomination']}\n",
    "            Date de création : {cleaned['date_creation']}\n",
    "            Catégorie entreprise : {cleaned['categorie_entreprise']}\n",
    "            Effectifs : {cleaned['effectifs']['tranche']} (année {cleaned['effectifs']['annee']})\n",
    "            Activité principale (NAF) : {cleaned['current']['naf']}\n",
    "            Catégorie juridique : {cleaned['current']['categorie_juridique']}\n",
    "            État administratif : {cleaned['current']['etat']}\n",
    "\n",
    "            Historique des noms :\n",
    "            {' → '.join(cleaned['history']['denominations'])}\n",
    "\n",
    "            Historique des activités :\n",
    "            {', '.join(cleaned['history']['activities'])}\n",
    "            \"\"\".strip()\n",
    "\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    page_content=page_content,\n",
    "                    metadata={\n",
    "                        \"source\": \"INSEE_SIRENE\",\n",
    "                        \"siren\": cleaned[\"siren\"],\n",
    "                        \"country\": \"FR\"\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "            log_ingestion(\"SIRENE\", \"SUCCESS\", f\"SIREN {siren} ingéré\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_ingestion(\"SIRENE\", \"ERROR\", f\"SIREN {siren} - {str(e)}\")\n",
    "\n",
    "        time.sleep(delay_seconds)\n",
    "\n",
    "    return documents\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE : DBNOMICS (DONNÉES MACRO MONDIALES)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_dbnomics_data(\n",
    "    series_list: List[Dict[str, str]],\n",
    "    max_observations: int = 12\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Charge des séries économiques depuis DBnomics.\n",
    "    \n",
    "    Paramètres :\n",
    "    -----------\n",
    "    series_list : List[Dict[str, str]]\n",
    "        Liste de dictionnaires avec les clés:\n",
    "        - 'provider': Code du fournisseur (ex: 'OECD')\n",
    "        - 'dataset': Code du dataset (ex: 'QNA')\n",
    "        - 'series': Code de la série (ex: 'FRA.B1_GE.CARSA.Q')\n",
    "        - 'name': (optionnel) Nom descriptif pour le logging\n",
    "    \n",
    "    max_observations : int\n",
    "        Nombre max d'observations à inclure (dernières valeurs)\n",
    "    \n",
    "    Exemple d'utilisation :\n",
    "    ----------------------\n",
    "    series = [\n",
    "        {\n",
    "            'provider': 'OECD',\n",
    "            'dataset': 'QNA',\n",
    "            'series': 'FRA.B1_GE.CARSA.Q',\n",
    "            'name': 'PIB France (trimestriel)'\n",
    "        },\n",
    "        {\n",
    "            'provider': 'OECD',\n",
    "            'dataset': 'MEI',\n",
    "            'series': 'FRA.LR.STSA.M',\n",
    "            'name': 'Taux chômage France'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    docs = load_dbnomics_data(series)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        documents = []\n",
    "        \n",
    "        log_ingestion(\"DBNOMICS\", \"INFO\", f\"Récupération de {len(series_list)} séries\")\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # RÉCUPÉRATION DE CHAQUE SÉRIE\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        for series_info in series_list:\n",
    "            provider = series_info['provider']\n",
    "            dataset = series_info['dataset']\n",
    "            series_code = series_info['series']\n",
    "            series_name = series_info.get('name', f\"{provider}/{dataset}/{series_code}\")\n",
    "            \n",
    "            try:\n",
    "                log_ingestion(\"DBNOMICS\", \"INFO\", f\"Fetch: {series_name}\")\n",
    "                \n",
    "                # Appel API DBnomics\n",
    "                df = db.fetch_series(provider, dataset, series_code)\n",
    "                \n",
    "                if df is None or df.empty:\n",
    "                    log_ingestion(\"DBNOMICS\", \"WARNING\", f\"Série vide: {series_name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Formatage en Document LangChain\n",
    "                doc = _format_dbnomics_series(df, max_observations, series_name)\n",
    "                documents.append(doc)\n",
    "                \n",
    "                # Rate limiting éthique\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            except Exception as e:\n",
    "                log_ingestion(\"DBNOMICS\", \"ERROR\", f\"{series_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # LOGGING FINAL\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        log_ingestion(\"DBNOMICS\", \"SUCCESS\", f\"{len(documents)} séries récupérées\")\n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"DBNOMICS\", \"ERROR\", f\"Erreur globale: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def _format_dbnomics_series(df: pd.DataFrame, max_obs: int, series_name: str) -> Document:\n",
    "    \"\"\"\n",
    "    Formate une série DBnomics en Document LangChain.\n",
    "    \n",
    "    Fonction utilitaire interne.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tri par date décroissante\n",
    "    df_sorted = df.sort_values('period', ascending=False).head(max_obs)\n",
    "    \n",
    "    # Extraction des métadonnées (colonnes toujours présentes)\n",
    "    provider = df['provider_code'].iloc[0]\n",
    "    dataset = df['dataset_code'].iloc[0]\n",
    "    full_series_code = df['series_code'].iloc[0]\n",
    "    \n",
    "    # Informations optionnelles (peuvent ne pas exister)\n",
    "    frequency = df.get('@frequency', pd.Series(['Unknown'] * len(df))).iloc[0]\n",
    "    unit = df.get('unit', pd.Series([''] * len(df))).iloc[0]\n",
    "    \n",
    "    # Construction du texte des observations\n",
    "    observations_text = []\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        period = row['period']\n",
    "        value = row['value']\n",
    "        \n",
    "        if pd.notna(value):\n",
    "            if isinstance(value, (int, float)):\n",
    "                value_str = f\"{value:,.2f}\"\n",
    "            else:\n",
    "                value_str = str(value)\n",
    "        else:\n",
    "            value_str = \"N/A\"\n",
    "        \n",
    "        observations_text.append(f\"  - {period} : {value_str} {unit}\")\n",
    "    \n",
    "    # Contenu pour le RAG\n",
    "    content = f\"\"\"\n",
    "    SÉRIE ÉCONOMIQUE DBnomics\n",
    "\n",
    "    Source : {provider} / {dataset}\n",
    "    Série : {series_name}\n",
    "    Code complet : {full_series_code}\n",
    "    Fréquence : {frequency}\n",
    "    Unité : {unit}\n",
    "\n",
    "    Dernières observations ({len(df_sorted)} valeurs) :\n",
    "    {chr(10).join(observations_text)}\n",
    "\n",
    "    Dernière mise à jour : {df_sorted['period'].iloc[0]}\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    # Calcul de statistiques\n",
    "    values = df_sorted['value'].dropna()\n",
    "    \n",
    "    metadata = {\n",
    "        \"source\": \"dbnomics\",\n",
    "        \"provider\": provider,\n",
    "        \"dataset\": dataset,\n",
    "        \"series_code\": full_series_code,\n",
    "        \"series_name\": series_name,\n",
    "        \"frequency\": str(frequency),\n",
    "        \"unit\": str(unit),\n",
    "        \"last_period\": str(df_sorted['period'].iloc[0]),\n",
    "        \"observations_count\": len(df_sorted),\n",
    "        \"namespace\": \"macro_data\"\n",
    "    }\n",
    "    \n",
    "    # Ajout des stats si données numériques\n",
    "    if not values.empty and len(values) > 1:\n",
    "        metadata[\"last_value\"] = float(values.iloc[0])\n",
    "        metadata[\"mean\"] = float(values.mean())\n",
    "        metadata[\"min\"] = float(values.min())\n",
    "        metadata[\"max\"] = float(values.max())\n",
    "    \n",
    "    return Document(\n",
    "        page_content=content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE : ADZUNA (OFFRES D'EMPLOI & MARCHÉ DU TRAVAIL)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_adzuna_data(\n",
    "    what: str = None,\n",
    "    where: str = None,\n",
    "    country: str = \"fr\",\n",
    "    results_per_page: int = 20,\n",
    "    page: int = 1,\n",
    "    salary_min: int = None,\n",
    "    category: str = None,\n",
    "    endpoint_type: str = \"search\"\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Charge des données du marché de l'emploi depuis l'API Adzuna.\n",
    "    \n",
    "    Paramètres :\n",
    "    -----------\n",
    "    what : str\n",
    "        Mots-clés de recherche (ex: \"data scientist\", \"consultant stratégie\")\n",
    "    where : str\n",
    "        Localisation (ex: \"Paris\", \"Lyon\", \"France\")\n",
    "    country : str\n",
    "        Code pays (fr, gb, us, de, etc.)\n",
    "    results_per_page : int\n",
    "        Nombre de résultats par page (max 50)\n",
    "    page : int\n",
    "        Numéro de page (pagination)\n",
    "    salary_min : int\n",
    "        Salaire minimum annuel\n",
    "    category : str\n",
    "        Catégorie d'emploi (ex: \"it-jobs\", \"consultancy-jobs\")\n",
    "    endpoint_type : str\n",
    "        Type de données : \"search\", \"histogram\", \"top_companies\"\n",
    "    \n",
    "    Exemples d'utilisation :\n",
    "    ------------------------\n",
    "    # Recherche d'offres\n",
    "    load_adzuna_data(what=\"data scientist\", where=\"Paris\", results_per_page=30)\n",
    "    \n",
    "    # Tendances salariales\n",
    "    load_adzuna_data(what=\"consultant\", endpoint_type=\"histogram\")\n",
    "    \n",
    "    # Top recruteurs\n",
    "    load_adzuna_data(what=\"fintech\", endpoint_type=\"top_companies\")\n",
    "    \"\"\"\n",
    "    \n",
    "    APP_ID = os.getenv(\"ADZUNA_APP_ID\")\n",
    "    APP_KEY = os.getenv(\"ADZUNA_APP_KEY\")\n",
    "    \n",
    "    if not APP_ID or not APP_KEY:\n",
    "        log_ingestion(\"ADZUNA\", \"ERROR\", \"Clés ADZUNA_APP_ID ou ADZUNA_APP_KEY manquantes dans .env\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        documents = []\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 1. CONSTRUCTION DE L'URL SELON LE TYPE D'ENDPOINT\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        BASE_URL = \"https://api.adzuna.com/v1/api/jobs\"\n",
    "        \n",
    "        if endpoint_type == \"search\":\n",
    "            url = f\"{BASE_URL}/{country}/search/{page}\"\n",
    "        elif endpoint_type == \"histogram\":\n",
    "            url = f\"{BASE_URL}/{country}/histogram\"\n",
    "        elif endpoint_type == \"top_companies\":\n",
    "            url = f\"{BASE_URL}/{country}/top_companies\"\n",
    "        else:\n",
    "            log_ingestion(\"ADZUNA\", \"ERROR\", f\"Type d'endpoint inconnu: {endpoint_type}\")\n",
    "            return []\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 2. PARAMÈTRES DE LA REQUÊTE\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        params = {\n",
    "            \"app_id\": APP_ID,\n",
    "            \"app_key\": APP_KEY,\n",
    "            \"content-type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Ajout des paramètres optionnels\n",
    "        if what:\n",
    "            params[\"what\"] = what\n",
    "        if where:\n",
    "            params[\"where\"] = where\n",
    "        if results_per_page and endpoint_type == \"search\":\n",
    "            params[\"results_per_page\"] = min(results_per_page, 50)  # Max 50 par page\n",
    "        if salary_min:\n",
    "            params[\"salary_min\"] = salary_min\n",
    "        if category:\n",
    "            params[\"category\"] = category\n",
    "        \n",
    "        log_ingestion(\"ADZUNA\", \"INFO\", \n",
    "            f\"Requête {endpoint_type}: what='{what}', where='{where}', country={country}\")\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 3. APPEL API\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        response = requests.get(url, params=params, timeout=15)\n",
    "        \n",
    "        # Debug\n",
    "        if response.status_code != 200:\n",
    "            log_ingestion(\"ADZUNA\", \"ERROR\", \n",
    "                f\"Code {response.status_code} - URL: {response.url[:100]}...\")\n",
    "        \n",
    "        # Gestion des erreurs\n",
    "        if response.status_code == 401:\n",
    "            log_ingestion(\"ADZUNA\", \"ERROR\", \"Authentification échouée - Vérifiez vos clés API\")\n",
    "            return []\n",
    "        elif response.status_code == 429:\n",
    "            log_ingestion(\"ADZUNA\", \"WARNING\", \"Rate limit atteint - Patientez 1 minute\")\n",
    "            return []\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 4. PARSING SELON LE TYPE D'ENDPOINT\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        if endpoint_type == \"search\":\n",
    "            documents = _parse_adzuna_jobs(data, what, where, country)\n",
    "        \n",
    "        elif endpoint_type == \"histogram\":\n",
    "            documents = _parse_adzuna_histogram(data, what, where, country)\n",
    "        \n",
    "        elif endpoint_type == \"top_companies\":\n",
    "            documents = _parse_adzuna_top_companies(data, what, where, country)\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 5. LOGGING FINAL\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        log_ingestion(\"ADZUNA\", \"SUCCESS\", f\"{len(documents)} documents créés\")\n",
    "        return documents\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_ingestion(\"ADZUNA\", \"ERROR\", f\"Erreur réseau: {str(e)}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        log_ingestion(\"ADZUNA\", \"ERROR\", f\"Erreur: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# FONCTIONS UTILITAIRES DE PARSING\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def _parse_adzuna_jobs(data: dict, what: str, where: str, country: str) -> List[Document]:\n",
    "    \"\"\"Parse les offres d'emploi\"\"\"\n",
    "    \n",
    "    documents = []\n",
    "    results = data.get(\"results\", [])\n",
    "    count = data.get(\"count\", 0)\n",
    "    mean_salary = data.get(\"mean\", 0)\n",
    "    \n",
    "    log_ingestion(\"ADZUNA\", \"INFO\", \n",
    "        f\"Trouvé {count} offres (moyenne salaire: {mean_salary:.0f}€)\")\n",
    "    \n",
    "    for job in results:\n",
    "        title = job.get(\"title\", \"N/A\")\n",
    "        company = job.get(\"company\", {}).get(\"display_name\", \"N/A\")\n",
    "        location = job.get(\"location\", {}).get(\"display_name\", \"N/A\")\n",
    "        description = job.get(\"description\", \"\")[:500]  # Limiter la description\n",
    "        \n",
    "        salary_min = job.get(\"salary_min\")\n",
    "        salary_max = job.get(\"salary_max\")\n",
    "        salary_text = \"\"\n",
    "        if salary_min and salary_max:\n",
    "            salary_text = f\"{salary_min:,.0f}€ - {salary_max:,.0f}€\"\n",
    "        elif salary_min:\n",
    "            salary_text = f\"À partir de {salary_min:,.0f}€\"\n",
    "        else:\n",
    "            salary_text = \"Non communiqué\"\n",
    "        \n",
    "        created = job.get(\"created\", \"N/A\")\n",
    "        category = job.get(\"category\", {}).get(\"label\", \"N/A\")\n",
    "        contract_type = job.get(\"contract_type\", \"N/A\")\n",
    "        \n",
    "        # Contenu pour le RAG\n",
    "        content = f\"\"\"\n",
    "OFFRE D'EMPLOI - Adzuna\n",
    "\n",
    "Poste : {title}\n",
    "Entreprise : {company}\n",
    "Localisation : {location}\n",
    "\n",
    "Rémunération : {salary_text}\n",
    "Type de contrat : {contract_type}\n",
    "Catégorie : {category}\n",
    "Date de publication : {created}\n",
    "\n",
    "Description :\n",
    "{description}...\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        # Métadonnées\n",
    "        metadata = {\n",
    "            \"source\": \"adzuna\",\n",
    "            \"job_title\": title,\n",
    "            \"company\": company,\n",
    "            \"location\": location,\n",
    "            \"salary_min\": salary_min,\n",
    "            \"salary_max\": salary_max,\n",
    "            \"category\": category,\n",
    "            \"contract_type\": contract_type,\n",
    "            \"created\": created,\n",
    "            \"search_query\": what or \"\",\n",
    "            \"search_location\": where or \"\",\n",
    "            \"country\": country,\n",
    "            \"namespace\": \"macro_data\",  # Ou \"news\" selon votre logique\n",
    "            \"url\": job.get(\"redirect_url\", \"\")\n",
    "        }\n",
    "        \n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def _parse_adzuna_histogram(data: dict, what: str, where: str, country: str) -> List[Document]:\n",
    "    \"\"\"Parse la distribution salariale (histogram)\"\"\"\n",
    "    \n",
    "    histogram = data.get(\"histogram\", {})\n",
    "    \n",
    "    if not histogram:\n",
    "        log_ingestion(\"ADZUNA\", \"WARNING\", \"Histogram vide\")\n",
    "        return []\n",
    "    \n",
    "    # Formatage des données de distribution\n",
    "    salary_ranges = []\n",
    "    for salary_str, count in histogram.items():\n",
    "        salary_ranges.append(f\"  - {salary_str}€ : {count} offres\")\n",
    "    \n",
    "    content = f\"\"\"\n",
    "ANALYSE SALARIALE - Adzuna\n",
    "\n",
    "Recherche : {what or 'Tous métiers'}\n",
    "Localisation : {where or 'France entière'}\n",
    "Pays : {country.upper()}\n",
    "\n",
    "Distribution des salaires :\n",
    "{chr(10).join(sorted(salary_ranges))}\n",
    "\n",
    "Date de l'analyse : {datetime.now().strftime('%Y-%m-%d')}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    metadata = {\n",
    "        \"source\": \"adzuna\",\n",
    "        \"data_type\": \"salary_histogram\",\n",
    "        \"search_query\": what or \"\",\n",
    "        \"search_location\": where or \"\",\n",
    "        \"country\": country,\n",
    "        \"namespace\": \"macro_data\",\n",
    "        \"histogram_data\": histogram\n",
    "    }\n",
    "    \n",
    "    return [Document(page_content=content, metadata=metadata)]\n",
    "\n",
    "\n",
    "def _parse_adzuna_top_companies(data: dict, what: str, where: str, country: str) -> List[Document]:\n",
    "    \"\"\"Parse le top des entreprises qui recrutent\"\"\"\n",
    "    \n",
    "    leaderboard = data.get(\"leaderboard\", [])\n",
    "    \n",
    "    if not leaderboard:\n",
    "        log_ingestion(\"ADZUNA\", \"WARNING\", \"Top companies vide\")\n",
    "        return []\n",
    "    \n",
    "    # Formatage du classement\n",
    "    top_companies_text = []\n",
    "    for i, company_data in enumerate(leaderboard[:20], 1):  # Top 20\n",
    "        company_name = company_data.get(\"canonical_name\", \"N/A\")\n",
    "        count = company_data.get(\"count\", 0)\n",
    "        top_companies_text.append(f\"  {i}. {company_name} : {count} offres\")\n",
    "    \n",
    "    content = f\"\"\"\n",
    "TOP RECRUTEURS - Adzuna\n",
    "\n",
    "Recherche : {what or 'Tous secteurs'}\n",
    "Localisation : {where or 'France entière'}\n",
    "\n",
    "Classement des entreprises qui recrutent le plus :\n",
    "{chr(10).join(top_companies_text)}\n",
    "\n",
    "Date de l'analyse : {datetime.now().strftime('%Y-%m-%d')}\n",
    "Total entreprises analysées : {len(leaderboard)}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    metadata = {\n",
    "        \"source\": \"adzuna\",\n",
    "        \"data_type\": \"top_companies\",\n",
    "        \"search_query\": what or \"\",\n",
    "        \"search_location\": where or \"\",\n",
    "        \"country\": country,\n",
    "        \"namespace\": \"macro_data\",\n",
    "        \"top_20\": [c.get(\"canonical_name\") for c in leaderboard[:20]]\n",
    "    }\n",
    "    \n",
    "    return [Document(page_content=content, metadata=metadata)]\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SOURCE : BLUESKY (SIGNAUX SOCIAUX & VEILLE)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_bluesky_data(\n",
    "    search_queries: List[str] = None,\n",
    "    author_handle: str = None,\n",
    "    limit: int = 25,\n",
    "    lang: str = \"fr\",\n",
    "    since_days: int = 7\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Charge des posts depuis Bluesky pour la veille stratégique.\n",
    "    \n",
    "    Paramètres :\n",
    "    -----------\n",
    "    search_queries : List[str]\n",
    "        Liste de requêtes de recherche (ex: [\"fintech\", \"LVMH\", \"data science\"])\n",
    "    author_handle : str\n",
    "        Handle d'un auteur spécifique (ex: \"company.bsky.social\")\n",
    "    limit : int\n",
    "        Nombre max de posts par requête (max 100)\n",
    "    lang : str\n",
    "        Code langue (fr, en, etc.)\n",
    "    since_days : int\n",
    "        Chercher les posts des X derniers jours\n",
    "    \n",
    "    Exemples d'utilisation :\n",
    "    ------------------------\n",
    "    # Recherche par mots-clés\n",
    "    load_bluesky_data(search_queries=[\"fintech France\", \"AI startup\"], limit=30)\n",
    "    \n",
    "    # Posts d'un compte spécifique\n",
    "    load_bluesky_data(author_handle=\"techcrunch.bsky.social\", limit=20)\n",
    "    \n",
    "    # Veille multi-thèmes\n",
    "    load_bluesky_data(search_queries=[\"KPMG\", \"audit digital\", \"ESG reporting\"])\n",
    "    \"\"\"\n",
    "    \n",
    "    HANDLE = os.getenv(\"BLUESKY_HANDLE\")\n",
    "    PASSWORD = os.getenv(\"BLUESKY_APP_PASSWORD\")\n",
    "    \n",
    "    if not HANDLE or not PASSWORD:\n",
    "        log_ingestion(\"BLUESKY\", \"ERROR\", \n",
    "            \"Credentials BLUESKY_HANDLE ou BLUESKY_APP_PASSWORD manquants\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        documents = []\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 1. AUTHENTIFICATION\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        log_ingestion(\"BLUESKY\", \"INFO\", \"Connexion à Bluesky...\")\n",
    "        \n",
    "        client = Client()\n",
    "        profile = client.login(HANDLE, PASSWORD)\n",
    "        \n",
    "        log_ingestion(\"BLUESKY\", \"SUCCESS\", f\"Connecté en tant que {profile.display_name}\")\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 2. COLLECTE DES POSTS\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        if search_queries:\n",
    "            # Mode recherche par mots-clés\n",
    "            for query in search_queries:\n",
    "                log_ingestion(\"BLUESKY\", \"INFO\", f\"Recherche: '{query}'\")\n",
    "                \n",
    "                try:\n",
    "                    # Appel à l'API de recherche\n",
    "                    response = client.app.bsky.feed.search_posts(\n",
    "                        params={\n",
    "                            'q': query,\n",
    "                            'limit': limit,\n",
    "                            'lang': lang\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    posts = response.posts if hasattr(response, 'posts') else []\n",
    "                    \n",
    "                    log_ingestion(\"BLUESKY\", \"INFO\", \n",
    "                        f\"Trouvé {len(posts)} posts pour '{query}'\")\n",
    "                    \n",
    "                    # Parsing des posts\n",
    "                    for post in posts:\n",
    "                        doc = _parse_bluesky_post(post, query)\n",
    "                        if doc:\n",
    "                            documents.append(doc)\n",
    "                    \n",
    "                    time.sleep(1)  # Rate limiting\n",
    "                \n",
    "                except Exception as e:\n",
    "                    log_ingestion(\"BLUESKY\", \"ERROR\", \n",
    "                        f\"Erreur recherche '{query}': {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        elif author_handle:\n",
    "            # Mode posts d'un auteur spécifique\n",
    "            log_ingestion(\"BLUESKY\", \"INFO\", f\"Posts de @{author_handle}\")\n",
    "            \n",
    "            try:\n",
    "                response = client.app.bsky.feed.get_author_feed(\n",
    "                    params={\n",
    "                        'actor': author_handle,\n",
    "                        'limit': limit\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                feed = response.feed if hasattr(response, 'feed') else []\n",
    "                \n",
    "                log_ingestion(\"BLUESKY\", \"INFO\", \n",
    "                    f\"Trouvé {len(feed)} posts de @{author_handle}\")\n",
    "                \n",
    "                for item in feed:\n",
    "                    post = item.post\n",
    "                    doc = _parse_bluesky_post(post, f\"author:{author_handle}\")\n",
    "                    if doc:\n",
    "                        documents.append(doc)\n",
    "            \n",
    "            except Exception as e:\n",
    "                log_ingestion(\"BLUESKY\", \"ERROR\", \n",
    "                    f\"Erreur récupération feed @{author_handle}: {str(e)}\")\n",
    "        \n",
    "        else:\n",
    "            log_ingestion(\"BLUESKY\", \"ERROR\", \n",
    "                \"Aucun critère de recherche fourni (search_queries ou author_handle)\")\n",
    "            return []\n",
    "        \n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        # 3. LOGGING FINAL\n",
    "        # ─────────────────────────────────────────────────────────\n",
    "        \n",
    "        log_ingestion(\"BLUESKY\", \"SUCCESS\", f\"{len(documents)} posts collectés\")\n",
    "        return documents\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"BLUESKY\", \"ERROR\", f\"Erreur globale: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# FONCTION UTILITAIRE DE PARSING\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def _parse_bluesky_post(post, search_query: str = \"\") -> Document:\n",
    "    \"\"\"\n",
    "    Parse un post Bluesky en Document LangChain.\n",
    "    \n",
    "    Fonction utilitaire interne.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Extraction des données du post\n",
    "        author = post.author\n",
    "        record = post.record\n",
    "        \n",
    "        author_handle = author.handle\n",
    "        author_name = author.display_name or author_handle\n",
    "        \n",
    "        # Texte du post\n",
    "        text = record.text\n",
    "        \n",
    "        # Date de création\n",
    "        created_at_str = record.created_at\n",
    "        created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00'))\n",
    "        \n",
    "        # Engagement metrics\n",
    "        like_count = post.like_count or 0\n",
    "        repost_count = post.repost_count or 0\n",
    "        reply_count = post.reply_count or 0\n",
    "        \n",
    "        # URI du post (pour créer le lien)\n",
    "        post_uri = post.uri\n",
    "        # Format URI: at://did:plc:xxx/app.bsky.feed.post/yyy\n",
    "        # Conversion en URL web\n",
    "        post_id = post_uri.split('/')[-1]\n",
    "        post_url = f\"https://bsky.app/profile/{author_handle}/post/{post_id}\"\n",
    "        \n",
    "        # Contenu pour le RAG\n",
    "        content = f\"\"\"\n",
    "POST BLUESKY - Veille Sociale\n",
    "\n",
    "Auteur : @{author_handle} ({author_name})\n",
    "Date : {created_at.strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "Contenu :\n",
    "{text}\n",
    "\n",
    "Engagement :\n",
    "-   {like_count} likes\n",
    "-  {repost_count} reposts\n",
    "-  {reply_count} réponses\n",
    "\n",
    "Lien : {post_url}\n",
    "Recherche associée : {search_query}\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        # Métadonnées\n",
    "        metadata = {\n",
    "            \"source\": \"bluesky\",\n",
    "            \"author_handle\": author_handle,\n",
    "            \"author_name\": author_name,\n",
    "            \"created_at\": created_at.isoformat(),\n",
    "            \"like_count\": like_count,\n",
    "            \"repost_count\": repost_count,\n",
    "            \"reply_count\": reply_count,\n",
    "            \"engagement_total\": like_count + repost_count + reply_count,\n",
    "            \"post_url\": post_url,\n",
    "            \"post_uri\": post_uri,\n",
    "            \"search_query\": search_query,\n",
    "            \"namespace\": \"news\",  # Ou \"macro_data\" selon votre logique\n",
    "            \"text_length\": len(text)\n",
    "        }\n",
    "        \n",
    "        # Ajout de tags si présents\n",
    "        if hasattr(record, 'tags') and record.tags:\n",
    "            metadata[\"tags\"] = record.tags\n",
    "        \n",
    "        # Détection de langue (si disponible)\n",
    "        if hasattr(record, 'langs') and record.langs:\n",
    "            metadata[\"language\"] = record.langs[0]\n",
    "        \n",
    "        return Document(\n",
    "            page_content=content,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_ingestion(\"BLUESKY\", \"WARNING\", f\"Erreur parsing post: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# PIPELINE D'INGESTION COMPLET\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "    \"\"\"\n",
    "    Lance l'ingestion de TOUTES les sources configurées.\n",
    "    \n",
    "    Sources incluses :\n",
    "    - SEC EDGAR (métadonnées + documents complets)\n",
    "    - NewsAPI (20 queries stratégiques)\n",
    "    - Google News RSS\n",
    "    - Communiqués de presse\n",
    "    - yfinance (Top 50 S&P500)\n",
    "    - FRED (données macro US)\n",
    "    - DBnomics (données macro mondiales)\n",
    "    - Bluesky (signaux sociaux)\n",
    "    - INSEE SIRENE (entreprises françaises)\n",
    "    - Adzuna (marché de l'emploi)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def ingest_all_sources() -> Dict[str, List[Document]]:\n",
    "    \"\"\"Lance l'ingestion de toutes les sources\"\"\"\n",
    "    \n",
    "    all_documents = {\n",
    "        \"financial_reports\": [],\n",
    "        \"news\": [],\n",
    "        \"startups\": [],\n",
    "        \"macro_data\": [],\n",
    "        \"social_signals\": []  # AJOUTER\n",
    "    }\n",
    "\n",
    "# \"web_quarantine\" : Pas besoin (créé dynamiquement)\n",
    "# \"facts\" : Pas besoin (créé dans Notebook 3)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\" DÉMARRAGE DE L'INGESTION MULTI-SOURCES\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 1. SEC EDGAR\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    print(\" Chargement SEC EDGAR...\")\n",
    "    from config import SEC_TARGET_COMPANIES\n",
    "        # 1A. Chargement des métadonnées (rapide, garde comme référence)\n",
    "    sec_meta_docs = []\n",
    "    for cik, company_name in SEC_TARGET_COMPANIES:  \n",
    "        print(f\"    Métadonnées : {company_name}\")\n",
    "        meta_docs = load_sec_edgar_filing(cik, \"10-K\", limit=1)  # 2 rapports les plus récents\n",
    "        sec_meta_docs.extend(meta_docs)\n",
    "        time.sleep(1)  # Rate limiting SEC\n",
    "\n",
    "    all_documents[\"financial_reports\"].extend(sec_meta_docs)\n",
    "    print(f\"    {len(sec_meta_docs)} métadonnées de rapports chargées.\")\n",
    "\n",
    "    # 1B. Téléchargement des documents complets (NOUVEAU - prend 5-10 min)\n",
    "    print(\"    Téléchargement des rapports complets...\")\n",
    "    print(\"    Patientez ~10 minutes (fichiers lourds)...\\n\")\n",
    "\n",
    "    sec_full_docs = []\n",
    "    for meta_doc in sec_meta_docs:  # Pour chaque métadonnée, charger le document complet\n",
    "        filing_url = meta_doc.metadata.get(\"url\")\n",
    "        company = meta_doc.metadata.get(\"company\", \"Unknown\")\n",
    "        \n",
    "        if filing_url:\n",
    "            print(f\"   Téléchargement : {company}\")\n",
    "            chunks = load_sec_full_document(filing_url, meta_doc.metadata)\n",
    "            sec_full_docs.extend(chunks)\n",
    "            print(f\"      → {len(chunks)} chunks extraits\")\n",
    "            time.sleep(2)  # Rate limiting SEC (important !)\n",
    "\n",
    "    all_documents[\"financial_reports\"].extend(sec_full_docs)\n",
    "    print(f\"    {len(sec_full_docs)} chunks de documents complets.\\n\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 2. NEWSAPI - MULTI-QUERIES STRATÉGIQUES\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    if NEWSAPI_KEY:\n",
    "        print(\"Collecte des Actualités (NewsAPI Multi-Queries)...\")\n",
    "        from config import KPMG_QUERIES_NEWS_API\n",
    "        \n",
    "        news_docs = []\n",
    "        for i, query in enumerate( KPMG_QUERIES_NEWS_API, 1):\n",
    "            print(f\"    Query {i}/{len( KPMG_QUERIES_NEWS_API)}: {query}\")\n",
    "            articles = load_newsapi_articles(query, days_back=30)\n",
    "            news_docs.extend(articles)\n",
    "            time.sleep(1)  # Rate limiting\n",
    "        \n",
    "        all_documents[\"news\"].extend(news_docs)\n",
    "        print(f\"    {len(news_docs)} articles totaux via NewsAPI.\\n\")\n",
    "        \n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 3. GOOGLE NEWS RSS (NOUVEAU)\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    print(\" Flux Google News RSS...\")\n",
    "    from config import GOOGLE_RSS_QUERIES\n",
    "    rss_docs = []\n",
    "    for i, query in enumerate(GOOGLE_RSS_QUERIES, 1):\n",
    "        print(f\"    RSS Query {i}/{len(GOOGLE_RSS_QUERIES)}: {query}\")\n",
    "        articles = load_google_news_rss(query, limit=25)  # 25 articles par query\n",
    "        rss_docs.extend(articles)\n",
    "        time.sleep(random.uniform(1, 2))  # Rate limiting (Google est tolérant mais restons fair)\n",
    "\n",
    "    all_documents[\"news\"].extend(rss_docs)\n",
    "    print(f\"    {len(rss_docs)} articles totaux via RSS.\\n\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 4. COMMUNIQUÉS DE PRESSE\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    print(\" Scraping Communiqués de presse...\")\n",
    "    from config import PRESS_RELEASE_URLS\n",
    "\n",
    "    press_docs = load_press_releases(PRESS_RELEASE_URLS, chunk_articles=True)\n",
    "    all_documents[\"news\"].extend(press_docs)\n",
    "    print(f\"    {len(press_docs)} documents de communiqués (articles chunkés).\\n\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 5. YFINANCE (AVEC RATE LIMITING)\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "\n",
    "    '''\n",
    "    print(\" Analyse Macro & Marchés (yfinance - Top 50 S&P500)...\")\n",
    "    print(\"    Patientez ~5 minutes (rate limiting Yahoo Finance)...\\n\")\n",
    "    from config import SP500_TOP50_YFINANCE\n",
    "\n",
    "    finance_docs = []\n",
    "    for i, ticker in enumerate(SP500_TOP50_YFINANCE, 1):\n",
    "        print(f\"    {i}/50 : {ticker}\")\n",
    "        docs = load_yfinance_data(ticker)\n",
    "        finance_docs.extend(docs)\n",
    "        # Respect strict du rate limit (CRITIQUE)\n",
    "        time.sleep(6)\n",
    "\n",
    "    all_documents[\"macro_data\"].extend(finance_docs)\n",
    "    print(f\"   {len(finance_docs)} fiches financières récupérées.\\n\")\n",
    "    '''\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 7. FRED \n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "\n",
    "    print(\" Collecte des indicateurs Macro (FRED)...\")\n",
    "    from config import FRED_SERIES_COMPLETE\n",
    "\n",
    "    fred_docs = load_fred_macro_data(FRED_SERIES_COMPLETE)\n",
    "    all_documents[\"macro_data\"].extend(fred_docs)\n",
    "    print(f\"    {len(fred_docs)} séries économiques FRED chargées.\\n\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 8. INSEE SIRENE - ENTREPRISES FRANÇAISES (ÉTENDU)\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    print(\" Collecte des données entreprises françaises (INSEE SIRENE)...\")\n",
    "    from config import SIREN_EXTENDED_INSEE\n",
    "\n",
    "    sirene_docs = load_sirene_data_by_siren(SIREN_EXTENDED_INSEE, delay_seconds=0.5)\n",
    "    all_documents[\"financial_reports\"].extend(sirene_docs)\n",
    "    print(f\"    {len(sirene_docs)} entreprises françaises analysées.\\n\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 6. DBNOMICS - INDICATEURS MACRO-ÉCONOMIQUES\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # Importer depuis config.py\n",
    "    from config import KPMG_DBNOMICS_SERIES\n",
    "        \n",
    "    dbnomics_docs = load_dbnomics_data(KPMG_DBNOMICS_SERIES, max_observations=10)\n",
    "    all_documents[\"macro_data\"].extend(dbnomics_docs)\n",
    "    print(f\"   {len(dbnomics_docs)} séries économiques chargées.\\n\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 9. ADZUNA - MARCHÉ DE L'EMPLOI\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    from config import KPMG_ADZUNA_SEARCHES\n",
    "        \n",
    "    adzuna_docs = []\n",
    "    for search_config in KPMG_ADZUNA_SEARCHES:\n",
    "        docs = load_adzuna_data(**search_config)\n",
    "        adzuna_docs.extend(docs)\n",
    "        time.sleep(1)  # Rate limiting\n",
    "        \n",
    "    all_documents[\"macro_data\"].extend(adzuna_docs)\n",
    "    print(f\"    {len(adzuna_docs)} analyses emploi collectées.\\n\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # X. BLUESKY (SIGNAUX SOCIAUX)\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    \n",
    "    print(\" Collecte des signaux sociaux (Bluesky)...\")\n",
    "    from config import ALL_BLUESKY_QUERIES \n",
    "\n",
    "\n",
    "    bluesky_docs = load_bluesky_data(\n",
    "        search_queries=ALL_BLUESKY_QUERIES,\n",
    "        limit=15,  # 15 posts par requête\n",
    "        lang=\"fr\"\n",
    "    )\n",
    "    \n",
    "    all_documents[\"social_signals\"].extend(bluesky_docs)\n",
    "    print(f\"   {len(bluesky_docs)} posts Bluesky.\\n\")\n",
    "    \n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # OPTIONNEL : Suivi de comptes spécifiques\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    \n",
    "    # Exemple : Suivre les posts d'un média tech\n",
    "    # influential_accounts = [\"techcrunch.bsky.social\", \"wired.bsky.social\"]\n",
    "    # \n",
    "    # for account in influential_accounts:\n",
    "    #     account_docs = load_bluesky_data(\n",
    "    #         author_handle=account,\n",
    "    #         limit=10\n",
    "    #     )\n",
    "    #     all_documents[\"news\"].extend(account_docs)\n",
    "    #     time.sleep(2)\n",
    "    \n",
    "\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    # RÉSUMÉ FINAL\n",
    "    # ═══════════════════════════════════════════════════════════════\n",
    "    print(\"=\"*60)\n",
    "    print(\" RÉSUMÉ DE L'INGESTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_docs = sum(len(docs) for docs in all_documents.values())\n",
    "    \n",
    "    for namespace, docs in all_documents.items():\n",
    "        status = \"\" if len(docs) > 0 else \" VIDE\"\n",
    "        print(f\"   {namespace:20s} : {len(docs):4d} documents {status}\")\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    print(f\"  TOTAL DOCUMENTS : {total_docs}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Sauvegarde\n",
    "    with open(\"ingested_documents.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        serializable_docs = [\n",
    "            {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "            for docs in all_documents.values() for doc in docs\n",
    "        ]\n",
    "        json.dump(serializable_docs, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Documents sauvegardés dans 'ingested_documents.json'\")\n",
    "    print(\"Prêt pour le chunking et les embeddings (Notebook 3)\")\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# EXÉCUTION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents_by_namespace = ingest_all_sources()\n",
    "    \n",
    "    # Sauvegarde\n",
    "    with open(\"ingested_documents.json\", \"w\") as f:\n",
    "        serializable = {}\n",
    "        for ns, docs in documents_by_namespace.items():\n",
    "            serializable[ns] = [\n",
    "                {\n",
    "                    \"page_content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                }\n",
    "                for doc in docs\n",
    "            ]\n",
    "        json.dump(serializable, f, indent=2)\n",
    "    \n",
    "    print(\" Documents sauvegardés dans 'ingested_documents.json'\")\n",
    "    print(\" Prêt pour le chunking et les embeddings (Notebook 3)\")\n",
    "\n",
    "    #Yfinace ne marche pas comme il faut,\n",
    "    # Opencorporates et crunchbase ne marchent pas non plus \n",
    "    # Reddit bloqué recommencer plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunker adaptatif initialisé\n",
      " 1326 documents chargés\n",
      "\n",
      "============================================================\n",
      " PHASE DE CHUNKING ADAPTATIF\n",
      "============================================================\n",
      "\n",
      " Chunking pour namespace 'financial_reports'...\n",
      "    40 docs → 10668 chunks\n",
      "\n",
      " Chunking pour namespace 'news'...\n",
      "    1103 docs → 1698 chunks\n",
      "\n",
      " Chunking pour namespace 'macro_data'...\n",
      "    88 docs → 144 chunks\n",
      "\n",
      " Chunking pour namespace 'social_signals'...\n",
      "    95 docs → 150 chunks\n",
      "\n",
      " Résumé du chunking :\n",
      "   financial_reports: 10668 chunks\n",
      "   news: 1698 chunks\n",
      "   macro_data: 144 chunks\n",
      "   social_signals: 150 chunks\n",
      "\n",
      "============================================================\n",
      " PHASE FACTS (LITE) — Extraction structurée\n",
      "============================================================\n",
      " 10 facts extraits depuis 'financial_reports'\n",
      " 8 facts extraits depuis 'news'\n",
      " 12 facts extraits depuis 'macro_data'\n",
      " Facts sauvegardés dans 'facts.json'\n",
      " Ajout de 30 facts dans chunked_by_namespace['facts']\n",
      "\n",
      " Modèle Mistral-embed initialisé\n",
      "\n",
      " Étape 1 : Chunking terminé\n",
      "\n",
      "============================================================\n",
      " ÉTAPE 1B : EXTRACTION DE FACTS STRUCTURÉS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      " PHASE FACTS (LITE) — Extraction structurée\n",
      "============================================================\n",
      " 10 facts extraits depuis 'financial_reports'\n",
      " 7 facts extraits depuis 'news'\n",
      " 12 facts extraits depuis 'macro_data'\n",
      " Facts sauvegardés dans 'facts.json'\n",
      " 29 facts ajoutés au namespace 'facts'\n",
      "\n",
      "============================================================\n",
      "ÉTAPE 2 : GÉNÉRATION DES EMBEDDINGS\n",
      "============================================================\n",
      "\n",
      " Traitement namespace 'financial_reports'...\n",
      "\n",
      " Génération de 10668 embeddings...\n",
      "    Batch 1/214 traité\n",
      "    Batch 2/214 traité\n",
      "    Batch 3/214 traité\n",
      "    Batch 4/214 traité\n",
      "    Batch 5/214 traité\n",
      "    Batch 6/214 traité\n",
      "    Batch 7/214 traité\n",
      "    Batch 8/214 traité\n",
      "    Batch 9/214 traité\n",
      "    Batch 10/214 traité\n",
      "    Batch 11/214 traité\n",
      "    Batch 12/214 traité\n",
      "    Batch 13/214 traité\n",
      "    Batch 14/214 traité\n",
      "    Batch 15/214 traité\n",
      "    Batch 16/214 traité\n",
      "    Batch 17/214 traité\n",
      "    Batch 18/214 traité\n",
      "    Batch 19/214 traité\n",
      "    Batch 20/214 traité\n",
      "    Batch 21/214 traité\n",
      "    Batch 22/214 traité\n",
      "    Batch 23/214 traité\n",
      "    Batch 24/214 traité\n",
      "    Batch 25/214 traité\n",
      "    Batch 26/214 traité\n",
      "    Batch 27/214 traité\n",
      "    Batch 28/214 traité\n",
      "    Batch 29/214 traité\n",
      "    Batch 30/214 traité\n",
      "    Batch 31/214 traité\n",
      "    Batch 32/214 traité\n",
      "    Batch 33/214 traité\n",
      "    Batch 34/214 traité\n",
      "    Batch 35/214 traité\n",
      "    Batch 36/214 traité\n",
      "    Batch 37/214 traité\n",
      "    Batch 38/214 traité\n",
      "    Batch 39/214 traité\n",
      "    Batch 40/214 traité\n",
      "    Batch 41/214 traité\n",
      "    Batch 42/214 traité\n",
      "    Batch 43/214 traité\n",
      "    Batch 44/214 traité\n",
      "    Batch 45/214 traité\n",
      "    Batch 46/214 traité\n",
      "    Batch 47/214 traité\n",
      "    Batch 48/214 traité\n",
      "    Batch 49/214 traité\n",
      "    Batch 50/214 traité\n",
      "    Batch 51/214 traité\n",
      "    Batch 52/214 traité\n",
      "    Batch 53/214 traité\n",
      "    Batch 54/214 traité\n",
      "    Batch 55/214 traité\n",
      "    Batch 56/214 traité\n",
      "    Batch 57/214 traité\n",
      "    Batch 58/214 traité\n",
      "    Batch 59/214 traité\n",
      "    Batch 60/214 traité\n",
      "    Batch 61/214 traité\n",
      "    Batch 62/214 traité\n",
      "    Batch 63/214 traité\n",
      "    Batch 64/214 traité\n",
      "    Batch 65/214 traité\n",
      "    Batch 66/214 traité\n",
      "    Batch 67/214 traité\n",
      "    Batch 68/214 traité\n",
      "    Batch 69/214 traité\n",
      "    Batch 70/214 traité\n",
      "    Batch 71/214 traité\n",
      "    Batch 72/214 traité\n",
      "    Batch 73/214 traité\n",
      "    Batch 74/214 traité\n",
      "    Batch 75/214 traité\n",
      "    Batch 76/214 traité\n",
      "    Batch 77/214 traité\n",
      "    Batch 78/214 traité\n",
      "    Batch 79/214 traité\n",
      "    Batch 80/214 traité\n",
      "    Batch 81/214 traité\n",
      "    Batch 82/214 traité\n",
      "    Batch 83/214 traité\n",
      "    Batch 84/214 traité\n",
      "    Batch 85/214 traité\n",
      "    Batch 86/214 traité\n",
      "    Batch 87/214 traité\n",
      "    Batch 88/214 traité\n",
      "    Batch 89/214 traité\n",
      "    Batch 90/214 traité\n",
      "    Batch 91/214 traité\n",
      "    Batch 92/214 traité\n",
      "    Batch 93/214 traité\n",
      "    Batch 94/214 traité\n",
      "    Batch 95/214 traité\n",
      "    Batch 96/214 traité\n",
      "    Batch 97/214 traité\n",
      "    Batch 98/214 traité\n",
      "    Batch 99/214 traité\n",
      "    Batch 100/214 traité\n",
      "    Batch 101/214 traité\n",
      "    Batch 102/214 traité\n",
      "    Batch 103/214 traité\n",
      "    Batch 104/214 traité\n",
      "    Batch 105/214 traité\n",
      "    Batch 106/214 traité\n",
      "    Batch 107/214 traité\n",
      "    Batch 108/214 traité\n",
      "    Batch 109/214 traité\n",
      "    Batch 110/214 traité\n",
      "    Batch 111/214 traité\n",
      "    Batch 112/214 traité\n",
      "    Batch 113/214 traité\n",
      "    Batch 114/214 traité\n",
      "    Batch 115/214 traité\n",
      "    Batch 116/214 traité\n",
      "    Batch 117/214 traité\n",
      "    Batch 118/214 traité\n",
      "    Batch 119/214 traité\n",
      "    Batch 120/214 traité\n",
      "    Batch 121/214 traité\n",
      "    Batch 122/214 traité\n",
      "    Batch 123/214 traité\n",
      "    Batch 124/214 traité\n",
      "    Batch 125/214 traité\n",
      "    Batch 126/214 traité\n",
      "    Batch 127/214 traité\n",
      "    Batch 128/214 traité\n",
      "    Batch 129/214 traité\n",
      "    Batch 130/214 traité\n",
      "    Batch 131/214 traité\n",
      "    Batch 132/214 traité\n",
      "    Batch 133/214 traité\n",
      "    Batch 134/214 traité\n",
      "    Batch 135/214 traité\n",
      "    Batch 136/214 traité\n",
      "    Batch 137/214 traité\n",
      "    Batch 138/214 traité\n",
      "    Batch 139/214 traité\n",
      "    Batch 140/214 traité\n",
      "    Batch 141/214 traité\n",
      "    Batch 142/214 traité\n",
      "    Batch 143/214 traité\n",
      "    Batch 144/214 traité\n",
      "    Batch 145/214 traité\n",
      "    Batch 146/214 traité\n",
      "    Batch 147/214 traité\n",
      "    Batch 148/214 traité\n",
      "    Batch 149/214 traité\n",
      "    Batch 150/214 traité\n",
      "    Batch 151/214 traité\n",
      "    Batch 152/214 traité\n",
      "    Batch 153/214 traité\n",
      "    Batch 154/214 traité\n",
      "    Batch 155/214 traité\n",
      "    Batch 156/214 traité\n",
      "    Batch 157/214 traité\n",
      "    Batch 158/214 traité\n",
      "    Batch 159/214 traité\n",
      "    Batch 160/214 traité\n",
      "    Batch 161/214 traité\n",
      "    Batch 162/214 traité\n",
      "    Batch 163/214 traité\n",
      "    Batch 164/214 traité\n",
      "    Batch 165/214 traité\n",
      "    Batch 166/214 traité\n",
      "    Batch 167/214 traité\n",
      "    Batch 168/214 traité\n",
      "    Batch 169/214 traité\n",
      "    Batch 170/214 traité\n",
      "    Batch 171/214 traité\n",
      "    Batch 172/214 traité\n",
      "    Batch 173/214 traité\n",
      "    Batch 174/214 traité\n",
      "    Batch 175/214 traité\n",
      "    Batch 176/214 traité\n",
      "    Batch 177/214 traité\n",
      "    Batch 178/214 traité\n",
      "    Batch 179/214 traité\n",
      "    Batch 180/214 traité\n",
      "    Batch 181/214 traité\n",
      "    Batch 182/214 traité\n",
      "    Batch 183/214 traité\n",
      "    Batch 184/214 traité\n",
      "    Batch 185/214 traité\n",
      "    Batch 186/214 traité\n",
      "    Batch 187/214 traité\n",
      "    Batch 188/214 traité\n",
      "    Batch 189/214 traité\n",
      "    Batch 190/214 traité\n",
      "    Batch 191/214 traité\n",
      "    Batch 192/214 traité\n",
      "    Batch 193/214 traité\n",
      "    Batch 194/214 traité\n",
      "    Batch 195/214 traité\n",
      "    Batch 196/214 traité\n",
      "    Batch 197/214 traité\n",
      "    Batch 198/214 traité\n",
      "    Batch 199/214 traité\n",
      "    Batch 200/214 traité\n",
      "    Batch 201/214 traité\n",
      "    Batch 202/214 traité\n",
      "    Batch 203/214 traité\n",
      "    Batch 204/214 traité\n",
      "    Batch 205/214 traité\n",
      "    Batch 206/214 traité\n",
      "    Batch 207/214 traité\n",
      "    Batch 208/214 traité\n",
      "    Batch 209/214 traité\n",
      "    Batch 210/214 traité\n",
      "    Batch 211/214 traité\n",
      "    Batch 212/214 traité\n",
      "    Batch 213/214 traité\n",
      "    Batch 214/214 traité\n",
      "\n",
      " Traitement namespace 'news'...\n",
      "\n",
      " Génération de 1698 embeddings...\n",
      "    Batch 1/34 traité\n",
      "    Batch 2/34 traité\n",
      "    Batch 3/34 traité\n",
      "    Batch 4/34 traité\n",
      "    Batch 5/34 traité\n",
      "    Batch 6/34 traité\n",
      "    Batch 7/34 traité\n",
      "    Batch 8/34 traité\n",
      "    Batch 9/34 traité\n",
      "    Batch 10/34 traité\n",
      "    Batch 11/34 traité\n",
      "    Batch 12/34 traité\n",
      "    Batch 13/34 traité\n",
      "    Batch 14/34 traité\n",
      "    Batch 15/34 traité\n",
      "    Batch 16/34 traité\n",
      "    Batch 17/34 traité\n",
      "    Batch 18/34 traité\n",
      "    Batch 19/34 traité\n",
      "    Batch 20/34 traité\n",
      "    Batch 21/34 traité\n",
      "    Batch 22/34 traité\n",
      "    Batch 23/34 traité\n",
      "    Batch 24/34 traité\n",
      "    Batch 25/34 traité\n",
      "    Batch 26/34 traité\n",
      "    Batch 27/34 traité\n",
      "    Batch 28/34 traité\n",
      "    Batch 29/34 traité\n",
      "    Batch 30/34 traité\n",
      "    Batch 31/34 traité\n",
      "    Batch 32/34 traité\n",
      "    Batch 33/34 traité\n",
      "    Batch 34/34 traité\n",
      "\n",
      " Traitement namespace 'macro_data'...\n",
      "\n",
      " Génération de 144 embeddings...\n",
      "    Batch 1/3 traité\n",
      "    Batch 2/3 traité\n",
      "    Batch 3/3 traité\n",
      "\n",
      " Traitement namespace 'social_signals'...\n",
      "\n",
      " Génération de 150 embeddings...\n",
      "    Batch 1/3 traité\n",
      "    Batch 2/3 traité\n",
      "    Batch 3/3 traité\n",
      "\n",
      " Traitement namespace 'facts'...\n",
      "\n",
      " Génération de 29 embeddings...\n",
      "    Batch 1/1 traité\n",
      "\n",
      "============================================================\n",
      " VALIDATION DE LA QUALITÉ\n",
      "============================================================\n",
      "\n",
      "  Namespaces manquants ou vides : ['startups', 'startups']\n",
      "   (Normal si certaines sources n'ont pas été activées)\n",
      "\n",
      " Namespace: financial_reports\n",
      "     Chunk 10662 : métadonnées manquantes ['namespace']\n",
      "     Chunk 10663 : métadonnées manquantes ['namespace']\n",
      "     Chunk 10664 : métadonnées manquantes ['namespace']\n",
      "     Chunk 10665 : métadonnées manquantes ['namespace']\n",
      "     Chunk 10666 : métadonnées manquantes ['namespace']\n",
      "     Chunk 10667 : métadonnées manquantes ['namespace']\n",
      "    10668 chunks validés\n",
      "\n",
      " Namespace: news\n",
      "    1698 chunks validés\n",
      "\n",
      " Namespace: macro_data\n",
      "    144 chunks validés\n",
      "\n",
      " Namespace: social_signals\n",
      "    150 chunks validés\n",
      "\n",
      " Namespace: facts\n",
      "     Chunk 0 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 1 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 2 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 3 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 4 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 5 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 6 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 7 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 8 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 9 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 10 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 11 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 12 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 13 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 14 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 15 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 16 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 17 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 18 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 19 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 20 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 21 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 22 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 23 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 24 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 25 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 26 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 27 : métadonnées manquantes ['source', 'namespace']\n",
      "     Chunk 28 : métadonnées manquantes ['source', 'namespace']\n",
      "    29 chunks validés\n",
      "\n",
      "============================================================\n",
      " Chunks avec embeddings : 12689/12689\n",
      " Toutes les dimensions sont correctes (1024)\n",
      "============================================================\n",
      "\n",
      " Sauvegarde des documents pour indexation...\n",
      " Documents sauvegardés dans 'embedded_documents.json'\n",
      "\n",
      " Prêt pour l'indexation Pinecone (Notebook 4)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTEBOOK 3 : Chunking Adaptatif & Embeddings Mistral\n",
    "====================================================\n",
    "\n",
    "OBJECTIF : Découper les documents de manière intelligente\n",
    "           et générer des embeddings optimisés pour Pinecone.\n",
    "\n",
    "RÉFÉRENCES :\n",
    "- LangChain Text Splitters : https://python.langchain.com/docs/modules/data_connection/document_transformers/\n",
    "- Mistral Embeddings : https://docs.mistral.ai/capabilities/embeddings/\n",
    "- Chunking Best Practices : https://www.pinecone.io/learn/chunking-strategies/\n",
    "\n",
    "MÉTHODOLOGIE :\n",
    "1. Chunking adaptatif selon le type de document\n",
    "2. Enrichissement des métadonnées\n",
    "3. Génération d'embeddings par batch (optimisation)\n",
    "4. Validation de la qualité\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    HTMLHeaderTextSplitter\n",
    ")\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_mistralai import ChatMistralAI  # NOUVEAU pour FACT-RAG\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MISTRAL_API_KEY= os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 1 : STRATÉGIES DE CHUNKING PAR TYPE DE DOCUMENT\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "JUSTIFICATION DES CHOIX DE CHUNKING\n",
    "\n",
    "D'après vos notes (KPMG v2.pdf), le chunking est l'étape la plus sous-estimée\n",
    "mais déterminante pour la qualité du RAG.\n",
    "\n",
    "PRINCIPES :\n",
    "1. Granularité adaptée au contenu\n",
    "   - Petits chunks (500 chars) : KPIs, chiffres précis\n",
    "   - Gros chunks (1000+ chars) : analyses, raisonnements\n",
    "\n",
    "2. Overlap significatif (15-20%)\n",
    "   - Évite de couper les informations critiques\n",
    "   - Maintient la cohérence contextuelle\n",
    "\n",
    "3. Séparateurs intelligents\n",
    "   - Paragraphes > Phrases > Mots\n",
    "   - Préserve la structure sémantique\n",
    "\"\"\"\n",
    "\n",
    "class AdaptiveChunker:\n",
    "    \"\"\"Découpe intelligente selon le type de document\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Splitter pour rapports financiers (SEC, yfinance)\n",
    "        self.financial_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=800,      # Balance entre détail et contexte\n",
    "            chunk_overlap=150,   # ~19% overlap\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "            add_start_index=True  # Traçabilité dans le document source\n",
    "        )\n",
    "        \n",
    "        # Splitter pour actualités (courtes, denses)\n",
    "        self.news_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,      # Articles courts\n",
    "            chunk_overlap=100,   # 20% overlap\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "            add_start_index=True\n",
    "        )\n",
    "        \n",
    "        # Splitter HTML (communiqués de presse structurés)\n",
    "        self.html_splitter = HTMLHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"h1\", \"Header 1\"),\n",
    "                (\"h2\", \"Header 2\"),\n",
    "                (\"h3\", \"Header 3\"),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def chunk_documents(self, documents: List[Document], namespace: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Applique la stratégie de chunking appropriée selon le namespace\n",
    "        \"\"\"\n",
    "        print(f\"\\n Chunking pour namespace '{namespace}'...\")\n",
    "        \n",
    "        # Mapping namespace → splitter\n",
    "        if namespace in [\"financial_reports\", \"macro_data\", \"startups\"]:\n",
    "            # Documents denses, analytiques, avec chiffres\n",
    "            splitter = self.financial_splitter\n",
    "        elif namespace in [\"news\", \"social_signals\"]:\n",
    "            # Contenu court, rapide à consommer\n",
    "            splitter = self.news_splitter\n",
    "        elif namespace == \"facts\":\n",
    "            # Facts déjà structurés, pas de chunking nécessaire\n",
    "            return documents  # Retour direct sans chunking\n",
    "        elif namespace == \"web_quarantine\":\n",
    "            # Sources web externes, traitement léger\n",
    "            splitter = self.news_splitter\n",
    "        else:\n",
    "            # Défaut conservateur\n",
    "            splitter = self.news_splitter\n",
    "        \n",
    "        # Découpage\n",
    "        chunked_docs = []\n",
    "        for doc in documents:\n",
    "            chunks = splitter.split_documents([doc])\n",
    "            \n",
    "            # Enrichir métadonnées\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk.metadata.update({\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"chunking_strategy\": splitter.__class__.__name__\n",
    "                })\n",
    "                chunked_docs.append(chunk)\n",
    "        \n",
    "        print(f\"    {len(documents)} docs → {len(chunked_docs)} chunks\")\n",
    "        return chunked_docs\n",
    "\n",
    "chunker = AdaptiveChunker()\n",
    "print(\" Chunker adaptatif initialisé\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 2 : CHARGEMENT DES DOCUMENTS INGÉRÉS\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "Récupération des documents depuis le Notebook 2\n",
    "\"\"\"\n",
    "\n",
    "def load_ingested_documents() -> Dict[str, List[Document]]:\n",
    "    \"\"\"Charge les documents depuis le fichier JSON\"\"\"\n",
    "    try:\n",
    "        with open(\"ingested_documents.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Reconversion en objets Document\n",
    "        documents_by_ns = {}\n",
    "        for namespace, docs_data in data.items():\n",
    "            documents_by_ns[namespace] = [\n",
    "                Document(\n",
    "                    page_content=d[\"page_content\"],\n",
    "                    metadata=d[\"metadata\"]\n",
    "                )\n",
    "                for d in docs_data\n",
    "            ]\n",
    "        \n",
    "        print(f\" {sum(len(docs) for docs in documents_by_ns.values())} documents chargés\")\n",
    "        return documents_by_ns\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\" Fichier 'ingested_documents.json' introuvable.\")\n",
    "        print(\"   Exécutez d'abord le Notebook 2 (ingestion)\")\n",
    "        return {}\n",
    "\n",
    "docs_by_namespace = load_ingested_documents()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 3 : APPLICATION DU CHUNKING\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "Application du chunking adaptatif sur tous les namespaces\n",
    "\"\"\"\n",
    "\n",
    "chunked_by_namespace = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" PHASE DE CHUNKING ADAPTATIF\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for namespace, documents in docs_by_namespace.items():\n",
    "    if documents:\n",
    "        chunked_docs = chunker.chunk_documents(documents, namespace)\n",
    "        chunked_by_namespace[namespace] = chunked_docs\n",
    "\n",
    "print(\"\\n Résumé du chunking :\")\n",
    "for ns, chunks in chunked_by_namespace.items():\n",
    "    print(f\"   {ns}: {len(chunks)} chunks\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 3B : EXTRACTION DE FACTS (FACT-RAG \"LITE\")\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\"\"\"\n",
    "Objectif :\n",
    "- Extraire des \"facts\" structurés (chiffres / événements) à partir des chunks déjà produits\n",
    "- Les stocker comme Documents LangChain dans un namespace dédié : \"facts\"\n",
    "- Les facts seront ensuite embed + indexés dans Pinecone comme les autres chunks\n",
    "\n",
    "⚠️ Version \"Lite\" (hackathon) :\n",
    "- On limite volontairement le nombre d'appels LLM\n",
    "- On extrait uniquement quelques types de facts high-signal\n",
    "\"\"\"\n",
    "\n",
    "from typing import Optional\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "FACT_NAMESPACE = \"facts\"\n",
    "\n",
    "FACT_TYPES = [\n",
    "    \"deal\",              # acquisition / partnership / funding\n",
    "    \"financial_kpi\",      # revenue, margin, capex, guidance\n",
    "    \"market_size\",        # taille de marché\n",
    "    \"growth_rate\",        # croissance / CAGR\n",
    "    \"regulation_event\",   # régulation / amende / décision\n",
    "    \"product_launch\"      # lancement / pivot stratégique\n",
    "]\n",
    "\n",
    "# Limites anti-explosion de coûts (hackathon-friendly)\n",
    "MAX_CHUNKS_PER_NAMESPACE_FOR_FACTS = 10\n",
    "MAX_CHARS_PER_CHUNK_FOR_FACTS = 1200\n",
    "\n",
    "def _safe_json_loads(text: str) -> Optional[dict]:\n",
    "    \"\"\"Parse JSON robuste : récupère le premier bloc JSON trouvable.\"\"\"\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback : tenter d'extraire le 1er objet JSON dans le texte\n",
    "    import re\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(0))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def extract_facts_from_chunks(chunked_by_namespace: Dict[str, List[Document]]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Extrait des facts depuis un échantillon de chunks (par namespace).\n",
    "    Retourne une liste de Documents (facts) qui seront indexés dans Pinecone.\n",
    "    \"\"\"\n",
    "    if not MISTRAL_API_KEY:\n",
    "        print(\" MISTRAL_API_KEY manquante → extraction FACT ignorée.\")\n",
    "        return []\n",
    "\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-small\",\n",
    "        mistral_api_key=MISTRAL_API_KEY,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    fact_docs: List[Document] = []\n",
    "\n",
    "    # On cible les namespaces les plus pertinents pour des facts \"actionnables\"\n",
    "    # Namespaces pertinents pour extraction de facts\n",
    "    FACT_EXTRACTION_NAMESPACES = [\n",
    "        \"financial_reports\",  # Priorité : chiffres financiers\n",
    "        \"news\",               # Deals, événements\n",
    "        \"macro_data\",         # KPIs macro\n",
    "        \"startups\"            # Funding rounds, acquisitions\n",
    "    ]\n",
    "\n",
    "    target_namespaces = [ns for ns in FACT_EXTRACTION_NAMESPACES if ns in chunked_by_namespace]\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" PHASE FACTS (LITE) — Extraction structurée\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for ns in target_namespaces:\n",
    "        chunks = chunked_by_namespace.get(ns) or []\n",
    "        if not chunks:\n",
    "            continue\n",
    "\n",
    "        sample = chunks[:MAX_CHUNKS_PER_NAMESPACE_FOR_FACTS]\n",
    "\n",
    "        # On construit un \"mini-dossier\" pour limiter à 1 appel LLM / namespace\n",
    "        dossier_parts = []\n",
    "        for i, d in enumerate(sample, start=1):\n",
    "            url = d.metadata.get(\"url\") or d.metadata.get(\"source\") or \"\"\n",
    "            published = d.metadata.get(\"published_at\") or d.metadata.get(\"date\") or \"\"\n",
    "            text = (d.page_content or \"\")[:MAX_CHARS_PER_CHUNK_FOR_FACTS]\n",
    "            dossier_parts.append(f\"CHUNK {i}\\nURL: {url}\\nDATE: {published}\\nTEXTE:\\n{text}\\n\")\n",
    "\n",
    "        dossier = \"\\n---\\n\".join(dossier_parts)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Tu es un analyste en veille stratégique (cabinet de conseil).\n",
    "À partir des CHUNKS ci-dessous, extrais des FACTS structurés.\n",
    "\n",
    "Règles :\n",
    "- Retourne UNIQUEMENT du JSON valide (pas de Markdown).\n",
    "- Les facts doivent être vérifiables dans le texte.\n",
    "- Si tu n'es pas sûr, n'invente pas.\n",
    "- Maximum 12 facts.\n",
    "\n",
    "Schéma JSON :\n",
    "{{\n",
    "  \"facts\": [\n",
    "    {{\n",
    "      \"fact_type\": one_of({FACT_TYPES}),\n",
    "      \"entity\": \"Nom d'entreprise / marché / organisme\",\n",
    "      \"value\": \"nombre ou texte court (optionnel)\",\n",
    "      \"unit\": \"USD|EUR|%|... (optionnel)\",\n",
    "      \"period\": \"FY2023|Q3 2024|2025-01-15|... (optionnel)\",\n",
    "      \"geography\": \"US|EU|France|Global|... (optionnel)\",\n",
    "      \"source_url\": \"URL à citer\",\n",
    "      \"source_date\": \"date si disponible\",\n",
    "      \"quote_span\": \"extrait EXACT du chunk qui justifie le fact\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "CHUNKS (namespace={ns}) :\n",
    "{dossier}\n",
    "\"\"\"\n",
    "        try:\n",
    "            llm_out = llm.invoke(prompt)\n",
    "            parsed = _safe_json_loads(getattr(llm_out, \"content\", str(llm_out)))\n",
    "\n",
    "            if not parsed or \"facts\" not in parsed:\n",
    "                print(f\"⚠️ Aucun JSON FACT valide pour namespace '{ns}'\")\n",
    "                continue\n",
    "\n",
    "            ns_facts = parsed.get(\"facts\") or []\n",
    "            print(f\" {len(ns_facts)} facts extraits depuis '{ns}'\")\n",
    "\n",
    "            for f in ns_facts:\n",
    "                # Normalisation minimale\n",
    "                fact_type = f.get(\"fact_type\", \"\").strip()\n",
    "                entity = (f.get(\"entity\") or \"\").strip()\n",
    "                source_url = (f.get(\"source_url\") or \"\").strip()\n",
    "                quote = (f.get(\"quote_span\") or \"\").strip()\n",
    "\n",
    "                if not fact_type or not entity or not source_url or not quote:\n",
    "                    continue\n",
    "\n",
    "                # Texte compact pour embedding/retrieval\n",
    "                summary = f\"[{fact_type}] {entity} — {f.get('value','')} {f.get('unit','')}\".strip()\n",
    "                if f.get(\"period\"):\n",
    "                    summary += f\" (période: {f.get('period')})\"\n",
    "                if f.get(\"geography\"):\n",
    "                    summary += f\" (geo: {f.get('geography')})\"\n",
    "\n",
    "                fact_docs.append(Document(\n",
    "                    page_content=summary,\n",
    "                    metadata={\n",
    "                        \"fact_type\": fact_type,\n",
    "                        \"entity\": entity,\n",
    "                        \"value\": f.get(\"value\"),\n",
    "                        \"unit\": f.get(\"unit\"),\n",
    "                        \"period\": f.get(\"period\"),\n",
    "                        \"geography\": f.get(\"geography\"),\n",
    "                        \"source_url\": source_url,\n",
    "                        \"source_date\": f.get(\"source_date\"),\n",
    "                        \"quote_span\": quote,\n",
    "                        \"source_namespace\": ns,\n",
    "                        \"doc_type\": \"fact\"\n",
    "                    }\n",
    "                ))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Erreur extraction FACT sur namespace '{ns}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # Sauvegarde locale (MVP)\n",
    "    if fact_docs:\n",
    "        serializable = [\n",
    "            {\"page_content\": d.page_content, \"metadata\": d.metadata}\n",
    "            for d in fact_docs\n",
    "        ]\n",
    "        with open(\"facts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(serializable, f, ensure_ascii=False, indent=2)\n",
    "        print(\" Facts sauvegardés dans 'facts.json'\")\n",
    "\n",
    "    return fact_docs\n",
    "\n",
    "# Exécution : on extrait des facts puis on les ajoute au pipeline pour embeddings + indexation\n",
    "fact_documents = extract_facts_from_chunks(chunked_by_namespace)\n",
    "if fact_documents:\n",
    "    chunked_by_namespace[FACT_NAMESPACE] = fact_documents\n",
    "    print(f\" Ajout de {len(fact_documents)} facts dans chunked_by_namespace['{FACT_NAMESPACE}']\")\n",
    "else:\n",
    "    print(\"Aucun fact ajouté (soit aucun fact trouvé, soit extraction désactivée)\")\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 4 : GÉNÉRATION DES EMBEDDINGS MISTRAL\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION MISTRAL EMBEDDINGS\n",
    "\n",
    "Modèle : mistral-embed\n",
    "Dimension : 1024\n",
    "Coût : Gratuit avec limits (voir plan Mistral)\n",
    "\n",
    "RÉFÉRENCE :\n",
    "https://docs.mistral.ai/capabilities/embeddings/\n",
    "\n",
    "OPTIMISATION :\n",
    "- Traitement par batch pour limiter les appels API\n",
    "- Cache local des embeddings (évite recalcul)\n",
    "- Gestion des erreurs et retry\n",
    "\"\"\"\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "if not MISTRAL_API_KEY:\n",
    "    raise ValueError(\" MISTRAL_API_KEY manquante dans .env\")\n",
    "\n",
    "embeddings_model = MistralAIEmbeddings(\n",
    "    model=\"mistral-embed\",\n",
    "    mistral_api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "print(\"\\n Modèle Mistral-embed initialisé\")\n",
    "\n",
    "def generate_embeddings_batch(documents: List[Document], batch_size: int = 50) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Génère les embeddings par batch pour optimiser les appels API\n",
    "    \n",
    "    Args:\n",
    "        documents: Documents à embedder\n",
    "        batch_size: Nombre de documents par batch\n",
    "    \n",
    "    Returns:\n",
    "        Documents avec embeddings dans les métadonnées\n",
    "    \"\"\"\n",
    "    print(f\"\\n Génération de {len(documents)} embeddings...\")\n",
    "    \n",
    "    total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        try:\n",
    "            # Extraire les textes\n",
    "            texts = [doc.page_content for doc in batch]\n",
    "            \n",
    "            # Générer les embeddings\n",
    "            embeddings = embeddings_model.embed_documents(texts)\n",
    "            \n",
    "            # Ajouter les embeddings aux métadonnées\n",
    "            for doc, embedding in zip(batch, embeddings):\n",
    "                doc.metadata[\"embedding\"] = embedding\n",
    "            \n",
    "            print(f\"    Batch {batch_num}/{total_batches} traité\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    Erreur batch {batch_num}: {e}\")\n",
    "            # Continuer avec les autres batches\n",
    "            continue\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 5 : VALIDATION DE LA QUALITÉ\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "MÉTRIQUES DE QUALITÉ\n",
    "\n",
    "Avant d'envoyer vers Pinecone, on valide :\n",
    "1. Tous les chunks ont des embeddings\n",
    "2. Les dimensions sont correctes (1024)\n",
    "3. Les métadonnées sont complètes\n",
    "\"\"\"\n",
    "\n",
    "def validate_chunked_documents(docs_by_ns: Dict[str, List[Document]]) -> bool:\n",
    "    \"\"\"Valide la qualité des documents chunkés et embeddés\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" VALIDATION DE LA QUALITÉ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    # VÉRIFICATION : Tous les namespaces attendus sont présents ?\n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    EXPECTED_NAMESPACES = [\n",
    "        \"financial_reports\", \"news\", \"macro_data\", \n",
    "        \"startups\", \"social_signals\",\"startups\"\n",
    "        # \"facts\" et \"web_quarantine\" sont optionnels\n",
    "    ]\n",
    "    \n",
    "    missing_namespaces = [ns for ns in EXPECTED_NAMESPACES if ns not in docs_by_ns or not docs_by_ns[ns]]\n",
    "    if missing_namespaces:\n",
    "        print(f\"\\n  Namespaces manquants ou vides : {missing_namespaces}\")\n",
    "        print(\"   (Normal si certaines sources n'ont pas été activées)\")\n",
    "    \n",
    "    total_chunks = sum(len(docs) for docs in docs_by_ns.values())\n",
    "    chunks_with_embeddings = 0\n",
    "    invalid_embeddings = []\n",
    "    \n",
    "    for namespace, documents in docs_by_ns.items():\n",
    "        print(f\"\\n Namespace: {namespace}\")\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            # Vérifier présence embedding\n",
    "            if \"embedding\" in doc.metadata:\n",
    "                chunks_with_embeddings += 1\n",
    "                \n",
    "                # Vérifier dimension\n",
    "                emb_dim = len(doc.metadata[\"embedding\"])\n",
    "                if emb_dim != 1024:\n",
    "                    invalid_embeddings.append((namespace, i, emb_dim))\n",
    "            \n",
    "            # Vérifier métadonnées essentielles\n",
    "            required_metadata = [\"source\", \"namespace\"]\n",
    "            missing = [key for key in required_metadata if key not in doc.metadata]\n",
    "            if missing:\n",
    "                print(f\"     Chunk {i} : métadonnées manquantes {missing}\")\n",
    "        \n",
    "        print(f\"    {len(documents)} chunks validés\")\n",
    "    \n",
    "    # Rapport final\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\" Chunks avec embeddings : {chunks_with_embeddings}/{total_chunks}\")\n",
    "    \n",
    "    if invalid_embeddings:\n",
    "        print(f\"  Embeddings invalides (dimension ≠ 1024) : {len(invalid_embeddings)}\")\n",
    "        for ns, idx, dim in invalid_embeddings[:5]:  # Afficher les 5 premiers\n",
    "            print(f\"   - {ns}[{idx}] : dimension {dim}\")\n",
    "    else:\n",
    "        print(\" Toutes les dimensions sont correctes (1024)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return chunks_with_embeddings == total_chunks\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 6 : PIPELINE COMPLET\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "Orchestration complète : Chunking → Embeddings → Validation\n",
    "\"\"\"\n",
    "\n",
    "def process_all_documents():\n",
    "    \"\"\"Pipeline complet de traitement\"\"\"\n",
    "    \n",
    "    # Étape 1 : Chunking (déjà fait)\n",
    "    print(\"\\n Étape 1 : Chunking terminé\")\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    # ÉTAPE 1B : EXTRACTION DE FACTS (NOUVEAU)\n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" ÉTAPE 1B : EXTRACTION DE FACTS STRUCTURÉS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    fact_documents = extract_facts_from_chunks(chunked_by_namespace)\n",
    "    \n",
    "    if fact_documents:\n",
    "        chunked_by_namespace[FACT_NAMESPACE] = fact_documents\n",
    "        print(f\" {len(fact_documents)} facts ajoutés au namespace '{FACT_NAMESPACE}'\")\n",
    "    else:\n",
    "        print(\" Aucun fact extrait (soit aucun trouvé, soit extraction désactivée)\")\n",
    "    \n",
    "    # Étape 2 : Génération des embeddings\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ÉTAPE 2 : GÉNÉRATION DES EMBEDDINGS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for namespace, documents in chunked_by_namespace.items():\n",
    "        if documents:\n",
    "            print(f\"\\n Traitement namespace '{namespace}'...\")\n",
    "            generate_embeddings_batch(documents)\n",
    "    \n",
    "    # Étape 3 : Validation\n",
    "    is_valid = validate_chunked_documents(chunked_by_namespace)\n",
    "    \n",
    "    if is_valid:\n",
    "        # Sauvegarde pour le Notebook 4\n",
    "        print(\"\\n Sauvegarde des documents pour indexation...\")\n",
    "        \n",
    "        serializable = {}\n",
    "        for ns, docs in chunked_by_namespace.items():\n",
    "            serializable[ns] = [\n",
    "                {\n",
    "                    \"page_content\": doc.page_content,\n",
    "                    \"metadata\": {\n",
    "                        k: v for k, v in doc.metadata.items()\n",
    "                        if k != \"embedding\"  # Embeddings trop gros pour JSON\n",
    "                    },\n",
    "                    \"embedding\": doc.metadata.get(\"embedding\", [])\n",
    "                }\n",
    "                for doc in docs\n",
    "            ]\n",
    "        \n",
    "        with open(\"embedded_documents.json\", \"w\") as f:\n",
    "            json.dump(serializable, f, indent=2)\n",
    "        \n",
    "        print(\" Documents sauvegardés dans 'embedded_documents.json'\")\n",
    "        print(\"\\n Prêt pour l'indexation Pinecone (Notebook 4)\")\n",
    "    else:\n",
    "        print(\"\\n Validation échouée. Vérifiez les erreurs ci-dessus.\")\n",
    "\n",
    "# Exécution\n",
    "if __name__ == \"__main__\":\n",
    "    if docs_by_namespace:\n",
    "        process_all_documents()\n",
    "    else:\n",
    "        print(\"\\n  Aucun document à traiter.\")\n",
    "        print(\"   Exécutez d'abord les Notebooks 1 et 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Clients Pinecone et Mistral initialisés\n",
      " 12689 documents chargés depuis embedded_documents.json\n",
      "\n",
      "============================================================\n",
      " INDEXATION PINECONE\n",
      "============================================================\n",
      "\n",
      " Traitement namespace : financial_reports\n",
      "\n",
      " Upsert vers namespace 'financial_reports'...\n",
      "   10668 vecteurs en 107 batches\n",
      "   Batch 1/107 envoyé\n",
      "   Batch 2/107 envoyé\n",
      "   Batch 3/107 envoyé\n",
      "   Batch 4/107 envoyé\n",
      "   Batch 5/107 envoyé\n",
      "   Batch 6/107 envoyé\n",
      "   Batch 7/107 envoyé\n",
      "   Batch 8/107 envoyé\n",
      "   Batch 9/107 envoyé\n",
      "   Batch 10/107 envoyé\n",
      "   Batch 11/107 envoyé\n",
      "   Batch 12/107 envoyé\n",
      "   Batch 13/107 envoyé\n",
      "   Batch 14/107 envoyé\n",
      "   Batch 15/107 envoyé\n",
      "   Batch 16/107 envoyé\n",
      "   Batch 17/107 envoyé\n",
      "   Batch 18/107 envoyé\n",
      "   Batch 19/107 envoyé\n",
      "   Batch 20/107 envoyé\n",
      "   Batch 21/107 envoyé\n",
      "   Batch 22/107 envoyé\n",
      "   Batch 23/107 envoyé\n",
      "   Batch 24/107 envoyé\n",
      "   Batch 25/107 envoyé\n",
      "   Batch 26/107 envoyé\n",
      "   Batch 27/107 envoyé\n",
      "   Batch 28/107 envoyé\n",
      "   Batch 29/107 envoyé\n",
      "   Batch 30/107 envoyé\n",
      "   Batch 31/107 envoyé\n",
      "   Batch 32/107 envoyé\n",
      "   Batch 33/107 envoyé\n",
      "   Batch 34/107 envoyé\n",
      "   Batch 35/107 envoyé\n",
      "   Batch 36/107 envoyé\n",
      "   Batch 37/107 envoyé\n",
      "   Batch 38/107 envoyé\n",
      "   Batch 39/107 envoyé\n",
      "   Batch 40/107 envoyé\n",
      "   Batch 41/107 envoyé\n",
      "   Batch 42/107 envoyé\n",
      "   Batch 43/107 envoyé\n",
      "   Batch 44/107 envoyé\n",
      "   Batch 45/107 envoyé\n",
      "   Batch 46/107 envoyé\n",
      "   Batch 47/107 envoyé\n",
      "   Batch 48/107 envoyé\n",
      "   Batch 49/107 envoyé\n",
      "   Batch 50/107 envoyé\n",
      "   Batch 51/107 envoyé\n",
      "   Batch 52/107 envoyé\n",
      "   Batch 53/107 envoyé\n",
      "   Batch 54/107 envoyé\n",
      "   Batch 55/107 envoyé\n",
      "   Batch 56/107 envoyé\n",
      "   Batch 57/107 envoyé\n",
      "   Batch 58/107 envoyé\n",
      "   Batch 59/107 envoyé\n",
      "   Batch 60/107 envoyé\n",
      "   Batch 61/107 envoyé\n",
      "   Batch 62/107 envoyé\n",
      "   Batch 63/107 envoyé\n",
      "   Batch 64/107 envoyé\n",
      "   Batch 65/107 envoyé\n",
      "   Batch 66/107 envoyé\n",
      "   Batch 67/107 envoyé\n",
      "   Batch 68/107 envoyé\n",
      "   Batch 69/107 envoyé\n",
      "   Batch 70/107 envoyé\n",
      "   Batch 71/107 envoyé\n",
      "   Batch 72/107 envoyé\n",
      "   Batch 73/107 envoyé\n",
      "   Batch 74/107 envoyé\n",
      "   Batch 75/107 envoyé\n",
      "   Batch 76/107 envoyé\n",
      "   Batch 77/107 envoyé\n",
      "   Batch 78/107 envoyé\n",
      "   Batch 79/107 envoyé\n",
      "   Batch 80/107 envoyé\n",
      "   Batch 81/107 envoyé\n",
      "   Batch 82/107 envoyé\n",
      "   Batch 83/107 envoyé\n",
      "   Batch 84/107 envoyé\n",
      "   Batch 85/107 envoyé\n",
      "   Batch 86/107 envoyé\n",
      "   Batch 87/107 envoyé\n",
      "   Batch 88/107 envoyé\n",
      "   Batch 89/107 envoyé\n",
      "   Batch 90/107 envoyé\n",
      "   Batch 91/107 envoyé\n",
      "   Batch 92/107 envoyé\n",
      "   Batch 93/107 envoyé\n",
      "   Batch 94/107 envoyé\n",
      "   Batch 95/107 envoyé\n",
      "   Batch 96/107 envoyé\n",
      "   Batch 97/107 envoyé\n",
      "   Batch 98/107 envoyé\n",
      "   Batch 99/107 envoyé\n",
      "   Batch 100/107 envoyé\n",
      "   Batch 101/107 envoyé\n",
      "   Batch 102/107 envoyé\n",
      "   Batch 103/107 envoyé\n",
      "   Batch 104/107 envoyé\n",
      "   Batch 105/107 envoyé\n",
      "   Batch 106/107 envoyé\n",
      "   Batch 107/107 envoyé\n",
      "\n",
      " Traitement namespace : news\n",
      "\n",
      " Upsert vers namespace 'news'...\n",
      "   1698 vecteurs en 17 batches\n",
      "   Batch 1/17 envoyé\n",
      "   Batch 2/17 envoyé\n",
      "   Batch 3/17 envoyé\n",
      "   Batch 4/17 envoyé\n",
      "   Batch 5/17 envoyé\n",
      "   Batch 6/17 envoyé\n",
      "   Batch 7/17 envoyé\n",
      "   Batch 8/17 envoyé\n",
      "   Batch 9/17 envoyé\n",
      "   Batch 10/17 envoyé\n",
      "   Batch 11/17 envoyé\n",
      "   Batch 12/17 envoyé\n",
      "   Batch 13/17 envoyé\n",
      "   Batch 14/17 envoyé\n",
      "   Batch 15/17 envoyé\n",
      "   Batch 16/17 envoyé\n",
      "   Batch 17/17 envoyé\n",
      "\n",
      " Traitement namespace : macro_data\n",
      "\n",
      " Upsert vers namespace 'macro_data'...\n",
      "   144 vecteurs en 2 batches\n",
      "   Batch 1/2 envoyé\n",
      "   Batch 2/2 envoyé\n",
      "\n",
      " Traitement namespace : social_signals\n",
      "\n",
      " Upsert vers namespace 'social_signals'...\n",
      "   150 vecteurs en 2 batches\n",
      "   Batch 1/2 envoyé\n",
      "   Batch 2/2 envoyé\n",
      "\n",
      " Traitement namespace : facts\n",
      "\n",
      " Upsert vers namespace 'facts'...\n",
      "   29 vecteurs en 1 batches\n",
      "   Batch 1/1 envoyé\n",
      "\n",
      "============================================================\n",
      " INDEXATION TERMINÉE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      " VALIDATION DE L'INDEXATION\n",
      "============================================================\n",
      "\n",
      " Statistiques de l'index 'kpmg-veille' :\n",
      "   Total vecteurs : 12374\n",
      "\n",
      " Vecteurs par namespace :\n",
      "   - financial_reports: 10530 vecteurs\n",
      "   - news: 1521 vecteurs\n",
      "   - facts: 29 vecteurs\n",
      "   - macro_data: 144 vecteurs\n",
      "   - social_signals: 150 vecteurs\n",
      "\n",
      " Système RAG prêt pour les requêtes (Notebook 5)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTEBOOK 4 : Indexation Pinecone avec Namespaces\n",
    "=================================================\n",
    "\n",
    "OBJECTIF : Indexer les documents embeddés dans Pinecone\n",
    "           en utilisant les namespaces pour l'isolation des sources.\n",
    "\n",
    "RÉFÉRENCES :\n",
    "- Pinecone Upsert : https://docs.pinecone.io/docs/upsert-data\n",
    "- LangChain Pinecone : https://python.langchain.com/docs/integrations/vectorstores/pinecone\n",
    "- Namespaces Best Practices : https://docs.pinecone.io/docs/namespaces\n",
    "\n",
    "MÉTHODOLOGIE :\n",
    "1. Chargement des documents embeddés\n",
    "2. Conversion au format Pinecone\n",
    "3. Upsert par batch et namespace\n",
    "4. Validation de l'indexation\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "NOTEBOOK 4 : Indexation Pinecone avec Namespaces\n",
    "=================================================\n",
    "\n",
    "OBJECTIF : Indexer les documents embeddés dans Pinecone\n",
    "           en utilisant les namespaces pour l'isolation des sources.\n",
    "\n",
    "RÉFÉRENCES :\n",
    "- Pinecone Upsert : https://docs.pinecone.io/docs/upsert-data\n",
    "- LangChain Pinecone : https://python.langchain.com/docs/integrations/vectorstores/pinecone\n",
    "- Namespaces Best Practices : https://docs.pinecone.io/docs/namespaces\n",
    "\n",
    "MÉTHODOLOGIE :\n",
    "1. Chargement des documents embeddés\n",
    "2. Conversion au format Pinecone\n",
    "3. Upsert par batch et namespace\n",
    "4. Validation de l'indexation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pinecone\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 1 : INITIALISATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "INDEX_NAME = \"kpmg-veille\"\n",
    "\n",
    "if not PINECONE_API_KEY or not MISTRAL_API_KEY:\n",
    "    raise ValueError(\" Clés API manquantes dans .env\")\n",
    "\n",
    "# Clients\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "embeddings_model = MistralAIEmbeddings(\n",
    "    model=\"mistral-embed\",\n",
    "    mistral_api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "print(\" Clients Pinecone et Mistral initialisés\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 2 : CHARGEMENT DES DOCUMENTS EMBEDDÉS\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_embedded_documents() -> Dict[str, List[Document]]:\n",
    "    \"\"\"Charge les documents avec leurs embeddings depuis le Notebook 3\"\"\"\n",
    "    try:\n",
    "        with open(\"embedded_documents.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        docs_by_ns = {}\n",
    "        for namespace, docs_data in data.items():\n",
    "            docs_by_ns[namespace] = [\n",
    "                Document(\n",
    "                    page_content=d[\"page_content\"],\n",
    "                    metadata={\n",
    "                        **d[\"metadata\"],\n",
    "                        \"embedding\": d[\"embedding\"]\n",
    "                    }\n",
    "                )\n",
    "                for d in docs_data\n",
    "            ]\n",
    "        \n",
    "        total = sum(len(docs) for docs in docs_by_ns.values())\n",
    "        print(f\" {total} documents chargés depuis embedded_documents.json\")\n",
    "        \n",
    "        return docs_by_ns\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\" Fichier 'embedded_documents.json' introuvable.\")\n",
    "        print(\"   Exécutez d'abord le Notebook 3 (chunking & embeddings)\")\n",
    "        return {}\n",
    "\n",
    "docs_by_namespace = load_embedded_documents()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 3 : PRÉPARATION DES VECTEURS POUR PINECONE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "FORMAT PINECONE UPSERT\n",
    "\n",
    "Structure requise :\n",
    "{\n",
    "    \"id\": \"unique_id\",\n",
    "    \"values\": [0.1, 0.2, ...],  # Embedding (liste de 1024 floats)\n",
    "    \"metadata\": {...}            # Métadonnées (max 40 KB par vecteur)\n",
    "}\n",
    "\n",
    "RÉFÉRENCE :\n",
    "https://docs.pinecone.io/docs/upsert-data\n",
    "\"\"\"\n",
    "\n",
    "def prepare_vectors_for_pinecone(documents: List[Document], namespace: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convertit les Documents au format Pinecone avec gestion des doublons\n",
    "    \n",
    "    ANTI-DOUBLONS : Utilise un hash MD5 du contenu pour créer des IDs déterministes.\n",
    "    → Si vous relancez l'ingestion, Pinecone fera un UPDATE au lieu d'un INSERT.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # 1. GÉNÉRATION D'ID DÉTERMINISTE (ANTI-DOUBLONS)\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # Hash MD5 du contenu → toujours le même ID pour le même texte\n",
    "        content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()\n",
    "        vector_id = f\"{namespace}_{content_hash}\"\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # 2. EXTRACTION DE L'EMBEDDING\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        embedding = doc.metadata.pop(\"embedding\", None)\n",
    "        \n",
    "        if not embedding:\n",
    "            continue\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # 3. NETTOYAGE DES MÉTADONNÉES (LIMITE 40KB PINECONE)\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        clean_metadata = {}\n",
    "        \n",
    "        # Liste blanche : métadonnées à conserver\n",
    "        allowed_keys = [\n",
    "            \"source\", \"namespace\", \"title\", \"url\", \"published_at\",\n",
    "            \"company\", \"company_name\", \"cik\", \"form_type\", \"filing_date\",\n",
    "            \"author\", \"source_name\", \"ticker\", \"series_id\", \"siren\",\n",
    "            \"search_query\", \"author_handle\", \"created_at\"\n",
    "        ]\n",
    "        \n",
    "        for key in allowed_keys:\n",
    "            if key in doc.metadata:\n",
    "                value = doc.metadata[key]\n",
    "                \n",
    "                # Conversion en string si nécessaire\n",
    "                if isinstance(value, (list, dict)):\n",
    "                    clean_metadata[key] = str(value)[:500]  # Limiter taille\n",
    "                elif value is not None:\n",
    "                    clean_metadata[key] = str(value)[:500]\n",
    "        \n",
    "        # Ajouter un extrait du texte pour prévisualisation\n",
    "        clean_metadata[\"text\"] = doc.page_content[:1000]\n",
    "        \n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        # 4. CRÉATION DU VECTEUR PINECONE\n",
    "        # ═══════════════════════════════════════════════════════════\n",
    "        vector = {\n",
    "            \"id\": vector_id,           # ID déterministe (anti-doublons)\n",
    "            \"values\": embedding,       # Vecteur 1024 dimensions\n",
    "            \"metadata\": clean_metadata # Métadonnées nettoyées\n",
    "        }\n",
    "        \n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 4 : UPSERT PAR BATCH ET NAMESPACE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "STRATÉGIE D'UPSERT\n",
    "\n",
    "1. Traiter par batch de 100 vecteurs (limite Pinecone)\n",
    "2. Utiliser les namespaces pour isoler les sources\n",
    "3. Gérer les erreurs et retry\n",
    "\n",
    "RÉFÉRENCE :\n",
    "https://docs.pinecone.io/docs/upsert-data#batching-upserts\n",
    "\"\"\"\n",
    "\n",
    "def upsert_to_pinecone(vectors: List[Dict], namespace: str, batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Envoie les vecteurs vers Pinecone par batch\n",
    "    \n",
    "    Args:\n",
    "        vectors: Vecteurs au format Pinecone\n",
    "        namespace: Namespace de destination\n",
    "        batch_size: Taille des batches\n",
    "    \"\"\"\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    total_batches = (len(vectors) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\n Upsert vers namespace '{namespace}'...\")\n",
    "    print(f\"   {len(vectors)} vecteurs en {total_batches} batches\")\n",
    "    \n",
    "    for i in range(0, len(vectors), batch_size):\n",
    "        batch = vectors[i:i+batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        \n",
    "        try:\n",
    "            index.upsert(\n",
    "                vectors=batch,\n",
    "                namespace=namespace\n",
    "            )\n",
    "            print(f\"   Batch {batch_num}/{total_batches} envoyé\")\n",
    "            \n",
    "            # Rate limiting (10 req/sec max pour Pinecone gratuit)\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"    Erreur batch {batch_num}: {e}\")\n",
    "            # Retry une fois\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                index.upsert(vectors=batch, namespace=namespace)\n",
    "                print(f\"   Retry réussi pour batch {batch_num}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"    Retry échoué : {e2}\")\n",
    "                continue\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 5 : INDEXATION COMPLÈTE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def index_all_documents():\n",
    "    \"\"\"Pipeline complet d'indexation\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" INDEXATION PINECONE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not docs_by_namespace:\n",
    "        print(\" Aucun document à indexer\")\n",
    "        return\n",
    "    \n",
    "    for namespace, documents in docs_by_namespace.items():\n",
    "        if not documents:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n Traitement namespace : {namespace}\")\n",
    "        \n",
    "        # Préparation des vecteurs\n",
    "        vectors = prepare_vectors_for_pinecone(documents, namespace)\n",
    "        \n",
    "        if vectors:\n",
    "            # Upsert vers Pinecone\n",
    "            upsert_to_pinecone(vectors, namespace)\n",
    "        else:\n",
    "            print(f\"     Aucun vecteur valide pour {namespace}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" INDEXATION TERMINÉE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 6 : VALIDATION POST-INDEXATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "VALIDATION :\n",
    "- Vérifier le nombre de vecteurs indexés\n",
    "- Tester une recherche simple\n",
    "- Confirmer l'isolation des namespaces\n",
    "\"\"\"\n",
    "\n",
    "def validate_indexation():\n",
    "    \"\"\"Valide que l'indexation s'est bien passée\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" VALIDATION DE L'INDEXATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    stats = index.describe_index_stats()\n",
    "    \n",
    "    print(f\"\\n Statistiques de l'index '{INDEX_NAME}' :\")\n",
    "    print(f\"   Total vecteurs : {stats.total_vector_count}\")\n",
    "    print(f\"\\n Vecteurs par namespace :\")\n",
    "    \n",
    "    for namespace, info in stats.namespaces.items():\n",
    "        print(f\"   - {namespace}: {info.vector_count} vecteurs\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 7 : EXÉCUTION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Indexation\n",
    "    index_all_documents()\n",
    "    \n",
    "    # Validation\n",
    "    time.sleep(2)  # Laisser Pinecone finaliser l'indexation\n",
    "    validate_indexation()\n",
    "    \n",
    "    print(\"\\n Système RAG prêt pour les requêtes (Notebook 5)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faut il Nettoyer les métadonnées avant d’upser ?\n",
    "\n",
    "Pour identifier l'erreur, donner le requirement, l'env et le notebook a claude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================\n",
    "#RAG AGENTIC : SCORE\n",
    "#============================================================#\n",
    "from config import PRIMARY_SOURCES\n",
    "from config import SECONDARY_SOURCES\n",
    "from config import SPAM_INDICATORS\n",
    "\n",
    "def score_source(url: str) -> int:\n",
    "    \"\"\"\n",
    "    Scoring KPMG-grade des sources web\n",
    "    \n",
    "    Retourne :\n",
    "    - 3 : Source primaire (officielle, gouvernementale, académique)\n",
    "    - 2 : Source secondaire fiable (presse reconnue, institutionnels)\n",
    "    - 1 : Source tertiaire (blogs, forums)\n",
    "    - 0 : Source non fiable (spam, domaines suspects)\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return 0\n",
    "    \n",
    "    url = url.lower()\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    # NIVEAU 3 : SOURCES PRIMAIRES * * *\n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "\n",
    "    if any(domain in url for domain in PRIMARY_SOURCES):\n",
    "        return 3\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    # NIVEAU 2 : SOURCES SECONDAIRES FIABLES * *\n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "\n",
    "    \n",
    "    if any(domain in url for domain in SECONDARY_SOURCES):\n",
    "        return 2\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    # NIVEAU 0 : SOURCES NON FIABLES (BLACKLIST) \n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    \n",
    "    if any(indicator in url for indicator in SPAM_INDICATORS):\n",
    "        return 0\n",
    "    \n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    # NIVEAU 1 : SOURCES TERTIAIRES (PAR DÉFAUT) *\n",
    "    # ═══════════════════════════════════════════════════════════\n",
    "    # Blogs, forums, sites inconnus\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modèles Mistral initialisés\n",
      " Prompt KPMG configuré\n",
      "\n",
      " Système RAG KPMG opérationnel\n",
      " Prêt pour l'interface Gradio (optionnel)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTEBOOK 5 : RAG Query & Prompt Engineering pour Veille KPMG\n",
    "============================================================\n",
    "\n",
    "OBJECTIF : Créer un système de requêtes RAG optimisé pour la veille stratégique\n",
    "           avec prompting avancé et citations obligatoires.\n",
    "\n",
    "RÉFÉRENCES :\n",
    "- Mistral Prompting : https://docs.mistral.ai/guides/prompting_capabilities/\n",
    "- LangChain RAG : https://python.langchain.com/docs/use_cases/question_answering/\n",
    "- KPMG Requirements : hackathon KPMG (1).pdf\n",
    "\n",
    "EXIGENCES CRITIQUES :\n",
    "✓ Citations systématiques des sources\n",
    "✓ Indication de fiabilité et date\n",
    "✓ Réponses structurées (pas de bullet points sauf demande explicite)\n",
    "✓ IA explicable (chaîne de raisonnement)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# LangChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.documents import Document\n",
    "from pinecone import Pinecone\n",
    "import hashlib\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)\n",
    "WEB_QUARANTINE_NAMESPACE = \"web_quarantine\"\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 1 : INITIALISATION DES COMPOSANTS RAG\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "INDEX_NAME = \"kpmg-veille\"\n",
    "\n",
    "# Modèle d'embeddings\n",
    "embeddings = MistralAIEmbeddings(\n",
    "    model=\"mistral-embed\",\n",
    "    mistral_api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "# Modèle LLM (Mistral Medium pour raisonnement)\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-medium\",\n",
    "    temperature=0,  # Déterministe pour analyses factuelles\n",
    "    mistral_api_key=MISTRAL_API_KEY\n",
    ")\n",
    "\n",
    "print(\" Modèles Mistral initialisés\")\n",
    "\n",
    "def judge_context_sufficiency(question: str, docs: List, llm) -> bool:\n",
    "    \"\"\"\n",
    "    Juge si le contexte interne suffit pour répondre\n",
    "    selon les standards KPMG (fiabilité, fraîcheur, complétude).\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return False\n",
    "\n",
    "    joined_docs = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Tu es un Senior Manager KPMG.\n",
    "\n",
    "    QUESTION :\n",
    "    {question}\n",
    "\n",
    "    CONTEXTE DISPONIBLE :\n",
    "    {joined_docs}\n",
    "\n",
    "    Réponds STRICTEMENT par OUI ou NON.\n",
    "\n",
    "    Le contexte permet-il de répondre de manière :\n",
    "    - fiable\n",
    "    - récente\n",
    "    - sans extrapolation ?\n",
    "    \"\"\"\n",
    "\n",
    "    verdict = llm.invoke(prompt)\n",
    "    return \"OUI\" in verdict.content.upper()\n",
    "\n",
    "def has_reliable_sources(docs, min_confidence=2):\n",
    "    \"\"\"\n",
    "    Vérifie s'il existe au moins une source web fiable\n",
    "    \n",
    "    Args:\n",
    "        docs : Liste de documents\n",
    "        min_confidence : Seuil de confiance\n",
    "            - 3 : Uniquement sources primaires (très strict)\n",
    "            - 2 : Sources secondaires acceptées (standard KPMG)\n",
    "            - 1 : Toutes sources (permissif, déconseillé)\n",
    "    \"\"\"\n",
    "    for doc in docs:\n",
    "        if (\n",
    "            doc.metadata.get(\"origin\") == \"web\"\n",
    "            and doc.metadata.get(\"confidence\", 0) >= min_confidence\n",
    "        ):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 2 : RETRIEVERS PAR NAMESPACE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "STRATÉGIE DE RETRIEVAL\n",
    "\n",
    "On crée un retriever par namespace pour permettre des requêtes ciblées.\n",
    "L'utilisateur peut spécifier le namespace ou interroger tous les namespaces.\n",
    "\n",
    "PARAMÈTRES :\n",
    "- k=5 : Top 5 documents les plus pertinents\n",
    "- score_threshold : Filtrage par similarité (optionnel)\n",
    "\n",
    "RÉFÉRENCE :\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "\"\"\"\n",
    "\n",
    "NAMESPACES = [\n",
    "    \"financial_reports\",\n",
    "    \"news\",\n",
    "    \"macro_data\",\n",
    "    \"facts\",\n",
    "    \"startups\",\n",
    "    \"social_signals\",\n",
    "    \"web_quarantine\"\n",
    "]\n",
    "\n",
    "\n",
    "def get_retriever(namespace: Optional[str] = None, k: int = 5):\n",
    "    \"\"\"\n",
    "    Crée un retriever pour un namespace spécifique ou global\n",
    "    \n",
    "    Args:\n",
    "        namespace: Namespace ciblé (None = tous les namespaces)\n",
    "            Valeurs possibles :\n",
    "            - \"financial_reports\" : Rapports SEC, données financières\n",
    "            - \"news\" : Articles NewsAPI, Google RSS, communiqués\n",
    "            - \"macro_data\" : Indicateurs macro (FRED, DBnomics, yfinance)\n",
    "            - \"startups\" : Données Crunchbase, SIRENE\n",
    "            - \"social_signals\" : Posts Bluesky\n",
    "            - \"facts\" : Facts structurés (FACT-RAG)\n",
    "            - \"web_quarantine\" : Sources web dynamiques (RAG agentic)\n",
    "            - None : Recherche sur TOUS les namespaces\n",
    "        k: Nombre de documents à récupérer\n",
    "    \n",
    "    Returns:\n",
    "        Retriever configuré\n",
    "    \"\"\"\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=INDEX_NAME,\n",
    "        embedding=embeddings,\n",
    "        namespace=namespace\n",
    "    )\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": k}\n",
    "    )\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 3 : PROMPT ENGINEERING KPMG\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "PROMPT STRUCTURÉ SELON LES EXIGENCES KPMG\n",
    "\n",
    "Inspiré de vos notes (hackathon KPMG.pdf) :\n",
    "✓ Assistant Intelligent de Veille Stratégique\n",
    "✓ Citations OBLIGATOIRES avec source, date, fiabilité\n",
    "✓ Réponse en prose (pas de bullet points par défaut)\n",
    "✓ Indication si données manquantes ou payantes\n",
    "✓ Capacité à demander des précisions\n",
    "\n",
    "STRUCTURE :\n",
    "1. Rôle et expertise\n",
    "2. Instructions de citation\n",
    "3. Format de réponse\n",
    "4. Gestion des cas limites\n",
    "\"\"\"\n",
    "\n",
    "KPMG_PROMPT_TEMPLATE = \"\"\"Vous êtes l'Assistant Intelligent de Veille Stratégique de KPMG Global Strategy Group.\n",
    "\n",
    "Votre mission : Fournir des analyses de marché précises, sourcées et actionnables pour aider nos clients à prendre des décisions d'investissement éclairées.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "RÈGLES DE CITATION (OBLIGATOIRES)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Pour CHAQUE information factuelle (chiffres, dates, faits) vous DEVEZ :\n",
    "1. Citer la source exacte (ex: \"SEC Filing 10-K d'Apple - 2024-01-15\")\n",
    "2. Indiquer le niveau de fiabilité :\n",
    "   - ⭐⭐⭐ : Source primaire (SEC, rapport officiel, yfinance)\n",
    "   - ⭐⭐ : Source secondaire fiable (NewsAPI, presse reconnue)\n",
    "   - ⭐ : Source tertiaire (blogs, réseaux sociaux)\n",
    "3. Préciser la date de l'information si critique\n",
    "\n",
    "Format de citation : [Source | Fiabilité | Date]\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "CONTEXTE DISPONIBLE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "{context}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "QUESTION DU CLIENT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "{question}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "INSTRUCTIONS DE RÉPONSE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "0.\n",
    "RÈGLE ABSOLUE DE FACTUALITÉ :\n",
    "- Tu n’as PAS le droit d’utiliser tes connaissances générales\n",
    "- Tu dois UNIQUEMENT utiliser les informations présentes dans le CONTEXTE\n",
    "- Si une information n’est pas dans le CONTEXTE, tu dois dire explicitement qu’elle est indisponible\n",
    "- Toute affirmation doit être associée à une source identifiable\n",
    "\n",
    "\n",
    "1. STRUCTURE :\n",
    "   - Répondez en prose fluide (paragraphes, pas de bullet points)\n",
    "   - Organisez votre réponse de façon logique et narrative\n",
    "   - Utilisez des transitions naturelles entre les idées\n",
    "\n",
    "2. CONTENU :\n",
    "   - Citez systématiquement vos sources (format ci-dessus)\n",
    "   - Si une donnée est manquante : indiquez-le explicitement\n",
    "   - Si une information nécessite un accès payant : précisez-le\n",
    "   - Si le contexte est ambigu : demandez des précisions au client\n",
    "\n",
    "3. TONE :\n",
    "   - Professionnel mais accessible\n",
    "   - Factuel et analytique\n",
    "   - Confiant sur les données sourcées, prudent sur les spéculations\n",
    "\n",
    "4. CAS LIMITES :\n",
    "   - Si vous ne trouvez pas l'information : \"Les données disponibles ne permettent pas de répondre à cette question. Sources consultées : [liste]. Je recommande [action].\"\n",
    "   - Si deux sources se contredisent : Mentionnez les deux et expliquez pourquoi\n",
    "   - Si une entreprise est ambiguë : \"J'ai identifié plusieurs entreprises nommées [X]. Pouvez-vous préciser : secteur, géographie, ou autre contexte ?\"\n",
    "\n",
    "5. Indique explicitement si une information provient :\n",
    "  • de la base interne KPMG\n",
    "  • d’une recherche externe automatique\n",
    "\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "RÉPONSE ANALYTIQUE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(KPMG_PROMPT_TEMPLATE)\n",
    "\n",
    "print(\" Prompt KPMG configuré\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 4 : FORMATAGE DU CONTEXTE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "FORMATAGE DES DOCUMENTS RÉCUPÉRÉS\n",
    "\n",
    "On enrichit le contexte avec :\n",
    "- Source et type de document\n",
    "- Date de publication/scraping\n",
    "- Namespace d'origine\n",
    "- Score de pertinence (si disponible)\n",
    "\"\"\"\n",
    "\n",
    "def format_docs(docs) -> str:\n",
    "    \"\"\"\n",
    "    Formate les documents récupérés pour le prompt\n",
    "    \n",
    "    Args:\n",
    "        docs: Documents LangChain\n",
    "    \n",
    "    Returns:\n",
    "        String formaté avec métadonnées enrichies\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        metadata = doc.metadata\n",
    "        \n",
    "        # Construction de l'entrée\n",
    "        entry = f\"━━━ DOCUMENT {i} ━━━\\n\"\n",
    "        entry += f\"Source : {metadata.get('source', 'Unknown')}\\n\"\n",
    "        entry += f\"Type : {metadata.get('namespace', 'Unknown')}\\n\"\n",
    "        \n",
    "        # Date si disponible\n",
    "        date_fields = ['filing_date', 'published_at', 'scrape_date', 'retrieval_date']\n",
    "        for field in date_fields:\n",
    "            if field in metadata:\n",
    "                entry += f\"Date : {metadata[field]}\\n\"\n",
    "                break\n",
    "        \n",
    "        # URL si disponible\n",
    "        if 'url' in metadata:\n",
    "            entry += f\"URL : {metadata['url']}\\n\"\n",
    "        \n",
    "        # Contenu\n",
    "        entry += f\"\\nContenu :\\n{doc.page_content}\\n\"\n",
    "        \n",
    "        formatted.append(entry)\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 5 : CHAÎNE RAG COMPLÈTE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "ARCHITECTURE LCEL (LangChain Expression Language)\n",
    "\n",
    "Pipeline : Retriever → Format Context → Prompt → LLM → Parse\n",
    "\n",
    "RÉFÉRENCE :\n",
    "https://python.langchain.com/docs/expression_language/\n",
    "\"\"\"\n",
    "\n",
    "def create_rag_chain(namespace: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Crée une chaîne RAG complète\n",
    "    \n",
    "    Args:\n",
    "        namespace: Namespace ciblé (None = tous)\n",
    "    \n",
    "    Returns:\n",
    "        Chaîne RAG exécutable\n",
    "    \"\"\"\n",
    "    retriever = get_retriever(namespace=namespace, k=5)\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 6 : INTERFACE DE REQUÊTE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "FONCTIONS D'INTERFACE UTILISATEUR\n",
    "\n",
    "Permettent d'interroger le système de différentes manières :\n",
    "- Requête simple (tous namespaces)\n",
    "- Requête ciblée (namespace spécifique)\n",
    "- Requête multi-namespaces (comparaison)\n",
    "\"\"\"\n",
    "\n",
    "def query_veille(question: str, namespace: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Interface principale de requête\n",
    "    \n",
    "    Args:\n",
    "        question: Question de l'utilisateur\n",
    "        namespace: Namespace ciblé (optionnel)\n",
    "    \n",
    "    Returns:\n",
    "        Réponse formatée avec citations\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" ANALYSE EN COURS...\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Question : {question}\")\n",
    "    \n",
    "    if namespace:\n",
    "        print(f\"Namespace : {namespace}\")\n",
    "    else:\n",
    "        print(\"Namespace : Tous (recherche globale)\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "                # 1. Récupération initiale (interne uniquement)\n",
    "        retriever = get_retriever(namespace=namespace, k=5)\n",
    "        docs = retriever.invoke(question)\n",
    "\n",
    "        # 2. Jugement de suffisance\n",
    "        context_ok = judge_context_sufficiency(\n",
    "            question=question,\n",
    "            docs=docs,\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "        final_docs = docs\n",
    "\n",
    "        # 3. Fallback Web si nécessaire\n",
    "    \n",
    "        if not context_ok:\n",
    "            print(\" Contexte insuffisant → recherche externe activée\")\n",
    "\n",
    "            web_results = web_search_tool.run(question)\n",
    "            web_docs = []\n",
    "\n",
    "            for res in web_results:\n",
    "                web_docs.append(\n",
    "                    Document(\n",
    "                        page_content=res[\"content\"],\n",
    "                        metadata={\n",
    "                            \"source\": res.get(\"url\"),\n",
    "                            \"origin\": \"web\",\n",
    "                            \"confidence\": score_source(res.get(\"url\")),\n",
    "                            \"validated\": False\n",
    "                        }\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # 4. Ingestion contrôlée (QUARANTAINE)\n",
    "            vectorstore = PineconeVectorStore(\n",
    "                index_name=INDEX_NAME,\n",
    "                embedding=embeddings,\n",
    "                namespace=WEB_QUARANTINE_NAMESPACE\n",
    "            )\n",
    "\n",
    "            # ════════════════════════════════════════════════════════════\n",
    "            # AJOUT CONTRÔLÉ À PINECONE (AVEC ANTI-DOUBLONS)\n",
    "            # ════════════════════════════════════════════════════════════\n",
    "\n",
    "            pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "            index = pc.Index(INDEX_NAME)\n",
    "\n",
    "            for doc in web_docs:\n",
    "                content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()\n",
    "                doc_id = f\"web_{content_hash}\"\n",
    "\n",
    "            \n",
    "                # Génération d'ID déterministe (même source = même ID)\n",
    "                content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()\n",
    "                vector_id = f\"web_{content_hash}\"\n",
    "                \n",
    "                # Récupérer l'embedding du document\n",
    "                embedding = embeddings.embed_documents([doc.page_content])[0]\n",
    "                \n",
    "                # Métadonnées nettoyées\n",
    "                clean_metadata = {\n",
    "                    \"source\": doc.metadata.get(\"source\", \"\")[:500],\n",
    "                    \"origin\": \"web\",\n",
    "                    \"confidence\": doc.metadata.get(\"confidence\", 1),\n",
    "                    \"text\": doc.page_content[:1000],\n",
    "                    \"validated\": False,\n",
    "                    \"added_date\": datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                # Upsert (remplace si ID existe déjà)\n",
    "                index.upsert(\n",
    "                    vectors=[{\n",
    "                        \"id\": vector_id,\n",
    "                        \"values\": embedding,\n",
    "                        \"metadata\": clean_metadata\n",
    "                    }],\n",
    "                    namespace=WEB_QUARANTINE_NAMESPACE\n",
    "                )\n",
    "\n",
    "            print(f\" {len(web_docs)} sources web ajoutées au namespace quarantine\")\n",
    "\n",
    "            # Fusion du contexte\n",
    "            final_docs = docs + web_docs\n",
    "\n",
    "            has_web_sources = any(\n",
    "            doc.metadata.get(\"origin\") == \"web\" for doc in final_docs\n",
    "        )\n",
    "\n",
    "            has_reliable_web_sources = has_reliable_sources(final_docs)\n",
    "\n",
    "            # Cas 1 — Aucune source fiable du tout → refus (normal)\n",
    "            if not has_reliable_web_sources:\n",
    "                return (\n",
    "                    \"⚠️ Information indisponible : aucune source fiable n’a été \"\n",
    "                    \"identifiée dans la base interne ou via la recherche externe.\\n\\n\"\n",
    "                    \"Conformément aux standards de gouvernance, \"\n",
    "                    \"le système ne produit pas d’analyse factuelle non sourcée.\"\n",
    "                )\n",
    "\n",
    "            # Cas 2 — Sources web fiables trouvées → ON RÉPOND\n",
    "            used_external_sources = has_web_sources\n",
    "\n",
    "\n",
    "\n",
    "        # 5. Génération finale\n",
    "        formatted_context = format_docs(final_docs)\n",
    "\n",
    "\n",
    "        external_disclaimer = \"\"\n",
    "        if used_external_sources:\n",
    "            external_disclaimer = (\n",
    "                \"NOTE IMPORTANTE :\\n\"\n",
    "                \"Aucune information pertinente n’a été trouvée dans la base interne.\\n\"\n",
    "                \"Une recherche externe ciblée a donc été menée.\\n\\n\"\n",
    "            )\n",
    "\n",
    "        response = (\n",
    "            prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        ).invoke({\n",
    "            \"context\": formatted_context,\n",
    "            \"question\": external_disclaimer + question\n",
    "        })\n",
    "\n",
    "\n",
    "        return response\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Erreur lors de la requête : {e}\"\n",
    "\n",
    "\n",
    "def compare_namespaces(question: str, namespaces: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Compare les réponses de plusieurs namespaces\n",
    "    \n",
    "    Args:\n",
    "        question: Question\n",
    "        namespaces: Liste de namespaces à comparer\n",
    "    \n",
    "    Returns:\n",
    "        Dictionnaire {namespace: réponse}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for ns in namespaces:\n",
    "        print(f\"\\n Interrogation de '{ns}'...\")\n",
    "        results[ns] = query_veille(question, namespace=ns)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# SECTION 7 : EXEMPLES D'UTILISATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "\"\"\"\n",
    "SCÉNARIOS DE DÉMONSTRATION KPMG\n",
    "\n",
    "Ces exemples illustrent les capacités du système :\n",
    "1. Analyse de marché\n",
    "2. Due diligence d'entreprise\n",
    "3. Détection de tendances\n",
    "4. Analyse concurrentielle\n",
    "\"\"\"\n",
    "\n",
    "def demo_scenarios():\n",
    "    \"\"\"Démontre les capacités du système avec des cas réels\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \" \"*20)\n",
    "    print(\" DÉMONSTRATION RAG VEILLE KPMG\")\n",
    "    print(\"\"*20 + \"\\n\")\n",
    "    \n",
    "    scenarios = [\n",
    "        {\n",
    "            \"titre\": \"1. ANALYSE FINANCIÈRE D'ENTREPRISE\",\n",
    "            \"question\": \"Quelle est la capitalisation boursière actuelle d'Apple et son évolution ?\",\n",
    "            \"namespace\": \"macro_data\"\n",
    "        },\n",
    "        {\n",
    "            \"titre\": \"2. VEILLE ACTUALITÉS SECTEUR TECH\",\n",
    "            \"question\": \"Quelles sont les dernières actualités concernant l'intelligence artificielle et la finance ?\",\n",
    "            \"namespace\": \"news\"\n",
    "        },\n",
    "        {\n",
    "            \"titre\": \"3. RECHERCHE GLOBALE (TOUS NAMESPACES)\",\n",
    "            \"question\": \"Quels sont les principaux risques et opportunités pour les entreprises tech en 2024 ?\",\n",
    "            \"namespace\": None\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        print(\"\\n\" + \"─\"*60)\n",
    "        print(f\" {scenario['titre']}\")\n",
    "        print(\"─\"*60)\n",
    "        \n",
    "        response = query_veille(\n",
    "            question=scenario['question'],\n",
    "            namespace=scenario['namespace']\n",
    "        )\n",
    "        \n",
    "        print(\"\\n RÉPONSE :\\n\")\n",
    "        print(response)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n Système RAG KPMG opérationnel\")\n",
    "print(\" Prêt pour l'interface Gradio (optionnel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initialisation de l'interface KPMG (mistral-small)...\n",
      " Variables d'environnement chargées\n",
      " Embeddings initialisés\n",
      " Retriever configuré\n",
      "LLM Mistral Small initialisé\n",
      "Prompt configuré\n",
      "Chaîne RAG construite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tm/7kbwgcgd1_v72w5khby1lll00000gn/T/ipykernel_11037/711394503.py:238: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme, css. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(theme=theme, css=custom_css, title=\"KPMG Analytics\") as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LANCEMENT DE L'INTERFACE\n",
      "* Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://adc8717e9cb04eee0e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Interface lancée avec succès !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GRADIO INTERFACE - VERSION AVEC MISTRAL-SMALL (GRATUIT)\n",
    "========================================================\n",
    "\n",
    "Cette version utilise mistral-small au lieu de mistral-medium.\n",
    "Performance légèrement inférieure mais GRATUIT et SUFFISANT pour votre démo.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# AJOUT DU PATH POUR LES MODULES src/\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "src_path = os.path.join(os.getcwd(), \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"✅ Ajouté au path: {src_path}\")\n",
    "\n",
    "import kpmg_interface\n",
    "import importlib\n",
    "import analytics_viz\n",
    "import facts_manager\n",
    "import market_estimation_engine\n",
    "import strategic_facts_service\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# INITIALISATION\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔧 Initialisation de l'interface KPMG (mistral-small)...\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = \"kpmg-veille\"\n",
    "\n",
    "if not MISTRAL_API_KEY or not PINECONE_API_KEY:\n",
    "    raise ValueError(\"❌ Clés API manquantes dans .env\")\n",
    "\n",
    "print(\"✅ Variables d'environnement chargées\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# COMPOSANTS RAG\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "try:\n",
    "    # Embeddings\n",
    "    embeddings = MistralAIEmbeddings(\n",
    "        model=\"mistral-embed\",\n",
    "        mistral_api_key=MISTRAL_API_KEY\n",
    "    )\n",
    "    print(\"✅ Embeddings initialisés\")\n",
    "    \n",
    "    # Vector Store\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=INDEX_NAME,\n",
    "        embedding=embeddings,\n",
    "        namespace=\"news\"\n",
    "    )\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 3}\n",
    "    )\n",
    "    print(\"✅ Retriever configuré\")\n",
    "    \n",
    "    # ╔═══════════════════════════════════════════════════════════╗\n",
    "    # ║ CHANGEMENT CRITIQUE : mistral-medium → mistral-small    ║\n",
    "    # ║                                                           ║\n",
    "    # ║ Mistral-small est GRATUIT et suffisant pour du RAG      ║\n",
    "    # ║ Performance : 85% de mistral-medium à coût zéro         ║\n",
    "    # ╚═══════════════════════════════════════════════════════════╝\n",
    "    \n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-small\",  # ✅ MODÈLE GRATUIT\n",
    "        temperature=0,\n",
    "        mistral_api_key=MISTRAL_API_KEY\n",
    "    )\n",
    "    print(\"✅ LLM Mistral Small initialisé\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors de l'initialisation : {e}\")\n",
    "    raise\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# PROMPT KPMG\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "KPMG_PROMPT_TEMPLATE = \"\"\"Vous êtes l'Assistant Intelligent de Veille Stratégique de KPMG Global Strategy Group.\n",
    "\n",
    "Votre mission : Fournir des analyses de marché précises, sourcées et actionnables pour aider nos clients à prendre des décisions d'investissement éclairées.\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "RÈGLES DE CITATION (OBLIGATOIRES)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Pour CHAQUE information factuelle (chiffres, dates, faits) vous DEVEZ :\n",
    "1. Citer la source exacte (ex: \"https://www.apple.com - 2024-01-15\")\n",
    "2. Indiquer le niveau de fiabilité :\n",
    "   - *** : Source primaire (SEC, rapport officiel, yfinance)\n",
    "   - ** : Source secondaire fiable (NewsAPI, presse reconnue)\n",
    "   - * : Source tertiaire (blogs, réseaux sociaux)\n",
    "3. Préciser la date de l'information si critique\n",
    "\n",
    "Format de citation : [Source au format URL | Fiabilité | Date]\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "CONTEXTE DISPONIBLE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "{context}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "QUESTION DU CLIENT\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "{question}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "INSTRUCTIONS DE RÉPONSE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. STRUCTURE :\n",
    "   - Répondez en prose fluide (paragraphes, pas de bullet points)\n",
    "   - Organisez votre réponse de façon logique et narrative\n",
    "   - Utilisez des transitions naturelles entre les idées\n",
    "\n",
    "2. CONTENU :\n",
    "   - Citez systématiquement vos sources (format ci-dessus)\n",
    "   - Si une donnée est manquante : indiquez-le explicitement\n",
    "   - Si une information nécessite un accès payant : précisez-le\n",
    "   - Si le contexte est ambigu : demandez des précisions au client\n",
    "\n",
    "3. TONE :\n",
    "   - Professionnel mais accessible\n",
    "   - Factuel et analytique\n",
    "   - Confiant sur les données sourcées, prudent sur les spéculations\n",
    "\n",
    "4. CAS LIMITES :\n",
    "   - Si vous ne trouvez pas l'information : \"Les données disponibles ne permettent pas de répondre à cette question. Sources consultées : [liste]. Je recommande [action].\"\n",
    "   - Si deux sources se contredisent : Mentionnez les deux et expliquez pourquoi\n",
    "   - Si une entreprise est ambiguë : \"J'ai identifié plusieurs entreprises nommées [X]. Pouvez-vous préciser : secteur, géographie, ou autre contexte ?\"\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "RÉPONSE ANALYTIQUE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(KPMG_PROMPT_TEMPLATE)\n",
    "print(\"✅ Prompt configuré\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# CHAÎNE RAG\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Formate les documents récupérés\"\"\"\n",
    "    if not docs:\n",
    "        return \"Aucun document pertinent trouvé.\"\n",
    "    \n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        date = doc.metadata.get('published_at', doc.metadata.get('scrape_date', 'N/A'))\n",
    "        content = doc.page_content[:500]\n",
    "        \n",
    "        formatted.append(f\"[Document {i} - Source: {source} - Date: {date}]\\n{content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ Chaîne RAG construite\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# FONCTION STREAMING\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def stream_kpmg_response(message, history):\n",
    "    \"\"\"Génère la réponse de manière progressive\"\"\"\n",
    "    try:\n",
    "        partial_message = \"\"\n",
    "        \n",
    "        for chunk in rag_chain.stream(message):\n",
    "            partial_message += chunk\n",
    "            yield partial_message\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"\"\"❌ Erreur lors de la recherche :\n",
    "        \n",
    "Détails : {str(e)}\n",
    "\n",
    "Suggestions :\n",
    "1. Vérifiez que l'index Pinecone contient des données\n",
    "2. Testez avec une question plus simple\n",
    "3. Vérifiez les crédits Mistral sur console.mistral.ai\"\"\"\n",
    "        \n",
    "        yield error_msg\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# LANCEMENT DE L'INTERFACE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "# Recharger les modules pour prendre en compte les modifications\n",
    "importlib.reload(facts_manager)\n",
    "importlib.reload(market_estimation_engine)\n",
    "importlib.reload(analytics_viz)\n",
    "importlib.reload(kpmg_interface)\n",
    "importlib.reload(strategic_facts_service)\n",
    "print(\"✅ Modules rechargés à chaud !\")\n",
    "\n",
    "# Lance le dashboard KPMG\n",
    "kpmg_interface.launch_dashboard(stream_kpmg_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    \n",
      "   DIAGNOSTIC COMPLET DU SYSTÈME RAG KPMG\n",
      "                    \n",
      "\n",
      "\n",
      "============================================================\n",
      " TEST : Variables d'environnement\n",
      "============================================================\n",
      "   MISTRAL_API_KEY: vd0pw28udW...\n",
      "   PINECONE_API_KEY: pcsk_4PWXZ...\n",
      " Variables d'environnement : RÉUSSI\n",
      "\n",
      "============================================================\n",
      " TEST : État de Pinecone\n",
      "============================================================\n",
      "   Index : kpmg-veille\n",
      "   Total vecteurs : 12374\n",
      "   Dimension : 1024\n",
      "\n",
      "    Namespaces :\n",
      "      - macro_data: 144 vecteurs\n",
      "      - news: 1521 vecteurs\n",
      "      - social_signals: 150 vecteurs\n",
      "      - financial_reports: 10530 vecteurs\n",
      "      - facts: 29 vecteurs\n",
      " État de Pinecone : RÉUSSI\n",
      "\n",
      "============================================================\n",
      " TEST : Mistral Embeddings\n",
      "============================================================\n",
      "   Texte : 'Apple annonce de nouveaux produits'\n",
      "   Dimension : 1024\n",
      "   Type : <class 'list'>\n",
      "   Premiers 5 valeurs : [-0.0162811279296875, 0.0140228271484375, 0.052886962890625, -0.0181427001953125, 0.04644775390625]\n",
      " Mistral Embeddings : RÉUSSI\n",
      "\n",
      "============================================================\n",
      " TEST : Mistral LLM\n",
      "============================================================\n",
      "   Modèle : mistral-medium\n",
      "   Question : Capitale de la France ?\n",
      "   Réponse : Paris.\n",
      " Mistral LLM : RÉUSSI\n",
      "\n",
      "============================================================\n",
      " TEST : Retriever\n",
      "============================================================\n",
      "   Query : 'Apple dernières actualités'\n",
      "   Namespace : news\n",
      "   Résultats : 3 documents\n",
      "\n",
      "    Premier résultat :\n",
      "      Source : press_release\n",
      "      Contenu : Accessibility\n",
      "Education\n",
      "Environment\n",
      "Inclusion and Diversity\n",
      "Privacy\n",
      "Racial Equity and Justice\n",
      "Supply Chain Innovation\n",
      "About Apple\n",
      "About Apple\n",
      "Newsroom...\n",
      " Retriever : RÉUSSI\n",
      "\n",
      "============================================================\n",
      " TEST : Chaîne RAG complète\n",
      "============================================================\n",
      "   Question : 'Quelles sont les dernières actualités ?'\n",
      "   Traitement...\n",
      "\n",
      "    Réponse générée (2182 caractères) :\n",
      "   Voici un résumé des dernières actualités liées à l'**AI Act** (Règlement européen sur l'IA) et ses implications pour les entreprises, d'après les liens partagés :\n",
      "\n",
      "1. **Adoption définitive de l'AI Act** :\n",
      "   - Le **Règlement IA** a été **formellement adopté** par le Parlement européen en avril 2024 ...\n",
      " Chaîne RAG complète : RÉUSSI\n",
      "\n",
      "============================================================\n",
      " TEST : Gradio\n",
      "============================================================\n",
      "   Version : 6.4.0\n",
      " Gradio : RÉUSSI\n",
      "\n",
      "============================================================\n",
      " RAPPORT FINAL\n",
      "============================================================\n",
      "\n",
      "Tests réussis : 7/7\n",
      "\n",
      " TOUS LES TESTS SONT PASSÉS !\n",
      " Votre système est prêt pour Gradio\n",
      "\n",
      " Prochaine étape :\n",
      "   Exécutez la cellule 'gradio_interface_fixed.py'\n",
      "\n",
      "============================================================\n",
      "Diagnostic terminé\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT DE TEST COMPLET - SYSTÈME RAG KPMG\n",
    "==========================================\n",
    "\n",
    "Exécutez ce script pour diagnostiquer tous les problèmes potentiels.\n",
    "Copier-coller dans une nouvelle cellule de notebook.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# FONCTION DE TEST AVEC GESTION D'ERREURS\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def run_test(test_name, test_func):\n",
    "    \"\"\"Exécute un test et affiche le résultat\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\" TEST : {test_name}\")\n",
    "        print('='*60)\n",
    "        result = test_func()\n",
    "        print(f\" {test_name} : RÉUSSI\")\n",
    "        return True, result\n",
    "    except Exception as e:\n",
    "        print(f\" {test_name} : ÉCHOUÉ\")\n",
    "        print(f\"   Erreur : {str(e)}\")\n",
    "        return False, None\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# TEST 1 : ENVIRONNEMENT\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_environment():\n",
    "    \"\"\"Vérifie les variables d'environnement\"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    required_vars = {\n",
    "        \"MISTRAL_API_KEY\": os.getenv(\"MISTRAL_API_KEY\"),\n",
    "        \"PINECONE_API_KEY\": os.getenv(\"PINECONE_API_KEY\")\n",
    "    }\n",
    "    \n",
    "    missing = [k for k, v in required_vars.items() if not v]\n",
    "    \n",
    "    if missing:\n",
    "        raise ValueError(f\"Variables manquantes : {', '.join(missing)}\")\n",
    "    \n",
    "    for key, value in required_vars.items():\n",
    "        masked = value[:10] + \"...\" if value else \"None\"\n",
    "        print(f\"   {key}: {masked}\")\n",
    "    \n",
    "    return required_vars\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# TEST 2 : PINECONE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_pinecone():\n",
    "    \"\"\"Vérifie l'état de l'index Pinecone\"\"\"\n",
    "    from pinecone import Pinecone\n",
    "    \n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    index = pc.Index(\"kpmg-veille\")\n",
    "    stats = index.describe_index_stats()\n",
    "    \n",
    "    print(f\"   Index : kpmg-veille\")\n",
    "    print(f\"   Total vecteurs : {stats.total_vector_count}\")\n",
    "    print(f\"   Dimension : 1024\")\n",
    "    \n",
    "    if stats.total_vector_count == 0:\n",
    "        print(\"     ATTENTION : Aucune donnée indexée !\")\n",
    "        print(\"    Exécutez les Notebooks 2, 3 et 4\")\n",
    "        raise ValueError(\"Index vide\")\n",
    "    \n",
    "    print(f\"\\n    Namespaces :\")\n",
    "    for ns, info in stats.namespaces.items():\n",
    "        print(f\"      - {ns}: {info.vector_count} vecteurs\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# TEST 3 : MISTRAL EMBEDDINGS\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_embeddings():\n",
    "    \"\"\"Teste la génération d'embeddings\"\"\"\n",
    "    from langchain_mistralai import MistralAIEmbeddings\n",
    "    \n",
    "    embeddings = MistralAIEmbeddings(\n",
    "        model=\"mistral-embed\",\n",
    "        mistral_api_key=os.getenv(\"MISTRAL_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Test sur une phrase simple\n",
    "    test_text = \"Apple annonce de nouveaux produits\"\n",
    "    embedding = embeddings.embed_query(test_text)\n",
    "    \n",
    "    print(f\"   Texte : '{test_text}'\")\n",
    "    print(f\"   Dimension : {len(embedding)}\")\n",
    "    print(f\"   Type : {type(embedding)}\")\n",
    "    print(f\"   Premiers 5 valeurs : {embedding[:5]}\")\n",
    "    \n",
    "    if len(embedding) != 1024:\n",
    "        raise ValueError(f\"Dimension incorrecte : {len(embedding)} (attendu : 1024)\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# TEST 4 : MISTRAL LLM\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_llm():\n",
    "    \"\"\"Teste le modèle LLM\"\"\"\n",
    "    from langchain_mistralai import ChatMistralAI\n",
    "    \n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-small\",\n",
    "        temperature=0,\n",
    "        mistral_api_key=os.getenv(\"MISTRAL_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Test simple\n",
    "    response = llm.invoke(\"Réponds en un mot : quelle est la capitale de la France ?\")\n",
    "    \n",
    "    print(f\"   Modèle : mistral-medium\")\n",
    "    print(f\"   Question : Capitale de la France ?\")\n",
    "    print(f\"   Réponse : {response.content}\")\n",
    "    \n",
    "    if \"Paris\" not in response.content:\n",
    "        print(\"     Réponse inattendue, mais API fonctionne\")\n",
    "    \n",
    "    return llm\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# TEST 5 : RETRIEVER\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_retriever():\n",
    "    \"\"\"Teste la recherche vectorielle\"\"\"\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "    from langchain_mistralai import MistralAIEmbeddings\n",
    "    \n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    \n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=\"kpmg-veille\",\n",
    "        embedding=embeddings,\n",
    "        namespace=\"news\"  # Utilise le namespace qui a des données\n",
    "    )\n",
    "    \n",
    "    # Test de recherche\n",
    "    query = \"Apple dernières actualités\"\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    \n",
    "    print(f\"   Query : '{query}'\")\n",
    "    print(f\"   Namespace : news\")\n",
    "    print(f\"   Résultats : {len(results)} documents\")\n",
    "    \n",
    "    if not results:\n",
    "        raise ValueError(\"Aucun résultat trouvé - vérifiez l'indexation\")\n",
    "    \n",
    "    print(f\"\\n    Premier résultat :\")\n",
    "    print(f\"      Source : {results[0].metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"      Contenu : {results[0].page_content[:150]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# TEST 6 : CHAÎNE RAG COMPLÈTE\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_rag_chain():\n",
    "    \"\"\"Teste la chaîne RAG complète\"\"\"\n",
    "    from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.runnables import RunnablePassthrough\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    \n",
    "    # Composants\n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=\"kpmg-veille\",\n",
    "        embedding=embeddings,\n",
    "        namespace=\"news\"\n",
    "    )\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "    llm = ChatMistralAI(model=\"mistral-medium\", temperature=0)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Contexte : {context}\\n\\nQuestion : {question}\\n\\nRéponse courte :\"\n",
    "    )\n",
    "    \n",
    "    # Chaîne\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([d.page_content[:200] for d in docs])\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    question = \"Quelles sont les dernières actualités ?\"\n",
    "    print(f\"   Question : '{question}'\")\n",
    "    print(f\"   Traitement...\")\n",
    "    \n",
    "    response = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"\\n    Réponse générée ({len(response)} caractères) :\")\n",
    "    print(f\"   {response[:300]}...\")\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# TEST 7 : GRADIO (OPTIONNEL)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def test_gradio():\n",
    "    \"\"\"Vérifie que Gradio est installé\"\"\"\n",
    "    import gradio as gr\n",
    "    \n",
    "    version = gr.__version__\n",
    "    print(f\"   Version : {version}\")\n",
    "    \n",
    "    if version < \"4.0.0\":\n",
    "        print(\"     Version ancienne détectée\")\n",
    "        print(\"   Recommandé : pip install --upgrade gradio\")\n",
    "    \n",
    "    return gr\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# EXÉCUTION DES TESTS\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "def main():\n",
    "    \"\"\"Exécute tous les tests dans l'ordre\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \" \"*20)\n",
    "    print(\"   DIAGNOSTIC COMPLET DU SYSTÈME RAG KPMG\")\n",
    "    print(\" \"*20 + \"\\n\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Liste des tests à exécuter\n",
    "    tests = [\n",
    "        (\"Variables d'environnement\", test_environment),\n",
    "        (\"État de Pinecone\", test_pinecone),\n",
    "        (\"Mistral Embeddings\", test_embeddings),\n",
    "        (\"Mistral LLM\", test_llm),\n",
    "        (\"Retriever\", test_retriever),\n",
    "        (\"Chaîne RAG complète\", test_rag_chain),\n",
    "        (\"Gradio\", test_gradio)\n",
    "    ]\n",
    "    \n",
    "    # Exécution\n",
    "    for test_name, test_func in tests:\n",
    "        success, result = run_test(test_name, test_func)\n",
    "        results[test_name] = {\"success\": success, \"result\": result}\n",
    "    \n",
    "    # Rapport final\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" RAPPORT FINAL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    passed = sum(1 for r in results.values() if r[\"success\"])\n",
    "    total = len(results)\n",
    "    \n",
    "    print(f\"\\nTests réussis : {passed}/{total}\")\n",
    "    \n",
    "    if passed == total:\n",
    "        print(\"\\n TOUS LES TESTS SONT PASSÉS !\")\n",
    "        print(\" Votre système est prêt pour Gradio\")\n",
    "        print(\"\\n Prochaine étape :\")\n",
    "        print(\"   Exécutez la cellule 'gradio_interface_fixed.py'\")\n",
    "    else:\n",
    "        print(\"\\n  CERTAINS TESTS ONT ÉCHOUÉ\")\n",
    "        print(\"\\n Actions recommandées :\")\n",
    "        \n",
    "        for test_name, result in results.items():\n",
    "            if not result[\"success\"]:\n",
    "                print(f\"\\n {test_name} :\")\n",
    "                \n",
    "                if \"environnement\" in test_name.lower():\n",
    "                    print(\"   → Vérifiez votre fichier .env\")\n",
    "                    print(\"   → load_dotenv() doit être appelé\")\n",
    "                \n",
    "                elif \"pinecone\" in test_name.lower():\n",
    "                    print(\"   → Exécutez les Notebooks 2, 3, 4 pour indexer des données\")\n",
    "                    print(\"   → Vérifiez que l'index 'kpmg-veille' existe\")\n",
    "                \n",
    "                elif \"embeddings\" in test_name.lower():\n",
    "                    print(\"   → Vérifiez votre MISTRAL_API_KEY\")\n",
    "                    print(\"   → Testez manuellement : https://console.mistral.ai/\")\n",
    "                \n",
    "                elif \"llm\" in test_name.lower():\n",
    "                    print(\"   → Vérifiez les crédits de votre compte Mistral\")\n",
    "                    print(\"   → Essayez 'mistral-small' si 'medium' ne fonctionne pas\")\n",
    "                \n",
    "                elif \"retriever\" in test_name.lower():\n",
    "                    print(\"   → L'index est vide ou le namespace 'news' n'existe pas\")\n",
    "                    print(\"   → Réexécutez le Notebook 4 (indexation)\")\n",
    "                \n",
    "                elif \"rag\" in test_name.lower():\n",
    "                    print(\"   → Un composant précédent a échoué\")\n",
    "                    print(\"   → Corrigez les erreurs ci-dessus d'abord\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Diagnostic terminé\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "# LANCEMENT\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpmgvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
